{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudelepere/ML_GitHub/blob/main/Prediction_EN_791112_50000_51000_below_04281630_thr03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Error: gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
        "Ignore this error as gcsfs is not used."
      ],
      "metadata": {
        "id": "5W_w9Z5kT02C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate\n",
        "!pip install -q huggingface_hub\n",
        "#!pip install -q scikit-learn\n",
        "!pip install -q transformers datasets  # 2 Hugging Face libraries\n",
        "#!pip install -q wandb"
      ],
      "metadata": {
        "id": "cgPqZs5ZMfky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27c0dc42-2756-42a0-b77f-8f3d4d53894c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m118.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy             as np\n",
        "import os\n",
        "import pandas            as pd\n",
        "import shutil\n",
        "import torch\n",
        "\n",
        "from contextlib                     import suppress\n",
        "from datasets                       import DatasetDict\n",
        "from google.colab                   import files, userdata\n",
        "from huggingface_hub                import create_repo, hf_hub_download, login, whoami\n",
        "from sklearn.metrics                import classification_report\n",
        "from transformers.models.longformer import LongformerTokenizerFast, LongformerForSequenceClassification\n",
        "from torch.utils.data               import DataLoader"
      ],
      "metadata": {
        "id": "qgqnbKcqSKlq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TO DO BEFORE RUN\n",
        "- GIVE HF HUB AND W&B ACCESSES\n",
        "- ADAPT HF HUB MODEL ID"
      ],
      "metadata": {
        "id": "rAN6xp95jGQJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98L73sydik95"
      },
      "source": [
        "## Hugging Face Hub authenticate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BNxYo9txwcJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a116258e-01dc-491d-ca5a-bbcc22524e94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user: {'type': 'user', 'id': '66ec3d5f61228b02f8780beb', 'name': 'claudelepere', 'fullname': 'Claude Lepère', 'isPro': False, 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66ec3d5f61228b02f8780beb/gvnf9pvm2KvE90ETMUQo3.jpeg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'jobs_token', 'role': 'fineGrained', 'createdAt': '2025-01-04T17:44:35.493Z', 'fineGrained': {'canReadGatedRepos': False, 'global': [], 'scoped': [{'entity': {'_id': '66ec3d5f61228b02f8780beb', 'type': 'user', 'name': 'claudelepere'}, 'permissions': ['repo.content.read', 'repo.write']}]}}}}\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")    # Store the key in os.environ\n",
        "hf_token               = os.environ.get('HF_TOKEN')\n",
        "\n",
        "login(token=hf_token)\n",
        "\n",
        "# Check\n",
        "user = whoami(token=hf_token)\n",
        "assert user['name'] == 'claudelepere', f\"{user['name']} is not claudelepere\"\n",
        "\n",
        "print(f\"user: {user}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration"
      ],
      "metadata": {
        "id": "FAEMJDTzBnJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Define literal data"
      ],
      "metadata": {
        "id": "VrwJB0WuByra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HF Hub repo\n",
        "repo_id = \"claudelepere/jobs_EN_791112_50000_below_04281630\"\n",
        "\n",
        "skills           = 791112\n",
        "\n",
        "all_rows_low_trEvTe  = 0\n",
        "all_rows_high_trEvTe = 50000\n",
        "\n",
        "all_rows_low     = 50000\n",
        "all_rows_high    = 51000\n",
        "num_datapoints   = all_rows_high - all_rows_low\n",
        "\n",
        "max_length       = 1024\n",
        "batch_size       = 4*8\n",
        "\n",
        "threshold_tuning = True\n",
        "\n",
        "min_threshold    = 0.3\n",
        "min_trEvTe_count = 50"
      ],
      "metadata": {
        "id": "Toq641qZB-MT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Define input paths"
      ],
      "metadata": {
        "id": "aFMFCe8bB-yX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# datasetDict zip file\n",
        "datasetDict_zip_file_name = f\"dataset_EN_{skills}_{all_rows_low}_{all_rows_high}_below.zip\"\n",
        "datasetDict_dir_name      = os.path.splitext(datasetDict_zip_file_name)[0]\n",
        "\n",
        "print(f\"datasetDict_zip_file_name: {datasetDict_zip_file_name}\")\n",
        "print(f\"datasetDict_dir_name     : {datasetDict_dir_name}\")\n",
        "\n",
        "# Optimized thresholds json file\n",
        "optimized_thresholds_json = hf_hub_download(repo_id=repo_id, filename=\"optimized_thresholds.json\", cache_dir=\"./my_HF_downloads\")\n",
        "print(f\"file_path: {optimized_thresholds_json}\")\n",
        "\n",
        "print(f\"cwd: {os.getcwd()}\")\n",
        "\n",
        "pr_below_count_json     = f\"pr_{all_rows_low}_{all_rows_high}_below_count.json\"\n",
        "trEvTe_below_count_json = f\"trEvTe_{all_rows_low_trEvTe}_{all_rows_high_trEvTe}_below_count.json\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "a6e81bbfdd9d437da3170aed42110996",
            "3a413fd339c04aaabac27f5b133a49ac",
            "fc7c90e6936a4b1ca46bbc177c2ef736",
            "248cec17179148b9a6ebb76fa38c3473",
            "4fd99eaa11884ff9938bc1553e31422b",
            "5d2d952ccca2417b89eb69c832d08a25",
            "d34ae3d912e64c9da76649316b88313b",
            "d4f5c29705244bf7a0afe0e2ce38d989",
            "1d8f8b84074c4c279cca3797487309a7",
            "0b70f528c1ad400c98cf0d058a18d4e1",
            "eea0d313f4844d70b1e25dac8f2ba5a4"
          ]
        },
        "id": "UDh01i3XCG8J",
        "outputId": "e2c59cc4-5b0e-4143-e21b-d54b70f541d6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasetDict_zip_file_name: dataset_EN_791112_50000_51000_below.zip\n",
            "datasetDict_dir_name     : dataset_EN_791112_50000_51000_below\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "optimized_thresholds.json:   0%|          | 0.00/2.73k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a6e81bbfdd9d437da3170aed42110996"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file_path: ./my_HF_downloads/models--claudelepere--jobs_EN_791112_50000_below_04281630/snapshots/8768833c5721f3025397dc7c66d4bdafdd3e6b0f/optimized_thresholds.json\n",
            "cwd: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Define output paths"
      ],
      "metadata": {
        "id": "Pssak9pDCHyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# true_labels - preds comparison file\n",
        "compare_true_labels_preds_file = \"compare_true_labels_preds.txt\""
      ],
      "metadata": {
        "id": "U3pXqYAzCLaB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload"
      ],
      "metadata": {
        "id": "1Ld4clfKw8vT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the file pr_below_count_json and save it in /content, the current working directory\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"Uploaded file: {filename}\")\n",
        "\n",
        "with open(pr_below_count_json, 'r') as json_file:\n",
        "    both_dict = json.load(json_file)\n",
        "\n",
        "pr_counter_dict        = both_dict[\"counter_dict\"]\n",
        "pr_sorted_counter_dict = both_dict[\"sorted_counter_dict\"]\n",
        "print(f\"pr_counter_dict: {type(pr_counter_dict)} len={len(pr_counter_dict)}\\n{json.dumps(pr_counter_dict, indent=4)}\")\n",
        "\n",
        "raise Exception(\"ZZZZ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ay7GMSa0vClf",
        "outputId": "07db8b07-11c2-43b9-be06-1c52c3eb2f69"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bed3e7af-17df-45f9-9f5c-bc50581cf79f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bed3e7af-17df-45f9-9f5c-bc50581cf79f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dataset_EN_791112_50000_51000_below.zip to dataset_EN_791112_50000_51000_below.zip\n",
            "Saving pr_50000_51000_below_count.json to pr_50000_51000_below_count.json\n",
            "Saving trEvTe_0_50000_below_count.json to trEvTe_0_50000_below_count.json\n",
            "Uploaded file: dataset_EN_791112_50000_51000_below.zip\n",
            "Uploaded file: pr_50000_51000_below_count.json\n",
            "Uploaded file: trEvTe_0_50000_below_count.json\n",
            "pr_counter_dict: <class 'dict'> len=94\n",
            "{\n",
            "    \"143\": 15,\n",
            "    \"152\": 3,\n",
            "    \"155\": 10,\n",
            "    \"167\": 10,\n",
            "    \"173\": 12,\n",
            "    \"175\": 17,\n",
            "    \"176\": 7,\n",
            "    \"360\": 6,\n",
            "    \"370\": 1,\n",
            "    \"371\": 6,\n",
            "    \"373\": 11,\n",
            "    \"375\": 10,\n",
            "    \"376\": 12,\n",
            "    \"667\": 1,\n",
            "    \"686\": 13,\n",
            "    \"687\": 7,\n",
            "    \"756\": 3,\n",
            "    \"758\": 3,\n",
            "    \"760\": 4,\n",
            "    \"900\": 0,\n",
            "    \"901\": 0,\n",
            "    \"902\": 0,\n",
            "    \"903\": 0,\n",
            "    \"905\": 0,\n",
            "    \"907\": 0,\n",
            "    \"908\": 1,\n",
            "    \"909\": 0,\n",
            "    \"911\": 0,\n",
            "    \"912\": 0,\n",
            "    \"913\": 0,\n",
            "    \"914\": 0,\n",
            "    \"916\": 0,\n",
            "    \"263\": 9,\n",
            "    \"265\": 7,\n",
            "    \"266\": 3,\n",
            "    \"267\": 2,\n",
            "    \"268\": 7,\n",
            "    \"270\": 0,\n",
            "    \"272\": 14,\n",
            "    \"273\": 3,\n",
            "    \"274\": 18,\n",
            "    \"275\": 0,\n",
            "    \"276\": 16,\n",
            "    \"279\": 1,\n",
            "    \"283\": 8,\n",
            "    \"285\": 3,\n",
            "    \"286\": 8,\n",
            "    \"287\": 5,\n",
            "    \"288\": 15,\n",
            "    \"292\": 1,\n",
            "    \"293\": 5,\n",
            "    \"299\": 5,\n",
            "    \"301\": 0,\n",
            "    \"303\": 15,\n",
            "    \"358\": 8,\n",
            "    \"365\": 15,\n",
            "    \"369\": 4,\n",
            "    \"379\": 13,\n",
            "    \"382\": 3,\n",
            "    \"409\": 12,\n",
            "    \"610\": 11,\n",
            "    \"611\": 8,\n",
            "    \"615\": 11,\n",
            "    \"618\": 0,\n",
            "    \"619\": 3,\n",
            "    \"668\": 13,\n",
            "    \"670\": 7,\n",
            "    \"672\": 3,\n",
            "    \"673\": 1,\n",
            "    \"677\": 29,\n",
            "    \"678\": 0,\n",
            "    \"681\": 3,\n",
            "    \"682\": 3,\n",
            "    \"683\": 8,\n",
            "    \"810\": 0,\n",
            "    \"811\": 17,\n",
            "    \"812\": 15,\n",
            "    \"814\": 10,\n",
            "    \"815\": 1,\n",
            "    \"816\": 17,\n",
            "    \"817\": 11,\n",
            "    \"818\": 4,\n",
            "    \"819\": 6,\n",
            "    \"820\": 3,\n",
            "    \"822\": 4,\n",
            "    \"823\": 16,\n",
            "    \"824\": 6,\n",
            "    \"825\": 1,\n",
            "    \"826\": 7,\n",
            "    \"827\": 2,\n",
            "    \"828\": 5,\n",
            "    \"829\": 7,\n",
            "    \"830\": 10,\n",
            "    \"350\": 21\n",
            "}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ZZZZ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-09e697384a44>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"pr_counter_dict: {type(pr_counter_dict)} len={len(pr_counter_dict)}\\n{json.dumps(pr_counter_dict, indent=4)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ZZZZ\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mException\u001b[0m: ZZZZ"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checks"
      ],
      "metadata": {
        "id": "FNtnheEIS_uy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhAd6EZJwcJt"
      },
      "outputs": [],
      "source": [
        "!python -V\n",
        "\n",
        "print(f\"currentdir: {os.getcwd()}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # cuda = A100 GPU or L4 GPU\n",
        "print(f\"device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr4xZXGSXsEx"
      },
      "source": [
        "## Out Of Memory (OOM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmIswaWGsBBV"
      },
      "source": [
        "### OOM: check for and kill zombie processes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKeGyFYHwcJu"
      },
      "outputs": [],
      "source": [
        "!ps aux | grep python\n",
        "!kill -9 <PID>\n",
        "if torch.cuda.is_available():\n",
        "    !nvidia-smi\n",
        "    print(torch.cuda.memory_summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4v9XcFLuudV"
      },
      "source": [
        "### OOM: free up GPU memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJNB5_2HwcJu"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPt8yoHMsksM"
      },
      "source": [
        "## Upload and unzip job dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggSY4TBWXfvS"
      },
      "outputs": [],
      "source": [
        "def upload_unzip_dataset(filename):\n",
        "    \"\"\"Upload and unzip the dataset to /content, ensuring correct placement.\"\"\"\n",
        "\n",
        "    # Get the expected directory name (same as the zip filename without extension)\n",
        "    expected_dir = os.path.splitext(filename)[0]\n",
        "\n",
        "    # Check if the file and the directory exist in /content and delete them\n",
        "    with suppress(FileNotFoundError):\n",
        "        if os.path.isdir(expected_dir):\n",
        "            shutil.rmtree(expected_dir)               # Remove directory if it exists\n",
        "        if os.path.isfile(filename):\n",
        "            os.remove(filename)                       # Remove file if it exists\n",
        "\n",
        "    print(f\"Removed '{expected_dir}' and '{filename}' if they were present in /content.\")\n",
        "\n",
        "    # Upload the zip file\n",
        "    uploaded_files = files.upload()                  # Prompt file upload dialog\n",
        "\n",
        "    if filename not in uploaded_files:\n",
        "        raise FileNotFoundError(f\"'{filename}' was not uploaded.\")\n",
        "\n",
        "    print(f\"'{filename}' successfully uploaded to /content.\")\n",
        "\n",
        "    # Unzip the file to /content\n",
        "    shutil.unpack_archive(filename, \"/content\")\n",
        "\n",
        "    print(f\"Unzipped to '/content/{expected_dir}'.\")\n",
        "\n",
        "# Usage\n",
        "upload_unzip_dataset(datasetDict_zip_file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfBhdTN9wcJz"
      },
      "source": [
        "## Create datasetDict (HF DatasetDict) = 1 HF Dataset prediction\n",
        "The sequence of datapoints of dataset=datasetDict['prediction'] is the same as the sequence of SELECT, id, ... WHERE in_LanguageId=1 ORDER BY id DESC in MySQL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh_SgQD9wcJz"
      },
      "outputs": [],
      "source": [
        "datasetDict = DatasetDict.load_from_disk(datasetDict_dir_name)\n",
        "dataset     = datasetDict['prediction']\n",
        "\n",
        "print(f\"dataset: {type(dataset)} shape={dataset.shape}\\n{dataset}\")     # <class 'datasets.arrow_dataset.Dataset'> shape=(100, 8)\n",
        "print(f\"dataset.features: {type(dataset.features)} shape={dataset.features}\\n{dataset.features}\")\n",
        "\n",
        "# Convert the dataset to a pandas dataframe\n",
        "df_original = pd.DataFrame(dataset)\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "print(f\"df_original: {type(df_original)} shape={df_original.shape}\\n{df_original}\")                         # <class 'pandas.core.frame.DataFrame'> shape=(100, 8)\n",
        "pd.reset_option('display.max_columns')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEgARWcywcJz"
      },
      "source": [
        "## Create labels (list), id2label (dict) and label2id (dict).\n",
        "**The sequence of the labels list is the same as in dataset.\n",
        "And the sequences of the optimized thresholds, true labels and predictions are preserved.**\n",
        "\n",
        "* dataset 7_1000_125_125  ,  48 labels\n",
        "* dataset 7_128_18_54     ,  42 labels\n",
        "* dataset 8910_1087_68_204, 206 labels\n",
        "* dataset 11_1000         ,   6 labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vv6FWVzRwcJz"
      },
      "outputs": [],
      "source": [
        "labels = [label for label in dataset.features.keys() if label not in ['id', 'text']]\n",
        "#labels.sort()\n",
        "\n",
        "print(f\"labels: {type(labels)} {len(labels)}\\n{labels}\")\n",
        "\n",
        "num_labels = len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-xKopoHwcJz"
      },
      "outputs": [],
      "source": [
        "id2label = {idx: label for idx, label in enumerate(labels)}\n",
        "print(f\"id2label: {type(id2label)} {len(id2label)}\\n{id2label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5eYNbH5FNsd"
      },
      "outputs": [],
      "source": [
        "label2id = {label: idx for idx, label in enumerate(labels)}\n",
        "print(f\"label2id: {type(label2id)} {len(label2id)}\\n{label2id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3W9-Xdo_egG"
      },
      "source": [
        "## Download the tokenizer and the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vrs9lNbS-uh3"
      },
      "outputs": [],
      "source": [
        "print(\"Tokenizer\")\n",
        "tokenizer = LongformerTokenizerFast.from_pretrained(repo_id, timeout=60)  # Increased timeout to 60 seconds)\n",
        "\n",
        "print(\"Model\")\n",
        "model = LongformerForSequenceClassification.from_pretrained(repo_id)\n",
        "model.eval()  # Ensures no gradient computation\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload the json cout file"
      ],
      "metadata": {
        "id": "1pVJ5QyvtaDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the file pr_below_count_json and save it in /content, the current working directory\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Read the file prediction\n",
        "with open(pr_below_count_json, 'r') as json_file:\n",
        "    both_dict = json.load(json_file)\n",
        "\n",
        "pr_counter_dict        = both_dict[\"counter_dict\"]\n",
        "pr_sorted_counter_dict = both_dict[\"sorted_counter_dict\"]\n",
        "print(f\"pr_counter_dict: {type(pr_counter_dict)} len={len(pr_counter_dict)}\\n{json.dumps(pr_counter_dict, indent=4)}\")\n",
        "print(f\"pr_sorted_counter_dict: {type(pr_sorted_counter_dict)} len={len(pr_sorted_counter_dict)}\\n{json.dumps(pr_sorted_counter_dict, indent=4)}\")\n",
        "\n",
        "# Upload the file trEvTe_below_count_json and save it in /content, the current working directory\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Read the file trEvTe\n",
        "with open(trEvTe_below_count_json, 'r') as json_file:\n",
        "    both_dict = json.load(json_file)\n",
        "trEvTe_counter_dict        = both_dict[\"counter_dict\"]\n",
        "trEvTe_sorted_counter_dict = both_dict[\"sorted_counter_dict\"]\n",
        "print(f\"trEvTe_counter_dict: {type(trEvTe_counter_dict)} len={len(trEvTe_counter_dict)}\\n{json.dumps(trEvTe_counter_dict, indent=4)}\")\n",
        "print(f\"trEvTe_sorted_counter_dict: {type(trEvTe_sorted_counter_dict)} len={len(trEvTe_sorted_counter_dict)}\\n{json.dumps(trEvTe_sorted_counter_dict, indent=4)}\")\n",
        "\n",
        "#raise Exception(\"AAAA\")"
      ],
      "metadata": {
        "id": "fciXSq4aZYNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloads the optimized thresholds\n",
        "\n",
        "- count_0_list = [\"900\", \"901\", \"902\", \"903\", \"905\", \"907\", \"909\", \"911\", \"912\", \"913\", \"914\", \"916\"]\n",
        "  - count = 0 in prediction below\n",
        "  - count in trEvTE below:    \n",
        "        \"900\": 3,\n",
        "        \"901\": 2,\n",
        "        \"902\": 1,\n",
        "        \"903\": 7,\n",
        "        \"905\": 5,\n",
        "        \"907\": 2,\n",
        "        \"908\": 5,\n",
        "        \"909\": 1,\n",
        "        \"911\": 6,\n",
        "        \"912\": 1,\n",
        "        \"913\": 5,\n",
        "        \"914\": 3,\n",
        "        \"916\": 4,\n",
        "- count_01_025_list = [\"908\", \"913\", \"672\", \"673\"]\n",
        "  - first normalized_thresholds 908 and 913: 0.1, 672 and 673: 0.25\n",
        "  - count in prediction below: 908: 1, 913: 0, 672: 3, 673: 1\n",
        "  - count in trEvTe below: 908: 5, 913: 5, 672: 37, 673: 41\n"
      ],
      "metadata": {
        "id": "eI8Zd_7wxPQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the JSON file\n",
        "with open(optimized_thresholds_json, 'r') as json_file:\n",
        "    optimized_thresholds_json = json.load(json_file)\n",
        "\n",
        "#print(f\"optimized_thresholds_json: {type(optimized_thresholds_json)} {len(optimized_thresholds_json)}\")\n",
        "#print(json.dumps(optimized_thresholds_json, indent=2))\n",
        "\n",
        "optimized_thresholds = np.array(list(optimized_thresholds_json.values()))\n",
        "print(f\"optimized_thresholds before: {type(optimized_thresholds)} shape={optimized_thresholds.shape}\\n{optimized_thresholds}\")\n",
        "\n",
        "#optimized_thresholds = np.maximum(optimized_thresholds, min_threshold)\n",
        "#print(f\"optimized_thresholds: {type(optimized_thresholds)} shape={optimized_thresholds.shape}\\n{optimized_thresholds}\")\n",
        "\n",
        "\n",
        "#count_0_list = [\"900\", \"901\", \"902\", \"903\", \"905\", \"907\", \"909\", \"911\", \"912\", \"913\", \"914\", \"916\"]\n",
        "#count_01_025_list = [\"908\", \"913\", \"672\", \"673\"]\n",
        "#count_both = count_0_list + count_01_025_list\n",
        "\n",
        "#rare_labels_in_trEvTe = [label for label, count in trEvTe_counter_dict.items() if count < min_trEvTe_count]\n",
        "#print(f\"rare_labels_in_trEvTe: {type(rare_labels_in_trEvTe)} len={len(rare_labels_in_trEvTe)}\\n{rare_labels_in_trEvTe}\")\n",
        "\n",
        "for idx, (label, count) in enumerate(pr_counter_dict.items()):\n",
        "    print(f\"{idx}: {label} ->{count}: {optimized_thresholds[idx]:.2f}\")\n",
        "#    #if label in count_both:\n",
        "#    if label in rare_labels_in_trEvTe:\n",
        "#        optimized_thresholds[idx] = min_threshold\n",
        "\n",
        "# Vectorize the update of optimized_thresholds\n",
        "low_count_indices = [idx for idx, (label, count) in enumerate(trEvTe_counter_dict.items()) if count < min_trEvTe_count]\n",
        "optimized_thresholds[low_count_indices] = min_threshold\n",
        "\n",
        "print()\n",
        "\n",
        "print(f\"optimized_thresholds after: {type(optimized_thresholds)} shape={optimized_thresholds.shape}\\n{optimized_thresholds}\")\n",
        "#raise Exception(\"BBBB\")"
      ],
      "metadata": {
        "id": "BbOjh8YDzUbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_int_or_NaN(x):\n",
        "    return int(x) if not np.isnan(x) else \"NaN\""
      ],
      "metadata": {
        "id": "O_frM5Opalea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_confusion_multi(precision, recall, support, total_samples):\n",
        "    precision     = np.array(precision)\n",
        "    recall        = np.array(recall)\n",
        "    support       = np.array(support)\n",
        "    total_samples = np.array(total_samples)\n",
        "\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "        TP       = recall * support\n",
        "        FN       = support - TP\n",
        "        FP       = TP * (1 - precision)/precision\n",
        "        TN       = total_samples - TP - FN - FP\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'TP' : TP,\n",
        "        'FN' : FN,\n",
        "        'FP' : FP,\n",
        "        'TN' : TN\n",
        "    })"
      ],
      "metadata": {
        "id": "OCwrZOo2R4ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_multi_label_performance(y_true, y_pred, label_names=None, plot=True):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "    - label_df    : per-label precision, recall, f1, support (DataFrame)\n",
        "    - summary_df  : micro, macro, weighted and samples averages (DataFrame)\n",
        "    - confusion_df: per-label TP, FP, FN, TN (DataFrame)\n",
        "    \"\"\"\n",
        "    report_dict = classification_report(\n",
        "        y_true, y_pred,\n",
        "        target_names=label_names,\n",
        "        zero_division=0,\n",
        "        output_dict=True\n",
        "    )\n",
        "    report_df = pd.DataFrame(report_dict).T\n",
        "    #print(f\"report_df: {type(report_df)} shape={report_df.shape}\\n{report_df}\")\n",
        "\n",
        "    # label_df\n",
        "    label_df = report_df.loc[label_names].copy()\n",
        "    label_df['support'] = label_df['support'].apply(format_int_or_NaN)\n",
        "    #print(f\"label_df: {type(label_df)} shape={label_df.shape}\\n{label_df}\")\n",
        "\n",
        "    precision = label_df['precision'].values\n",
        "    recall    = label_df['recall'].values\n",
        "    f1_score  = label_df['f1-score'].values\n",
        "    support   = label_df['support'].values\n",
        "\n",
        "    total_samples = len(y_true)\n",
        "    print(f\"total_samples: {type(total_samples)} {total_samples}\")  # <class 'int'> 1000\n",
        "    total_support = label_df['support'].sum()\n",
        "    print(f\"total_support: {type(total_support)} {total_support}\")  # <class 'numpy.float64'> 6441.0\n",
        "\n",
        "    confusion_df       = calculate_confusion_multi(precision, recall, support, total_samples)\n",
        "    confusion_df.index = label_names\n",
        "    #confusion_df       = confusion_df.applymap(format_int_or_NaN)\n",
        "    confusion_df       = confusion_df.apply(lambda col: col.map(format_int_or_NaN))\n",
        "    #print(f\"confusion_df: {type(confusion_df)} shape={confusion_df.shape}\\n{confusion_df}\")\n",
        "\n",
        "    # All average rows: micro, macro, weighted, samples\n",
        "    avg_rows      = [\"micro avg\", \"macro avg\", \"weighted avg\", \"samples avg\"]\n",
        "    existing_avgs = [row for row in avg_rows if row in report_df.index]\n",
        "\n",
        "    summary_df            = report_df.loc[existing_avgs, [\"precision\", \"recall\", \"f1-score\", \"support\"]]\n",
        "    summary_df.loc[\"micro avg\", \"support\"]    = total_support\n",
        "    summary_df.loc[\"macro avg\", \"support\"]    = np.nan         # no clear meaning\n",
        "    summary_df.loc[\"weighted avg\", \"support\"] = total_support  # number of samples\n",
        "    summary_df.loc[\"samples avg\", \"support\"]  = total_samples  # number of samples\n",
        "    summary_df[\"support\"]                     = summary_df[\"support\"].apply(format_int_or_NaN)\n",
        "    #print(f\"summary_df: {type(summary_df)} shape={summary_df.shape}\\n{summary_df}\"\n",
        "\n",
        "\n",
        "    # Optional plot\n",
        "    if plot:\n",
        "        #label_df[[\"precision\", \"recall\", \"f1\", \"support\"]].plot(\n",
        "        label_df[[\"precision\", \"recall\", \"support\"]].plot(\n",
        "            kind=\"bar\", figsize=(10,6), ylim=(0,1),\n",
        "            title=\"Per-Label Precision Recall F1 Support\"\n",
        "        )\n",
        "        plt.grid(axis='y')\n",
        "        plt.ylabel(\"Score\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return label_df, confusion_df, summary_df\n",
        "\n"
      ],
      "metadata": {
        "id": "CWtpZ5OHQR61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## case TP=0 FN!=0 and case TP=0 FN=0\n",
        "\n",
        "|label|TP|FN|FP|TN|label|precision|recall|f1|support|\n",
        "|:---:|::|::|::|::|:---:|:-------:|:----:|::|:-----:|\n",
        "|266|0|3|NaN|NaN|266|0.00|0.00|0.00|3|\n",
        "|267|0|2|NaN|NaN|267|0.00|0.00|0.00|2|\n",
        "|273|0|3|NaN|NaN|273|0.00|0.00|0.00|3|\n",
        "|275|0|0|NaN|NaN|275|0.00|0.00|0.00|0|\n",
        "|279|0|1|NaN|NaN|279|0.00|0.00|0.00|1|\n",
        "|285|0|3|NaN|NaN|285|0.00|0.00|0.00|3|\n",
        "|301|0|0|NaN|NaN|301|0.00|0.00|0.00|0|\n",
        "|618|0|0|NaN|NaN|618|0.00|0.00|0.00|0|\n",
        "|681|0|3|NaN|NaN|681|0.00|0.00|0.00|3|\n",
        "|815|0|1|NaN|NaN|815|0.00|0.00|0.00|1|\n",
        "|825|0|1|NaN|NaN|825|0.00|0.00|0.00|1|\n",
        "\n",
        "- TP=0 and FN!=0:\n",
        " - precision = 0\n",
        " - recall    = 0\n",
        " - f1        = 0\n",
        " - support   = FN (some true positives to be found)\n",
        "- TP=FN=0:\n",
        " - precision = 0/0 (undefined)\n",
        " - recall    = 0/0 (undefined)\n",
        " - f1        = 0/0 (undefined)\n",
        " - support   = 0   (label never present in the ground truth, the model never predicted it)\n",
        "- Check using jobs_preprocessing_CounterPrediction:\n",
        " - count_275: <class 'int'> 0\n",
        " - count_301: <class 'int'> 0\n",
        " - count_618: <class 'int'> 0"
      ],
      "metadata": {
        "id": "gfQEIQsgsr90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Tokenize dataset\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples[\"text\"], padding='max_length', max_length=max_length, truncation=True)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "# <class 'datasets.arrow_dataset.Dataset'> shape=(100, 10)\n",
        "#print(f\"tokenized_dataset: {type(tokenized_dataset)} shape={tokenized_dataset.shape}\\n{tokenized_dataset}\")\n",
        "\n",
        "tokenized_df = tokenized_dataset.to_pandas()\n",
        "#print(f\"tokenized_df: {type(tokenized_df)} shape={tokenized_df.shape}\\n{tokenized_df}\")\n",
        "\n",
        "# Step 2: Convert to PyTorch DataLoader\n",
        "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"] + labels)\n",
        "dataloader = DataLoader(tokenized_dataset, batch_size=batch_size)\n",
        "\n",
        "# Step 3: Run Prediction\n",
        "all_preds = []\n",
        "\n",
        "with torch.no_grad():                       # No gradients needed for prediction\n",
        "    for batch in dataloader:\n",
        "        # Move batch to GPU if available\n",
        "        inputs = {k: batch[k].to(device) for k in [\"input_ids\", \"attention_mask\"]}\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits  # Model outputs raw logits\n",
        "\n",
        "        # Apply sigmoid to convert logits into probabilities\n",
        "        probs = torch.sigmoid(logits)\n",
        "\n",
        "        # Move probs to CPU and convert to NumPy\n",
        "        all_preds.extend(probs.cpu().numpy())\n",
        "\n",
        "all_preds_arr = np.array(all_preds)\n",
        "#print(f\"all_preds    : {type(all_preds)}     len={len(all_preds)}\\n{all_preds}\")\n",
        "print(f\"ZOZIZUall_preds_arr: {type(all_preds_arr)} shape={all_preds_arr.shape}\\n{all_preds_arr}\")\n",
        "print(f\"ZOZIZUall_preds_arr[0]: {all_preds_arr[0]}\")\n",
        "\n",
        "# Step 4: Convert Probabilities to Binary Predictions\n",
        "if threshold_tuning:\n",
        "    thresholds = optimized_thresholds\n",
        "else:\n",
        "    thresholds = np.full(num_labels, 0.5)\n",
        "print(f\"thresholds: {type(thresholds)} shape={thresholds.shape}\\n{thresholds}\")                  # <class 'numpy.ndarray'> shape=(6,)\n",
        "print()\n",
        "\n",
        "binary_preds = (np.array(all_preds) > thresholds).astype(int)\n",
        "#print(f\"binary_preds: {type(binary_preds)} shape={binary_preds.shape}\\n{binary_preds}\")          # <class 'numpy.ndarray'> shape=(100, 6)\n",
        "print(f\"binary_preds[0]: {binary_preds[0]}\")\n",
        "\n",
        "# Step 5: Compare with True Labels\n",
        "true_labels = tokenized_df[labels]\n",
        "#print(f\"true_labels: {type(true_labels)} shape={true_labels.shape}\\n{true_labels}\")              # <class 'pandas.core.frame.DataFrame'> shape=(100, 6)\n",
        "\n",
        "# Convert true_labels DataFrame to a NumPy array of 0 and 1\n",
        "true_labels_np = true_labels.astype(int).to_numpy()\n",
        "#print(f\"true_labels_np: {type(true_labels_np)} shape={true_labels_np.shape}\\n{true_labels_np}\")  # <class 'numpy.ndarray'> shape=(100, 6)\n",
        "\n",
        "# Convert id2label dict to a list of label names\n",
        "label_names = list(id2label.values())\n",
        "print(f\"label_names: {type(label_names)} len={len(label_names)}\\n{label_names}\")  # <class 'list'> len=126\n",
        "print()\n",
        "\n",
        "#classification_report(y_true, y_pred)\n",
        "report = classification_report(\n",
        "    true_labels_np, binary_preds, target_names=label_names, zero_division=0)\n",
        "print(f\"classification report:{type(report)} len={len(report)}\\n{report}\")      # <class 'str'> len=7076\n",
        "print()\n",
        "\n",
        "#report2 = classification_report(true_labels_np, binary_preds, target_names=label_names, zero_division=0, output_dict=True)\n",
        "#print(f\"classification report2:{type(report2)} len={len(report2)}\\n{report2}\")  # <class 'dict'> len=130\n",
        "#print()\n",
        "\n",
        "label_df, confusion_df, summary_df = analyze_multi_label_performance(\n",
        "    true_labels_np, binary_preds, label_names=label_names, plot=False)\n",
        "\n",
        "label_df[['precision', 'recall', 'f1-score']] = label_df[['precision', 'recall', 'f1-score']].round(2)\n",
        "label_df['support'] = label_df['support'].astype(int)\n",
        "\n",
        "summary_df[['precision', 'recall', 'f1-score']] = summary_df[['precision', 'recall', 'f1-score']].round(2)\n",
        "\n",
        "print(\"__________________________________________\")\n",
        "\n",
        "print(\"\\nclassification report (= label_df):\\n\", label_df)\n",
        "\n",
        "print(\"__________________________________________\")\n",
        "\n",
        "print(\"\\nAll averages:\\n\", summary_df)\n",
        "\n",
        "print(\"__________________________________________\")\n",
        "\n",
        "#pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "#pd.set_option('display.max_colwidth', None)\n",
        "print(\"\\nConfusion matrix:\")\n",
        "print(\"precision = TP / (TP+FP)\")\n",
        "print(\"recall    = TP / (TP+FN)\")\n",
        "print(\"1/f1      = 1/2 * (1/precision + 1/recall\")\n",
        "print(\"support   = TP+FN\")\n",
        "print(\"\\nConfusion matrix:\\n\", confusion_df)\n",
        "\n",
        "print(\"__________________________________________\")\n",
        "\n",
        "#raise Exception(\"I stop here\")\n",
        "df_compare                = df_original.copy()\n",
        "df_compare['true_labels'] = true_labels.values.tolist()\n",
        "df_compare['preds']       = binary_preds.tolist()\n",
        "df_compare['compare']     = (df_compare['true_labels']==df_compare['preds']).replace({True:'OK', False:'KO'})\n",
        "#print(f\"df_compare: {type(df_compare)} shape={df_compare.shape}\\n{df_compare}\")  # <class 'pandas.core.frame.DataFrame'>\n",
        "print()\n",
        "\n",
        "def print_row(row, file):\n",
        "    \"\"\"Prints the row information to the console and writes it to a file.\"\"\"\n",
        "    output  = f\"id: {row['id']}\\n\"\n",
        "    output += f\"text: {row['text']}\\n\"\n",
        "    output += \"labels     : [\" + \", \".join(f\"{label:>5}\" for label in labels) + \"]\\n\"\n",
        "    output += \"true_labels: [\" + \", \".join(f\"{true_label:>5}\" for true_label in row['true_labels']) + \"]\\n\"\n",
        "    output += \"preds      : [\" + \", \".join(f\"{pred:>5}\" for pred in row['preds']) + \"]\\n\"\n",
        "    output += f\"compare    : {row['compare']}\\n\"\n",
        "    output += \"\\n\"\n",
        "\n",
        "    print(output, end=\"\")  # Print to console without extra newline\n",
        "    file.write(output)     # Write to file\n",
        "\n",
        "# Open the file in write mode\n",
        "with open(compare_true_labels_preds_file, \"w\") as f:\n",
        "    _ = df_compare.apply(print_row, axis=1, args=(f,))  # Pass file object to print_row\n",
        "\n",
        "# Download the file\n",
        "files.download(compare_true_labels_preds_file)\n",
        "\n",
        "# Count total 'OK' and 'KO' values\n",
        "total_ok = (df_compare['compare'] == 'OK').sum()  # Total matches\n",
        "total_ko = (df_compare['compare'] == 'KO').sum()  # Total mismatches\n",
        "print(f\"Total 'OK': {total_ok}\")\n",
        "print(f\"Total 'KO': {total_ko}\")\n",
        "print()"
      ],
      "metadata": {
        "id": "1YbcLnbRYb01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvh_jDvdBt26"
      },
      "outputs": [],
      "source": [
        "print(\"It's the end\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raise Exception(\"I stop here\")"
      ],
      "metadata": {
        "id": "KeA3lS-8v6Ml"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "jupytext": {
      "formats": "ipynb,py:nomarker"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a6e81bbfdd9d437da3170aed42110996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a413fd339c04aaabac27f5b133a49ac",
              "IPY_MODEL_fc7c90e6936a4b1ca46bbc177c2ef736",
              "IPY_MODEL_248cec17179148b9a6ebb76fa38c3473"
            ],
            "layout": "IPY_MODEL_4fd99eaa11884ff9938bc1553e31422b"
          }
        },
        "3a413fd339c04aaabac27f5b133a49ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d2d952ccca2417b89eb69c832d08a25",
            "placeholder": "​",
            "style": "IPY_MODEL_d34ae3d912e64c9da76649316b88313b",
            "value": "optimized_thresholds.json: 100%"
          }
        },
        "fc7c90e6936a4b1ca46bbc177c2ef736": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4f5c29705244bf7a0afe0e2ce38d989",
            "max": 2732,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d8f8b84074c4c279cca3797487309a7",
            "value": 2732
          }
        },
        "248cec17179148b9a6ebb76fa38c3473": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b70f528c1ad400c98cf0d058a18d4e1",
            "placeholder": "​",
            "style": "IPY_MODEL_eea0d313f4844d70b1e25dac8f2ba5a4",
            "value": " 2.73k/2.73k [00:00&lt;00:00, 249kB/s]"
          }
        },
        "4fd99eaa11884ff9938bc1553e31422b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d2d952ccca2417b89eb69c832d08a25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d34ae3d912e64c9da76649316b88313b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4f5c29705244bf7a0afe0e2ce38d989": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d8f8b84074c4c279cca3797487309a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b70f528c1ad400c98cf0d058a18d4e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eea0d313f4844d70b1e25dac8f2ba5a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}