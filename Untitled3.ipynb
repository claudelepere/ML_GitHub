{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJ0OlJbBRC8UkKLFp7TK2D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudelepere/ML_GitHub/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "id": "_W8qWV5mZ53t",
        "outputId": "0e7f566f-e700-437f-8325-1e4e299fe793"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-901a0c0d-1635-4300-8744-6f2775ff6793\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-901a0c0d-1635-4300-8744-6f2775ff6793\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving training_metrics.json to training_metrics.json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip my_model.zip -d my_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8pQqHJ-aQij",
        "outputId": "ea2f766e-9968-4382-be52-ece1794ab0f4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  my_model.zip\n",
            "   creating: my_model/my_model/\n",
            "  inflating: my_model/my_model/tokenizer.json  \n",
            "  inflating: my_model/my_model/special_tokens_map.json  \n",
            "  inflating: my_model/my_model/model.safetensors  \n",
            "  inflating: my_model/my_model/training_args.bin  \n",
            "  inflating: my_model/my_model/tokenizer_config.json  \n",
            "  inflating: my_model/my_model/vocab.txt  \n",
            "  inflating: my_model/my_model/config.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"my_model/my_model\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"my_model/my_model\")\n",
        "\n",
        "# Define the labels"
      ],
      "metadata": {
        "id": "5g35y8gJnpmN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"training_metrics.json\", 'r') as f:\n",
        "    training_metrics = json.load(f)\n",
        "with open(\"eval_metrics.json\", 'r') as f:\n",
        "    eval_metrics = json.load(f)"
      ],
      "metadata": {
        "id": "e10roTWiosYl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "labels = ['142', '143', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '160', '162', '165', '167', '168', '169', '170', '171', '173', '174', '175', '176', '356', '360', '361', '362', '364', '371', '373', '375', '376', '394', '408', '409', '667', '685', '686', '689', '756', '757', '758', '760', '761']\n",
        "\n",
        "#text = \"Voor een klant van Talencia ben ik opzoek naar een Senior Full Stack Developer (Java & Angular) Job beschrijving Als Developer zal je een bestaand team toevoegen en meewerken aan de buitbouw van webapplicaties op Azure. Dit is om bestaande applicaties te vervangen die end-of-live zijn. Het project is al in volle realisatie. Profiel Zeer goede kennis van Java en Angular Goede kennis van Azure DevOps, AKS,.. is een grote pluspunt Kennis van Docker/ SQL/ OAuth/PWA/ RESTful API is vereist Taal: Nederlands met kennis van Engels Extra informatie Teamspeler met ervaring in Agile methodiek is vereist. Als je meer informatie wilt en dit klinkt interessant voor u, aarzel dan niet om uw meest recente CV door te sturen. Het kan zijn dat ik niet beschik over uw meest recente CV en dat ik daarom u deze opportuniteit doorstuur dat niet geschikt is voor u. Als u iemand kent dat deze missie interessant zou vinden mag u deze vacature doorsturen. Met vriendelijke groeten\"\n",
        "text = \"Atcon Global - Project Management Officer / PMO team management Atcon Global For one of our clients, we are looking for an experienced Project Management Officer (PMO) / Project Manager (PM) for permanent employment in the Flanders region. Your role? As a PMO, you will play a crucial role in setting up and improving our project management processes. You will not only be responsible for developing PM standards, but also for carrying out projects independently as a Project Manager. Your duties and responsibilities will include: Developing PMO and project management standards Executing and managing complex digital projects Oversee project progress and report to senior management Follow-up of project budgets, project selection, capacity planning and resource management Coaching and training project managers Identifying and managing project risks Promote continuous improvement in the project management domain Collaborate with stakeholders and external partners Who are we looking for? Bachelor's or master's degree 5+ years in a similar role in a dynamic organization Expertise in project management methods (Agile, Scrum, Lean, Kanban) Strong analytical and problem-solving skills Excellent communication and stakeholder management Experience in team management with clear objectives Proactive, Hands-on mentality and result-oriented Fluent in Dutch and English; French is a plus What's on offer? A dynamic and varied role in a growing, ambitious and innovative company Numerous opportunities for personal growth and career development A competitive salary with customizable benefits A friendly, collegial working atmosphere Flexible working hours, possibility to work from home\"\n",
        "encoding = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# Define the device based on availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move the model to the device\n",
        "model.to(device)\n",
        "# Move encoding to the device of the model\n",
        "encoding = {k: v.to(device) for k,v in encoding.items()}\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():    # no gradients needed for inference. Forward pass\n",
        "    outputs = model(**encoding)\n",
        "\n",
        "# Get logits from the model's output\n",
        "logits = outputs.logits\n",
        "\n",
        "# Apply softmax/sigmoid based on the type of classification\n",
        "if model.config.num_labels == 1:\n",
        "    probs = torch.sigmoid(logits.squeeze())\n",
        "else:\n",
        "    #probs = torch.softmax(logits, dim=1).squeeze()\n",
        "    probs = torch.sigmoid(logits)\n",
        "\n",
        "\n",
        "\n",
        "# To get predictions\n",
        "threshold = 0.5\n",
        "#predictions = torch.where(probs >= threshold, torch.ones_like(probs), torch.zeros_like(probs))\n",
        "#predictions = torch.argmax(probs, dim=-1) if model.config.num_labels > 1 else torch.where(probs >= threshold, torch.ones_like(probs), torch.zeros_like(probs))\n",
        "predictions = (probs > threshold).float()\n",
        "print(\"Predictions:\", predictions)\n",
        "print()\n",
        "\n",
        "# Turn predicted id's into actual label names\n",
        "print(\"Probabilites:\", probs)\n",
        "\n",
        "#[id2label[idx] for idx, label in enumerate(predictions['labels']) if label == 1.0]\n",
        "\n",
        "#predicted_labels = [id2label[idx.item()] for idx in predictions]\n",
        "#print(predicted_labels)\n",
        "\n",
        "for label, prob, pred in zip(labels, probs.squeeze(), predictions.squeeze()):\n",
        "  print(f\"Label: {label}: Probability: {prob.item():.4f} {int(pred.item())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19dx2Nrsoz_d",
        "outputId": "0c202905-56e4-4e9a-e9ff-70b01bff1524"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Predictions: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "\n",
            "Probabilites: tensor([[0.2652, 0.1300, 0.1654, 0.1712, 0.1901, 0.1957, 0.1869, 0.1165, 0.1103,\n",
            "         0.0983, 0.1326, 0.1353, 0.1424, 0.1732, 0.1379, 0.2224, 0.1831, 0.1398,\n",
            "         0.0940, 0.1665, 0.1384, 0.2184, 0.1229, 0.1108, 0.1167, 0.0980, 0.1255,\n",
            "         0.1802, 0.1030, 0.1131, 0.1672, 0.1326, 0.1276, 0.1420, 0.1545, 0.1457,\n",
            "         0.1240, 0.1093, 0.1292, 0.1084, 0.1492, 0.1027, 0.1620, 0.1139, 0.1301,\n",
            "         0.1234, 0.0744, 0.1477]])\n",
            "Label: 142: Probability: 0.2652 0\n",
            "Label: 143: Probability: 0.1300 0\n",
            "Label: 146: Probability: 0.1654 0\n",
            "Label: 147: Probability: 0.1712 0\n",
            "Label: 148: Probability: 0.1901 0\n",
            "Label: 149: Probability: 0.1957 0\n",
            "Label: 150: Probability: 0.1869 0\n",
            "Label: 151: Probability: 0.1165 0\n",
            "Label: 152: Probability: 0.1103 0\n",
            "Label: 153: Probability: 0.0983 0\n",
            "Label: 154: Probability: 0.1326 0\n",
            "Label: 155: Probability: 0.1353 0\n",
            "Label: 156: Probability: 0.1424 0\n",
            "Label: 157: Probability: 0.1732 0\n",
            "Label: 158: Probability: 0.1379 0\n",
            "Label: 160: Probability: 0.2224 0\n",
            "Label: 162: Probability: 0.1831 0\n",
            "Label: 165: Probability: 0.1398 0\n",
            "Label: 167: Probability: 0.0940 0\n",
            "Label: 168: Probability: 0.1665 0\n",
            "Label: 169: Probability: 0.1384 0\n",
            "Label: 170: Probability: 0.2184 0\n",
            "Label: 171: Probability: 0.1229 0\n",
            "Label: 173: Probability: 0.1108 0\n",
            "Label: 174: Probability: 0.1167 0\n",
            "Label: 175: Probability: 0.0980 0\n",
            "Label: 176: Probability: 0.1255 0\n",
            "Label: 356: Probability: 0.1802 0\n",
            "Label: 360: Probability: 0.1030 0\n",
            "Label: 361: Probability: 0.1131 0\n",
            "Label: 362: Probability: 0.1672 0\n",
            "Label: 364: Probability: 0.1326 0\n",
            "Label: 371: Probability: 0.1276 0\n",
            "Label: 373: Probability: 0.1420 0\n",
            "Label: 375: Probability: 0.1545 0\n",
            "Label: 376: Probability: 0.1457 0\n",
            "Label: 394: Probability: 0.1240 0\n",
            "Label: 408: Probability: 0.1093 0\n",
            "Label: 409: Probability: 0.1292 0\n",
            "Label: 667: Probability: 0.1084 0\n",
            "Label: 685: Probability: 0.1492 0\n",
            "Label: 686: Probability: 0.1027 0\n",
            "Label: 689: Probability: 0.1620 0\n",
            "Label: 756: Probability: 0.1139 0\n",
            "Label: 757: Probability: 0.1301 0\n",
            "Label: 758: Probability: 0.1234 0\n",
            "Label: 760: Probability: 0.0744 0\n",
            "Label: 761: Probability: 0.1477 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oU7qJ2ocpGyF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}