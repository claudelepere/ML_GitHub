{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudelepere/ML_GitHub/blob/main/Longformer_11_1200_32_768_5_A100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cgQnuPvUFNsY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bb21f12-0cb9-46b8-a8fb-8a4cdb5efd97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m471.0/480.6 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/179.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q accelerate\n",
        "!pip install -q huggingface_hub\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q transformers datasets\n",
        "!pip install -q wandb\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "import wandb\n",
        "\n",
        "from datasets              import DatasetDict\n",
        "from google.colab          import auth, drive, files, userdata\n",
        "from huggingface_hub       import create_repo, login, upload_file\n",
        "from huggingface_hub.utils import RepositoryNotFoundError\n",
        "from sklearn.metrics       import accuracy_score, average_precision_score, classification_report, f1_score, precision_score, precision_recall_fscore_support, recall_score, roc_auc_score\n",
        "from torch.utils.data      import DataLoader\n",
        "from tqdm.auto             import tqdm\n",
        "from transformers          import AdamW, EvalPrediction, LongformerTokenizerFast, LongformerForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.nn              import BCEWithLogitsLoss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gweGUl--FNsZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a18ea4-ad4b-47fb-fd05-f629c5b92a2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "currentdir: /content\n",
            "device: cpu\n",
            "datasetDict_zip_file_name: dataset_11_1200.zip\n",
            "datasetDict_dir_name     : dataset_11_1200\n",
            "\n",
            "root          68  9.7  0.0      0     0 ?        Z    16:42   0:16 [python3] <defunct>\n",
            "root          69  0.5  0.3  66788 52144 ?        S    16:42   0:00 python3 /usr/local/bin/colab-file\n",
            "root          90  2.9  0.9 365760 119624 ?       Sl   16:42   0:04 /usr/bin/python3 /usr/local/bin/j\n",
            "root         492 32.9  8.4 5467944 1125944 ?     Ssl  16:43   0:23 /usr/bin/python3 -m colab_kernel_\n",
            "root         527  1.6  0.1 543988 18352 ?        Sl   16:43   0:01 /usr/bin/python3 /usr/local/lib/p\n",
            "root         882  0.0  0.0   7376  3512 ?        S    16:45   0:00 /bin/bash -c ps aux | grep python\n",
            "root         884  0.0  0.0   6484  2372 ?        S    16:45   0:00 grep python\n",
            "run_name                 : Longformer-multilabel-dataset_11_1200-length768-batch8x4-epochs5-lr2e-05-fp16-threshold0.2\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "# Check the Python version\n",
        "print(sys.version)\n",
        "print()\n",
        "\n",
        "# Get the installed packages (you can see that conda is not installed (do not install it))\n",
        "!pip list\n",
        "print()\n",
        "\n",
        "# Check system information\n",
        "!cat /etc/os-release\n",
        "!uname -m\n",
        "print()\n",
        "\n",
        "# Check the GPU details (only if the runtime type is T4 GPU)\n",
        "#!nvidia-smi\n",
        "#print()\n",
        "\n",
        "# Check RAM\n",
        "!free -h\n",
        "print()\n",
        "\n",
        "# Check disk space\n",
        "!df -h\n",
        "print()\n",
        "\n",
        "# Get environment variables\n",
        "for key, value in os.environ.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\"\"\"\n",
        "!python -V\n",
        "\n",
        "print(f\"currentdir: {os.getcwd()}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device: {device}\")\n",
        "\n",
        "datasetDict_zip_file_name = \"dataset_11_1200.zip\"\n",
        "datasetDict_dir_name      = os.path.splitext(datasetDict_zip_file_name)[0]\n",
        "print(f\"datasetDict_zip_file_name: {datasetDict_zip_file_name}\")\n",
        "print(f\"datasetDict_dir_name     : {datasetDict_dir_name}\")\n",
        "print()\n",
        "\n",
        "# OOM: reduce batch size\n",
        "#      small sizes (1 to 32):            PROs: better generalization in some cases\n",
        "#                                        CONs: may produce noisier gradients\n",
        "#      large sizes (128, 256, or higer): PROs: gradients are smoother, leading to more stable training\n",
        "#                                        CONs: poorer generalization (overfitting) in some cases\n",
        "#      intermediate sizes (32, 64):      combines the benefits of small and large sizes\n",
        "batch_size = 8\n",
        "\n",
        "# OOM: enable gradient accumulation to compensate for smaller batch sizes by accumulating gradients over several steps\n",
        "#      effective batch size = per-device batch size x gradient accumulation steps;\n",
        "#      in each iteration, the model computes the gradients, these gradients are immediately used to update the model parameters\n",
        "gradient_accumulation_steps = 4  #<<<<<<<<<<<<<<<<<<< gradient_accumulation_steps may not be None => comment it in TrainingArguments\n",
        "\n",
        "# OOM: use PYTORCH_CUDA_ALLOC_CONF to handle memory fragmentation\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "# OOM: check for zombie processes\n",
        "if torch.cuda.is_available():\n",
        "  !nvidia-smi\n",
        "  torch.cuda.memory_summary()\n",
        "!ps aux | grep python\n",
        "#!kill -9 <PID>\n",
        "#!nvidia-smi     # Checked if killed\n",
        "\n",
        "# OOM: use fp16 (half precision) mixed precision training\n",
        "#      reduces memory requirements by up to 50%\n",
        "fp16 = True\n",
        "\n",
        "# OOM: limit the number of GPU workers: 0 (default) or 1 in Colab\n",
        "#dataloader_num_workers = 1\n",
        "\n",
        "# OOM: reduce model size or input tokens\n",
        "#      1) LongformerTokenizer.from_pretrained('allenai/longformer-base/large-4096'): large/base: 435M/149M parameters\n",
        "#      2) max_length: 4096 max for Longformer; 1 word can give several tokens, stop words are NOT discarded!\n",
        "#         word_text_length_counts_sorted: jobs count                 : 50000\n",
        "#                                         jobs count under  512 words: 44794  89.59%\n",
        "#                                         jobs count under  640 words: 47894  95.79%\n",
        "#                                         jobs count under  768 words: 49123  98.25%\n",
        "#                                         jobs count under  896 words: 49691  99.38%\n",
        "#                                         jobs count under 1024 words: 49917  99.83%\n",
        "#                                         jobs count under 2048 words: 50000 100.00%\n",
        "#                                         jobs count under 4096 words: 50000 100.00%\n",
        "max_length = 768\n",
        "\n",
        "# OOM: free up GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# OOM: monitor GPU memory usage\n",
        "#!nividia-smi\n",
        "\n",
        "# 1 epoch is a complete pass through the entire training dataset;\n",
        "# with n datapoints and batch size = b, n/b iterations to complete 1 epoch;\n",
        "# 1 iteration is a single update of the model's parameters\n",
        "epochs = 5 #8\n",
        "\n",
        "# A common rule is to scale the learning rate proportionaly with the effective batch size\n",
        "# note: get_linear_schedule_with_warmup <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "learning_rate = 2e-5  # 1e-5 x 32/8\n",
        "\n",
        "# Reduce the number of transformers layers\n",
        "#hidden_layers = 12    # 12 (default) or 6\n",
        "\n",
        "# Threshold: 0.5 (default)\n",
        "threshold = 0.2\n",
        "\n",
        "if fp16:\n",
        "  _fp = \"fp16\"\n",
        "else:\n",
        "  _fp = \"fp32\"\n",
        "\n",
        "if 'gradient_accumulation_steps' not in globals():\n",
        "  run_name = f\"Longformer-multilabel-{datasetDict_dir_name}-length{max_length}-batch{batch_size}-epochs{epochs}-lr{learning_rate}-{_fp}-threshold{threshold}\"\n",
        "else:\n",
        "  run_name = f\"Longformer-multilabel-{datasetDict_dir_name}-length{max_length}-batch{batch_size}x{gradient_accumulation_steps}-epochs{epochs}-lr{learning_rate}-{_fp}-threshold{threshold}\"\n",
        "print(f\"run_name                 : {run_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_unzip_dataset(file_name=datasetDict_zip_file_name):\n",
        "  # Check if the file exists\n",
        "  if not os.path.exists(file_name):\n",
        "    print(f\"'{file_name}' not found in /content. Uploading...\")\n",
        "    uploaded_files = files.upload()                              # Prompt file upload dialog\n",
        "    if file_name not in uploaded_files:\n",
        "      raise FileNotFoundError(f\"'{file_name}' was not uploaded. Please try again.\")\n",
        "    print(f\"'{file_name}' successfully uploaded to /content\")\n",
        "    uploaded_file_name = list(uploaded_files.keys())[0]          # Get the name of the uploaded file\n",
        "\n",
        "    !unzip {uploaded_file_name}\n",
        "\n",
        "    unzipped_dir_name = os.path.splitext(uploaded_file_name)[0]\n",
        "    assert unzipped_dir_name==datasetDict_dir_name, \"unzipped_dir_name != datasetDict_dir_name\"\n",
        "  else:\n",
        "    print(f\"'{datasetDict_dir_name}' already exists in /content.\")\n"
      ],
      "metadata": {
        "id": "EvZTXW-_ZAJ-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upload_unzip_dataset(datasetDict_zip_file_name)"
      ],
      "metadata": {
        "id": "0k70dawNeONH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "3923ba76-c265-4ef3-899f-b3318769ad95"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'dataset_11_1200.zip' not found in /content. Uploading...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cf2f6b73-3aa6-429c-9124-9bf2615f35c0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cf2f6b73-3aa6-429c-9124-9bf2615f35c0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dataset_11_1200.zip to dataset_11_1200.zip\n",
            "'dataset_11_1200.zip' successfully uploaded to /content\n",
            "Archive:  dataset_11_1200.zip\n",
            "  inflating: dataset_11_1200/dataset_dict.json  \n",
            "  inflating: dataset_11_1200/test/data-00000-of-00001.arrow  \n",
            "  inflating: dataset_11_1200/test/dataset_info.json  \n",
            "  inflating: dataset_11_1200/test/state.json  \n",
            "  inflating: dataset_11_1200/train/data-00000-of-00001.arrow  \n",
            "  inflating: dataset_11_1200/train/dataset_info.json  \n",
            "  inflating: dataset_11_1200/train/state.json  \n",
            "  inflating: dataset_11_1200/validation/data-00000-of-00001.arrow  \n",
            "  inflating: dataset_11_1200/validation/dataset_info.json  \n",
            "  inflating: dataset_11_1200/validation/state.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face Authenticate\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")    # Store the key in os.environ\n",
        "hf_token               = os.environ.get('HF_TOKEN')\n",
        "login(token=hf_token)\n",
        "\n",
        "# Verify\n",
        "!huggingface-cli whoami"
      ],
      "metadata": {
        "id": "CDimlzpeMDyl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0ef49ae-cfb0-483f-8291-813bfabfeb1a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "claudelepere\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the skill_classification repo on the Hugging Face Hub\n",
        "\n",
        "HF_name         = \"claudelepere/skill_classification\"\n",
        "repo_id_model   = HF_name\n",
        "repo_id_dataset = HF_name\n",
        "\n",
        "repo_model_url = create_repo(\n",
        "    repo_id   = repo_id_model,\n",
        "    repo_type = \"model\",\n",
        "    private   = True,\n",
        "    exist_ok  = True\n",
        ")\n",
        "print(f\"Repo model url: {repo_model_url} created successfully as a private repo.\")\n",
        "\n",
        "repo_dataset_url = create_repo(\n",
        "    repo_id   = repo_id_dataset,\n",
        "    repo_type = \"dataset\",\n",
        "    private   = True,\n",
        "    exist_ok  = True\n",
        ")\n",
        "print(f\"Repo datasets url: {repo_dataset_url} created successfully as a private repo.\")\n",
        "\n",
        "repo_id_dataset = f\"datasets/{HF_name}\"\n",
        "\n",
        "print(f\"repo_id_model: {repo_id_model}\")\n",
        "print(f\"repo_id_dataset: {repo_id_dataset}\")"
      ],
      "metadata": {
        "id": "9lvx8Nh_Ms_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2656038b-cd98-465c-8a6f-97f4a7db6add"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repo model url: https://huggingface.co/claudelepere/skill_classification created successfully as a private repo.\n",
            "Repo datasets url: https://huggingface.co/datasets/claudelepere/skill_classification created successfully as a private repo.\n",
            "repo_id_model: claudelepere/skill_classification\n",
            "repo_id_dataset: datasets/claudelepere/skill_classification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# W&B initialization\n",
        "\n",
        "os.environ[\"WANDB_API_KEY\"] = userdata.get(\"WANDB_API_KEY\")        # Store the key in os.environ\n",
        "wandb_api_key               = os.environ.get('WANDB_API_KEY')\n",
        "wandb.login(key=wandb_api_key)\n",
        "\n",
        "try:\n",
        "  wandb.init(\n",
        "      project = \"skill_classification\",\n",
        "      name    = run_name,\n",
        "      entity  = \"claudelepere-c-cile-cy\",\n",
        "      config  = {\n",
        "          \"learning_rate\": learning_rate,\n",
        "          \"epochs\"       : 5,\n",
        "          \"batch_size\"   : batch_size\n",
        "      }\n",
        "  )\n",
        "except wandb.errors.CommError as err:\n",
        "  print(f\"CommError: {err}\")\n",
        "except Exception as exc:\n",
        "  print(f\"Exception: {exc}\")"
      ],
      "metadata": {
        "id": "JYJMUPCONEiD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "141bc485-e619-4846-cc4e-acd6e74596c4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mclaudelepere\u001b[0m (\u001b[33mclaudelepere-c-cile-cy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241230_164526-qxkqitg3</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/claudelepere-c-cile-cy/skill_classification/runs/qxkqitg3' target=\"_blank\">Longformer-multilabel-dataset_11_1200-length768-batch8x4-epochs5-lr2e-05-fp16-threshold0.2</a></strong> to <a href='https://wandb.ai/claudelepere-c-cile-cy/skill_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/claudelepere-c-cile-cy/skill_classification' target=\"_blank\">https://wandb.ai/claudelepere-c-cile-cy/skill_classification</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/claudelepere-c-cile-cy/skill_classification/runs/qxkqitg3' target=\"_blank\">https://wandb.ai/claudelepere-c-cile-cy/skill_classification/runs/qxkqitg3</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xcF4Gm8GFNsc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7ebd540-9f14-4d3e-9baf-d475d3c90e70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasetDict: <class 'datasets.dataset_dict.DatasetDict'> {'train': (960, 8), 'validation': (120, 8), 'test': (120, 8)}\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'text', '390', '135', '136', '137', '138', '139'],\n",
            "        num_rows: 960\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'text', '390', '135', '136', '137', '138', '139'],\n",
            "        num_rows: 120\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'text', '390', '135', '136', '137', '138', '139'],\n",
            "        num_rows: 120\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Create the dataset: 3 Hugging Face Dataset in a Hugging Face DatasetDict\n",
        "\n",
        "datasetDict = DatasetDict.load_from_disk(datasetDict_dir_name)\n",
        "\n",
        "print(f\"datasetDict: {type(datasetDict)} {datasetDict.shape}\\n{datasetDict}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "train, validation, test lists: the 120 first of 1200 list are not the same as the 120 list\n",
        "train_list: the 96 first of 1200 train_list are not the same as the 120 train_list\n",
        "validation_list: the 12 first of 1200 validation_list are not the same as the 120 validation_list\n",
        "test_list: the 12 first of 1200 test_list are not the same as the 120 test_list\n",
        "but\n",
        "all_list: the 120 first of 1200 list are the same as the 120 list\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "ids = datasetDict['train'].map(lambda examples: {'id': examples['id']}, batched=True)\n",
        "train_list = ids['id']\n",
        "train_list.sort(reverse=True)\n",
        "ids = datasetDict['validation'].map(lambda examples: {'id': examples['id']}, batched=True)\n",
        "validation_list = ids['id']\n",
        "validation_list.sort(reverse=True)\n",
        "ids = datasetDict['test'].map(lambda examples: {'id': examples['id']}, batched=True)\n",
        "test_list = ids['id']\n",
        "test_list.sort(reverse=True)\n",
        "\n",
        "print(f\"train_list: {type(train_list)} {len(train_list)}\\n{train_list}\")\n",
        "print(f\"validation_list: {type(validation_list)} {len(validation_list)}\\n{validation_list}\")\n",
        "print(f\"test_list: {type(test_list)} {len(test_list)}\\n{test_list}\")\n",
        "\n",
        "all_list = train_list + validation_list + test_list\n",
        "all_list.sort(reverse=True)\n",
        "print(f\"all_list: {type(all_list)} {len(all_list)}\\n{all_list}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "raise Exception(\"Stop execution here\")"
      ],
      "metadata": {
        "id": "NFvHUWPBOOI-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unjuTtKUjZI3",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "example = datasetDict['train'][0]\n",
        "print(f\"example: {type(example)} {example.keys()}\\n{example}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5eYNbH5FNsd"
      },
      "outputs": [],
      "source": [
        "# Create the label list and the id2label and label2id mappings.\n",
        "\n",
        "\"\"\"\n",
        "dataset 7_1000_125_125  ,  48 labels\n",
        "dataset 7_128_18_54     ,  42 labels\n",
        "dataset 8910_1087_68_204, 206 labels\n",
        "dataset 11_1000         ,   6 labels\n",
        "\"\"\"\n",
        "\n",
        "labels = [label for label in datasetDict['train'].features.keys() if label not in ['id', 'text']]\n",
        "labels.sort()\n",
        "print(f\"labels: {type(labels)} {len(labels)}\\n{labels}\")\n",
        "\n",
        "id2label = {idx:label for idx, label in enumerate(labels)}\n",
        "print(f\"id2label: {type(id2label)} {len(id2label)}\\n{id2label}\")\n",
        "\n",
        "label2id = {label:idx for idx, label in enumerate(labels)}\n",
        "print(f\"label2id: {type(label2id)} {len(label2id)}\\n{label2id}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model\n",
        "\n",
        "model_name = \"allenai/longformer-base-4096\"\n",
        "\n",
        "tokenizer = LongformerTokenizerFast.from_pretrained(model_name)\n",
        "\n",
        "model = LongformerForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels        = len(labels),\n",
        "#    num_hidden_layers = hidden_layers,\n",
        "    problem_type      = 'multi_label_classification')\n",
        "\n",
        "# Configure attention window size\n",
        "model.config.attention_window = 512\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "PjLO31SssqAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFWlSsbZaRLc"
      },
      "outputs": [],
      "source": [
        "# Tokenize ('input_ids' and 'attention_mask'), add 'global_attention_mask' (for Longformer), add 'labels'\n",
        "\n",
        "def preprocess_data(examples, indices):\n",
        "  # Step 1: Extract text and tokenize\n",
        "  text = examples['text']             # Batch of texts\n",
        "  encoding = tokenizer(\n",
        "      text,                           # Tokenize text\n",
        "      truncation     = True,\n",
        "      padding        = 'max_length',\n",
        "      max_length     = max_length,\n",
        "      return_tensors = 'pt'           # Return PyTorch tensors\n",
        "  )\n",
        "\n",
        "  # Step 2: Create and add the global attention mask\n",
        "  global_attention_mask             = torch.zeros_like(encoding['input_ids'])  # Initialize global attention mask with zeros (same shape as input_ids)\n",
        "  global_attention_mask[:, 0]       = 1                                        # Set global attention on the first token ([CLS], token ID=0) in each sequence\n",
        "  encoding['global_attention_mask'] = global_attention_mask                    # Add the global_attention_mask to the batch\n",
        "\n",
        "  # Step 3: Create and populate the label matrix\n",
        "\n",
        "  labels_matrix = torch.zeros((len(text), len(labels)), dtype=torch.float32)   # Create an empty label matrix\n",
        "  #print(f\"labels_matrix: {type(labels_matrix)} {labels_matrix.shape}\")\n",
        "\n",
        "  # Populate label matrix\n",
        "  for idx, label in enumerate(labels):\n",
        "    #print(f\"idx:{idx} label:{label}\")\n",
        "    if label in examples:\n",
        "      labels_matrix[:, idx] = torch.tensor(\n",
        "          [1.0 if val else 0.0 for val in examples[label]],\n",
        "          dtype=torch.float32\n",
        "          )\n",
        "  #print(f\"labels_matrix: {type(labels_matrix)} {labels_matrix.shape}\")\n",
        "\n",
        "  encoding['labels'] = labels_matrix                                           # Add labels to the encoding\n",
        "  #print(f\"encoding['labels']: {encoding['labels']}\")\n",
        "\n",
        "  return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuft8rJe2Q03"
      },
      "outputs": [],
      "source": [
        "# Create the 3 encoded datasets, train, validation and test\n",
        "\n",
        "encoded_dataset = datasetDict.map(\n",
        "    preprocess_data,\n",
        "    batched        = True,\n",
        "    remove_columns = datasetDict['train'].column_names,\n",
        "    with_indices   = True\n",
        "    )\n",
        "train_dataset      = encoded_dataset['train']\n",
        "validation_dataset = encoded_dataset['validation']\n",
        "test_dataset       = encoded_dataset['test']\n",
        "print(f\"encoded_dataset: {type(encoded_dataset)} {encoded_dataset.shape}\\n{encoded_dataset}\")\n",
        "print(f\"train_dataset: {type(train_dataset)} {train_dataset.shape}\")\n",
        "print(f\"validation_dataset: {type(validation_dataset)} {validation_dataset.shape}\")\n",
        "print(f\"test_dataset['labels']: {type(test_dataset['labels'])} {len(test_dataset['labels'])}\\n{test_dataset['labels']}\")\n",
        "\n",
        "print(f\"train_dataset[0]['input_ids']: {type(train_dataset[0]['input_ids'])} {len(train_dataset[0]['input_ids'])}\\n{train_dataset['input_ids'][0]}\")\n",
        "print(f\"train_dataset[0]['attention_mask']: {type(train_dataset[0]['attention_mask'])} {len(train_dataset[0]['attention_mask'])}\\n{train_dataset['attention_mask'][0]}\")\n",
        "print(f\"train_dataset[0]['global_attention_mask']: {type(train_dataset[0]['global_attention_mask'])} {len(train_dataset[0]['global_attention_mask'])}\\n{train_dataset['global_attention_mask'][0]}\")\n",
        "\n",
        "print(f\"train_dataset[0]['labels']: {type(train_dataset[0]['labels'])} {len(train_dataset[0]['labels'])}\\n{train_dataset[0]['labels']}\")\n",
        "print(f\"train_dataset['labels'][0]: {type(train_dataset['labels'][0])} {len(train_dataset['labels'][0])}\\n{train_dataset['labels'][0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Truncated part\n",
        "\n",
        "def get_truncated_part(text):\n",
        "  tokens = tokenizer(\n",
        "      text,\n",
        "      truncation                = True,\n",
        "      padding                   = 'max_length',\n",
        "      max_length                = max_length,\n",
        "      return_overflowing_tokens = True,\n",
        "      return_tensors            = None\n",
        "      )\n",
        "  print(f\"tokens.keys(): {tokens.keys()}\")\n",
        "\n",
        "  # Get the truncated tokens\n",
        "  truncated_ids = tokens[\"input_ids\"][0]\n",
        "  print(f\"truncated_ids: {type(truncated_ids)} {truncated_ids}\")\n",
        "  #overflow_ids  = tokens[\"overflow_to_sample_mapping\"][0]\n",
        "  #print(f\"overflow_ids: {type(overflow_ids)} {overflow_ids}\")\n",
        "\n",
        "  # Decode the tokens back to text\n",
        "  truncated_text = tokenizer.decode(truncated_ids, skip_special_tokens=True)\n",
        "  #overflow_text  = tokenizer.decode(overflow_ids, skip_special_tokens=True)\n",
        "\n",
        "  print(f\"original_text :\\n{text}\")\n",
        "  print(f\"truncated_text:\\n{truncated_text}\")\n",
        "  #print(f\"overflow_text:\\n{overflow_text}\")\n",
        "\n",
        "  original_tokens  = tokenizer.tokenize(text)\n",
        "  truncated_tokens = tokenizer.tokenize(truncated_text)\n",
        "  #overflow_tokens  = tokenizer.tokenize(overflow_text)\n",
        "\n",
        "  print(f\"original_tokens count : {len(original_tokens)}\")\n",
        "  print(f\"truncated_tokens count: {len(truncated_tokens)}\")\n",
        "  #print(f\"overflow_tokens count: {len(overflow_tokens)}\")\n"
      ],
      "metadata": {
        "id": "brkRdqdjN-Ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = datasetDict['train'][0]['text']\n",
        "get_truncated_part(example_text)\n",
        "\n",
        "inputs = tokenizer(\n",
        "    example_text,\n",
        "    truncation     = True,\n",
        "    padding        = 'max_length',\n",
        "    max_length     = max_length,\n",
        "    return_tensors = 'pt'\n",
        ")\n",
        "\n",
        "#print(f\"inputs: {type(inputs)} {inputs.keys()}\\n{inputs}\")\n",
        "#print(f\"inputs_ids: {type(inputs.input_ids)} {inputs.input_ids.shape}\\n{inputs.input_ids}\")\n",
        "#print(f\"attention_mask: {type(inputs.attention_mask)} {inputs.attention_mask.shape}\\n{inputs.attention_mask}\")\n",
        "#print(f\"token_type_ids: {inputs.token_type_ids.shape}\")\n",
        "#print(f\"labels: {inputs.labels.shape}\")\n"
      ],
      "metadata": {
        "id": "DD_bjwTfRiQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Forward pass for multi-label classification\n",
        "\n",
        "outputs = model(\n",
        "    input_ids      = inputs.input_ids,\n",
        "    attention_mask = inputs.attention_mask\n",
        "    )\n",
        "\n",
        "print(f\"outputs: {type(outputs)} {outputs.keys()}\\n{outputs}\")\n",
        "\n",
        "# Logits (= raw model outputs)\n",
        "logits = outputs.logits\n",
        "print(f\"logits: {type(logits)} {logits.shape}\\n{logits}\")\n",
        "\n",
        "# Convert logits to probabilities\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "probs   = sigmoid(logits)\n",
        "print(f\"probs: {type(probs)} {probs.shape}\\n{probs}\")"
      ],
      "metadata": {
        "id": "sMscqNTXuY8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0enAb0W9o25W",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "example = encoded_dataset['train'][0]\n",
        "\n",
        "#print(f\"example: {type(example)} {example.keys()}\\n{example}\")\n",
        "#print()\n",
        "#print(f\"example['input_ids']: {type(example['input_ids'])} {len(example['input_ids'])}\\n{example['input_ids']}\")\n",
        "##print(f\"example['token_type_ids']: {type(example['token_type_ids'])} {len(example['token_type_ids'])}\\n{example['token_type_ids']}\")\n",
        "#print(f\"example['attention_mask']: {type(example['attention_mask'])} {len(example['attention_mask'])}\\n{example['attention_mask']}\")\n",
        "#print(f\"example['labels']:  {type(example['labels'])} {len(example['labels'])}\\n{example['labels']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0McCtJ8HRJY",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(example['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LAyThO7Jnvj",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4ENBTdulBEI",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# Set PyTorch format to ensures correctness and compatibility with PyTorch pipelines\n",
        "\n",
        "# The 3 Hugging Face Dataset are formatted as PyTorch Dataset\n",
        "encoded_dataset.set_format('torch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5a8_vIKqr7P"
      },
      "outputs": [],
      "source": [
        "batch_size  = batch_size\n",
        "metric_name = \"f1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dR2GmpvDqbuZ",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir                  = './training_results',  # where model predictions and checkpoints will be written during training\n",
        "    overwrite_output_dir        = True,\n",
        "    logging_dir                 = './logs',\n",
        "    logging_steps               = 50,\n",
        "    save_steps                  = 500,\n",
        "    save_total_limit            = 2,\n",
        "    eval_strategy               = 'epoch',\n",
        "    save_strategy               = 'epoch',\n",
        "    learning_rate               = learning_rate,\n",
        "    per_device_train_batch_size = batch_size,\n",
        "    per_device_eval_batch_size  = batch_size,\n",
        "    gradient_accumulation_steps = gradient_accumulation_steps,\n",
        "    num_train_epochs            = epochs,\n",
        "    weight_decay                = 0.01,\n",
        "    load_best_model_at_end      = True,\n",
        "    metric_for_best_model       = metric_name,\n",
        "    run_name                    = run_name,\n",
        "    fp16                        = fp16,\n",
        "    #dataloader_num_workers      = dataloader_num_workers,\n",
        "    report_to                  = 'wandb'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LaQCQoJFNsi",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "# Metrics\n",
        "#   source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
        "\n",
        "def multi_label_metrics(predictions, labels):\n",
        "    average = 'micro'    # 'micro' or 'weighted'\n",
        "\n",
        "    # first, apply sigmoid on predictions whose shape is (batch_size, num_labels)\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs   = sigmoid(torch.Tensor(predictions))\n",
        "\n",
        "    # next, use threshold to turn them into integer predictions\n",
        "    y_pred = np.zeros(probs.shape)\n",
        "    y_pred[np.where(probs >= threshold)] = 1\n",
        "\n",
        "    # finally, compute metrics\n",
        "    y_true               = labels\n",
        "    f1                   = f1_score               (y_true=y_true, y_pred=y_pred, average=average)    #, zero_division=1)\n",
        "    precision            = precision_score        (y_true=y_true, y_pred=y_pred, average=average)    #, zero_division=1)\n",
        "    recall               = recall_score           (y_true=y_true, y_pred=y_pred, average=average)    #, zero_division=1)\n",
        "    roc_auc              = roc_auc_score          (y_true=y_true, y_score=probs, average=average)\n",
        "    precision_recall_auc = average_precision_score(y_true=y_true, y_score=probs, average=average)\n",
        "    accuracy             = accuracy_score         (y_true=y_true, y_pred=y_pred)\n",
        "\n",
        "    # return as dictionary\n",
        "    metrics = {\n",
        "        'f1'                  : f1,\n",
        "        'precision'           : precision,\n",
        "        'recall'              : recall,\n",
        "        'roc_auc'             : roc_auc,\n",
        "        'precision_recall_auc': precision_recall_auc,\n",
        "        'accuracy'            : accuracy\n",
        "        }\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "797b2WHJqUgZ"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
        "    result = multi_label_metrics(\n",
        "        predictions = preds,\n",
        "        labels      = p.label_ids\n",
        "        )\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxNo4_TsvzDm"
      },
      "source": [
        "Let's verify a batch as well as a forward pass:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adTwB7XvFNsj"
      },
      "outputs": [],
      "source": [
        "print(f\"inputids:        {type(encoded_dataset['train']['input_ids'][0])}\\t{encoded_dataset['train']['input_ids'][0].shape}\")\n",
        "#print(f\"token_type_ids': {type(encoded_dataset['train']['token_type_ids'][0])}\\t{encoded_dataset['train']['token_type_ids'][0].shape}\")\n",
        "print(f\"attention_mask:  {type(encoded_dataset['train']['attention_mask'][0])}\\t{encoded_dataset['train']['attention_mask'][0].shape}\")\n",
        "print(f\"labels:          {type(encoded_dataset['train'][0]['labels'])}\\t{encoded_dataset['train'][0]['labels'].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxWcnZ8ku12V",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# Execute a forward pass for debugging or verification purposes (cf. BERT_3_1 in Notion BERT database)\n",
        "\n",
        "outputs = model(\n",
        "    input_ids      = encoded_dataset['train']['input_ids'][0].unsqueeze(0),\n",
        "    attention_mask = encoded_dataset['train']['attention_mask'][0].unsqueeze(0),\n",
        "    labels         = encoded_dataset['train'][0]['labels'].unsqueeze(0)\n",
        "    )\n",
        "\n",
        "print(f\"outputs: {type(outputs)} {outputs.keys()}\\n{outputs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chq_3nUz73ib"
      },
      "outputs": [],
      "source": [
        "# Create the trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset = encoded_dataset[\"train\"],\n",
        "    eval_dataset  = encoded_dataset[\"validation\"],\n",
        "    tokenizer     = tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train, save the results as a JSON file\n",
        "\n",
        "train_output  = trainer.train()\n",
        "\n",
        "train_results = {\n",
        "    'global_step':   train_output.global_step,    # total steps completed during training\n",
        "    'training_loss': train_output.training_loss,  # average loss during training\n",
        "    'metrics':       train_output.metrics         # dictionary of metrics\n",
        "}\n",
        "\n",
        "# Save train results\n",
        "with open(\"train_results.json\", \"w\") as f:\n",
        "  json.dump(train_results, f, indent=4)\n",
        "print(f\"train_results: {type(train_results)} {len(train_results)}\\n{train_results}\")"
      ],
      "metadata": {
        "id": "HvhT1c_h_oul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training successfully completed.\")"
      ],
      "metadata": {
        "id": "I2SMR-lI8Y8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiloh9eMK91o"
      },
      "source": [
        "## Evaluate\n",
        "\n",
        "After training, we evaluate our model on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_results(model, dataset, batch_size, threshold):\n",
        "  # Clear GPU cache\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  # Set the model to evaluation mode to disable dropout and other training-specific behaviors\n",
        "  model.eval()\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "\n",
        "  test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  all_preds       = []\n",
        "  all_probs       = []\n",
        "  all_true_labels = []\n",
        "\n",
        "  for batch in tqdm(test_loader):\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "      outputs = model(**batch)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Convert logits to probabilities and probabilities to predictions\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs   = sigmoid(logits).cpu().numpy()    # Convert to Numpy\n",
        "    preds   = (probs > threshold).astype(int)  # Convert to binary Numpy array\n",
        "\n",
        "    # Accumulate probabilities, predictions and labels\n",
        "    all_probs.append(probs)\n",
        "    all_preds.append(preds)\n",
        "    all_true_labels.append(batch['labels'].cpu().numpy())\n",
        "\n",
        "  # Concatenate results from all batches\n",
        "  all_probs       = np.concatenate(all_probs, axis=0)        # shape: [num_samples, num_labels]\n",
        "  all_preds       = np.concatenate(all_preds, axis=0)        # shape: [num_samples, num_labels]\n",
        "  all_true_labels = np.concatenate(all_true_labels, axis=0)  # shape: [num_samples, num_labels]\n",
        "\n",
        "  print(f\"all_probs:       {type(all_probs)} {all_probs.shape}\")\n",
        "  print(f\"all_preds:       {type(all_preds)} {all_preds.shape}\")\n",
        "  print(f\"all_true_labels: {type(all_true_labels)} {all_true_labels.shape}\")\n",
        "\n",
        "  # Classification report for precision, recall, F1 score\n",
        "  print(classification_report(\n",
        "      y_true        = all_true_labels,\n",
        "      y_pred        = all_preds,\n",
        "      target_names  = labels,\n",
        "      zero_division = 0\n",
        "      ))\n",
        "\n",
        "  # ROC AUC for multi-label classification\n",
        "  roc_auc = roc_auc_score(\n",
        "      y_true  = all_true_labels,\n",
        "      y_score = all_probs,\n",
        "      average = 'micro'\n",
        "      )\n",
        "  print(f\"ROC AUC: {roc_auc}\")"
      ],
      "metadata": {
        "id": "gS9EbjVmVpjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First evaluate results NO SAVE\n",
        "\n",
        "get_results(model=model, dataset=validation_dataset, batch_size=batch_size, threshold=threshold)"
      ],
      "metadata": {
        "id": "hoJpTASkY8De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First evaluation successfully completed.\")"
      ],
      "metadata": {
        "id": "pRVTBgrPVJ8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMlebJ83LRYG"
      },
      "outputs": [],
      "source": [
        "# Second evaluate results; save to /content\n",
        "\n",
        "eval_output = trainer.evaluate()\n",
        "\n",
        "# Save evaluate results\n",
        "with open(\"eval_results.json\", \"w\") as f:\n",
        "  json.dump(eval_output, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QwshIQGSFxg"
      },
      "outputs": [],
      "source": [
        "print(\"Second evaluation successfully completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload model, tokenizer, train results, evaluate results"
      ],
      "metadata": {
        "id": "a55QdBEkYhJv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfPiHh62FNsl"
      },
      "outputs": [],
      "source": [
        "# Save model to /content\n",
        "\n",
        "model_path = \"model\"\n",
        "trainer.save_model(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSAz3pspFNsm"
      },
      "outputs": [],
      "source": [
        "# Upload model and tokenizer to the HF repo_id_model\n",
        "\n",
        "tokenizer = LongformerTokenizerFast.from_pretrained(model_path)\n",
        "model     = LongformerForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "tokenizer.push_to_hub(repo_id_model)\n",
        "model.push_to_hub(repo_id_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYmURgAz4Xk-"
      },
      "outputs": [],
      "source": [
        "# Upload train_results.json and eval_results.json to the HF repo_id_dataset BETTER to upload to wanddb repo?\n",
        "\n",
        "upload_file(\n",
        "    path_or_fileobj = \"train_results.json\",\n",
        "    path_in_repo    = \"train_results.json\",\n",
        "    repo_id         = HF_name,\n",
        "    repo_type       = \"dataset\"\n",
        "    )\n",
        "\n",
        "upload_file(\n",
        "    path_or_fileobj = \"eval_results.json\",\n",
        "    path_in_repo    = \"eval_results.json\",\n",
        "    repo_id         = HF_name,\n",
        "    repo_type       = \"dataset\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "pwO6_V_lYcJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test: first results NO SAVE\n",
        "\n",
        "get_results(model=model, dataset=test_dataset, batch_size=batch_size, threshold=threshold)"
      ],
      "metadata": {
        "id": "WPHcZrdsEARs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First test successfully completed.\")"
      ],
      "metadata": {
        "id": "yibAPgTaVljm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test: second results NO SAVE\n",
        "\n",
        "predictions = trainer.predict(test_dataset)\n",
        "\n",
        "#print(f\"predictions.predictions: {type(predictions.predictions)} {predictions.predictions.shape}\\n{predictions.predictions}\")  # Model logits\n",
        "#print(f\"predictions.label_ids: {type(predictions.label_ids)} {predictions.label_ids.shape}\\n{predictions.label_ids}\")          # Ground truth labels\n",
        "print(f\"predictions.metrics: {type(predictions.metrics)} {len(predictions.metrics)}\\n{predictions.metrics}\")                  # Metrics\n"
      ],
      "metadata": {
        "id": "39vYrXt4HSaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Second test successfully completed.\")"
      ],
      "metadata": {
        "id": "_F1Y8Hve8qsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Or otherwise"
      ],
      "metadata": {
        "id": "CPsOjc1s8bSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test: third results NO SAVE\n",
        "\n",
        "predictions = trainer.predict(test_dataset)\n",
        "#print(predictions.predictions)  # Model logits\n",
        "#print(predictions.label_ids)    # Ground truth labels\n",
        "print(predictions.metrics)      # Metrics"
      ],
      "metadata": {
        "id": "6C8KYLfNgMWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Third test successfully completed.\")"
      ],
      "metadata": {
        "id": "0lCtbXsf8zsZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "jupytext": {
      "formats": "ipynb,py:nomarker"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}