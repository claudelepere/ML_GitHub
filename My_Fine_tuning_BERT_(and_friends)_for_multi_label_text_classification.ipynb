{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudelepere/ML_GitHub/blob/main/My_Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgQnuPvUFNsY",
        "outputId": "f2d9acc2-c710-470f-dd36-cb971cc17575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.18.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "# Do not try to install with conda in Colab\n",
        "!pip install -q huggingface_hub\n",
        "!pip install -q transformers datasets\n",
        "!pip install wandb\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import wandb\n",
        "\n",
        "from datasets              import DatasetDict\n",
        "from google.colab          import auth, files, userdata\n",
        "from huggingface_hub       import create_repo, HfApi, login, upload_file\n",
        "from huggingface_hub.utils import RepositoryNotFoundError\n",
        "from sklearn.metrics       import accuracy_score, average_precision_score, f1_score, precision_score, recall_score, roc_auc_score\n",
        "from transformers          import AutoModelForSequenceClassification, AutoTokenizer, EvalPrediction, Trainer, TrainingArguments\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gweGUl--FNsZ",
        "outputId": "615ca83f-5bdc-4547-ac35-94c5d4835bf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "currentdir: /content\n",
            "\n",
            "device: cpu\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "# Check the Python version\n",
        "print(sys.version)\n",
        "print()\n",
        "\n",
        "# Get the installed packages (you can see that conda is not installed (do not install it))\n",
        "!pip list\n",
        "print()\n",
        "\n",
        "# Check system information\n",
        "!cat /etc/os-release\n",
        "!uname -m\n",
        "print()\n",
        "\n",
        "# Check the GPU details (only if the runtime type is T4 GPU)\n",
        "#!nvidia-smi\n",
        "#print()\n",
        "\n",
        "# Check RAM\n",
        "!free -h\n",
        "print()\n",
        "\n",
        "# Check disk space\n",
        "!df -h\n",
        "print()\n",
        "\n",
        "# Get environment variables\n",
        "for key, value in os.environ.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\"\"\"\n",
        "print(f\"currentdir: {os.getcwd()}\")\n",
        "print()\n",
        "\n",
        "# Check the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Google Colab authentication\n",
        "\n",
        "# Required for accessing Colab Secrets\n",
        "auth.authenticate_user()\n"
      ],
      "metadata": {
        "id": "cFalQQ0Agl_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Hugging Face authentications\n",
        "\n",
        "login(token=userdata.get('HF_TOKEN'))\n",
        "\n",
        "# Verify\n",
        "!huggingface-cli whoami\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZZ-tVIaGdqI",
        "outputId": "ffb36475-ecb1-4e11-e80a-d776b46905fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "claudelepere\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Create the skill_classification repo on the Hugging Face Hub\n",
        "\n",
        "\"\"\"\n",
        "Save anything related to the FINAL artifacts of the project:\n",
        "- Model weights (e.g., pytorch_model.bin or tf_model_h5)\n",
        "- Tokenizer and vocabulary files (e.g., tokenizer.json, vocal.txt)\n",
        "- Config files (e.g., config.json, preprocessor.json)\n",
        "- Datasets for training and inference (if small or public)\n",
        "- Model cards describing the model (e.g., README.md)\n",
        "\"\"\"\n",
        "\n",
        "name            = \"claudelepere/skill_classification\"\n",
        "repo_id_model   = name\n",
        "repo_id_dataset = name\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "repo_model_url = create_repo(\n",
        "    repo_id   = repo_id_model,\n",
        "    repo_type = \"model\",\n",
        "    private   = True,\n",
        "    exist_ok  = True\n",
        "    )\n",
        "print(f\"Repo model url: {repo_model_url} created successfully as a private repo.\")\n",
        "\n",
        "repo_dataset_url = create_repo(\n",
        "    repo_id   = repo_id_dataset,\n",
        "    repo_type = \"dataset\",\n",
        "    private   = True,\n",
        "    exist_ok  = True\n",
        "    )\n",
        "print(f\"Repo datasets url: {repo_dataset_url} created successfully as a private repo.\")\n",
        "\n",
        "repo_id_dataset = f\"datasets/{name}\"\n",
        "\n",
        "print(f\"repo_id_model: {repo_id_model}\")\n",
        "print(f\"repo_id_dataset: {repo_id_dataset}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHvQQaaK9Bzo",
        "outputId": "1b8d84c8-9b10-470a-d4e5-613a75276299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repo model url: https://huggingface.co/claudelepere/skill_classification created successfully as a private repo.\n",
            "Repo datasets url: https://huggingface.co/datasets/claudelepere/skill_classification created successfully as a private repo.\n",
            "repo_id_model: claudelepere/skill_classification\n",
            "repo_id_dataset: datasets/claudelepere/skill_classification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Weights & Biases (W&B, wandb) authentication\n",
        "\n",
        "\"\"\"\n",
        "Track the training process and experiment management:\n",
        "- Metrics: Log training and validation loss, accuracy, F1 scores, etc.\n",
        "- Hyperparameters: save configurations for the experiments (learning rate, batch size)\n",
        "- Training artifacts:\n",
        "  - Intermediate model checkpoints\n",
        "  - Training logs or charts\n",
        "- Data versions: save different datasets splits\n",
        "\"\"\"\n",
        "\n",
        "wandb.login(key=userdata.get('WANDB_API_KEY'))\n",
        "wandb.init(project=\"skill_classification\", entity=\"claudelepere\")"
      ],
      "metadata": {
        "id": "o4f6l4kBhAGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLB3I4FKZ5Lr"
      },
      "source": [
        "# My fine-tuning BERT (and friends) for multi-label text classification\n",
        "\n",
        "In this notebook, we are going to fine-tune BERT to predict one or more labels for a given piece of text. Note that this notebook illustrates how to fine-tune a bert-base-uncased model, but you can also fine-tune a RoBERTa, DeBERTa, DistilBERT, CANINE, ... checkpoint in the same way.\n",
        "\n",
        "All of those work in the same way: they add a **linear layer on top of the base model, which is used to produce a tensor of shape (batch_size, num_labels)**, indicating the unnormalized scores for a number of labels for every example in the batch.\n",
        "\n",
        "\n",
        "\n",
        "## Set-up environment\n",
        "\n",
        "First, we install the libraries which we'll use: HuggingFace Transformers and Datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIH9NP0MZ6-O"
      },
      "source": [
        "## Load dataset\n",
        "\n",
        "Next, let's download a multi-label text classification dataset from the [hub](https://huggingface.co/).\n",
        "\n",
        "At the time of writing, I picked a random one as follows:   \n",
        "\n",
        "* first, go to the \"datasets\" tab on huggingface.co\n",
        "* next, select the \"multi-label-classification\" tag on the left as well as the the \"1k<10k\" tag (fo find a relatively small dataset).\n",
        "\n",
        "Note that you can also easily load your local data (i.e. csv files, txt files, Parquet files, JSON, ...) as explained [here](https://huggingface.co/docs/datasets/loading.html#local-and-remote-files).\n",
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "yskdGyNJFNsa"
      },
      "source": [
        "### Upload datasetHF_128_18_54.zip or datasetHF_1000_125_125.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sd1LiXGjZ420",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "### Upload and unzip the dataset zip file\n",
        "\n",
        "uploaded_files     = files.upload()\n",
        "uploaded_file_name = list(uploaded_files.keys())[0]\n",
        "print(f\"uploaded_file_name: {uploaded_file_name} {len(uploaded_files)}\")\n",
        "\n",
        "!unzip {uploaded_file_name}\n",
        "\n",
        "unzipped_file_dir_name = os.path.splitext(uploaded_file_name)[0]\n",
        "print(f\"unzipped_file_dir_name: {unzipped_file_dir_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcF4Gm8GFNsc"
      },
      "outputs": [],
      "source": [
        "### Create the dataset\n",
        "\n",
        "dataset = DatasetDict.load_from_disk(unzipped_file_dir_name)\n",
        "\n",
        "print(f\"dataset: {type(dataset)} {dataset.shape}\\n{dataset}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCL02vQgxYTO"
      },
      "source": [
        "As we can see, the dataset contains 3 splits: one for training, one for validation and one for testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5qgMAhrlql-"
      },
      "source": [
        "Let's test the first example of the training split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unjuTtKUjZI3",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "example = dataset['train'][0]\n",
        "print(f\"example: {type(example)} {example.keys()}\\n{example}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5eYNbH5FNsd"
      },
      "outputs": [],
      "source": [
        "### Create the label list and the id2label and label2id mappings.\n",
        "\n",
        "\"\"\"\n",
        "labels\n",
        "    if dataset 7_1000_125_125  , 48 labels\n",
        "    if dataset 7_128_18_54     , 42 labels\n",
        "    if dataset 8910_1087_68_204, 206 labels\n",
        "\"\"\"\n",
        "\n",
        "labels = [label for label in dataset['train'].features.keys() if label not in ['id', 'text']]\n",
        "labels.sort()\n",
        "print(f\"labels: {type(labels)} {len(labels)}\\n{labels}\")\n",
        "\n",
        "id2label = {idx:label for idx, label in enumerate(labels)}\n",
        "print(f\"id2label: {type(id2label)} {len(id2label)}\\n{id2label}\")\n",
        "\n",
        "label2id = {label:idx for idx, label in enumerate(labels)}\n",
        "print(f\"label2id: {type(label2id)} {len(label2id)}\\n{label2id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdkFBRCcl5dU"
      },
      "source": [
        "The dataset consists of texts, labeled with one or more skills.\n",
        "\n",
        "Let's create a list that contains the labels, as well as 2 dictionaries that map labels to integers and back."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGtb9Op-FNsf"
      },
      "outputs": [],
      "source": [
        "### Upload the label list as a JSON file on the HF repo\n",
        "\n",
        "labels_path = \"labels.json\"\n",
        "with open(labels_path, 'w') as f:\n",
        "    json.dump(labels, f)\n",
        "print(f\"labels saved to {labels_path}\")\n",
        "\n",
        "repo_labels_path  = \"labels.json\"\n",
        "upload_file(\n",
        "    path_or_fileobj = labels_path,\n",
        "    path_in_repo    = repo_labels_path,\n",
        "    repo_id         = repo_id_labels,\n",
        "    repo_type       = \"dataset\"\n",
        "    )\n",
        "print(f\"labels uploaded to https://huggingface.co/datasets/{repo_id_labels}/tree/main/{repo_labels_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ3Teyjmank2"
      },
      "source": [
        "## Preprocess data\n",
        "\n",
        "As models like BERT don't expect text as direct input, but rather **`input_ids`**, etc., we tokenize the text using the tokenizer. Here I'm using the `AutoTokenizer` API, which will automatically load the appropriate tokenizer based on the checkpoint on the hub.\n",
        "\n",
        "What's a bit tricky is that we also need to provide labels to the model. For multi-label text classification, this is a **matrix of shape (batch_size, num_labels)**. Also important: this should be a tensor of floats rather than integers, otherwise PyTorch' **BCEWithLogitsLoss** (which the model will use) will complain, as explained [here](https://discuss.pytorch.org/t/multi-label-binary-classification-result-type-float-cant-be-cast-to-the-desired-output-type-long/117915/3)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJCqsGCLFNsg"
      },
      "source": [
        "### Preprocess (examples, not example, because batched=True => examples is a batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "guGi7K-DFNsg"
      },
      "outputs": [],
      "source": [
        "### Tokenize 'text'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFWlSsbZaRLc"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(examples, indices):\n",
        "  text = examples['text']    # Batch of texts\n",
        "\n",
        "  encoding = tokenizer(\n",
        "      text,                             # Tokenize text\n",
        "      truncation     = True,\n",
        "      padding        = 'max_length',\n",
        "      max_length     = 512,\n",
        "      return_tensors = 'pt'             # Return PyTorch tensors\n",
        "      )\n",
        "\n",
        "  # Create an empty label matrix\n",
        "  labels_matrix = torch.zeros((len(text), len(labels)), dtype=torch.float32)\n",
        "  #print(f\"labels_matrix: {type(labels_matrix)} {labels_matrix.shape}\")\n",
        "\n",
        "  # Populate label matrix\n",
        "  for idx, label in enumerate(labels):\n",
        "    #print(f\"idx:{idx} label:{label}\")\n",
        "    if label in examples:\n",
        "      labels_matrix[:, idx] = torch.tensor(\n",
        "          [1.0 if val else 0.0 for val in examples[label]],\n",
        "          dtype=torch.float32\n",
        "          )\n",
        "  #print(f\"labels_matrix: {type(labels_matrix)} {labels_matrix.shape}\")\n",
        "\n",
        "  # Add labels to the encoding\n",
        "  encoding['labels'] = labels_matrix\n",
        "  #print(f\"encoding['labels']: {encoding['labels']}\")\n",
        "\n",
        "  return encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuft8rJe2Q03"
      },
      "outputs": [],
      "source": [
        "### Create the encoded dataset\n",
        "\n",
        "encoded_dataset = dataset.map(\n",
        "    preprocess_data,\n",
        "    batched        = True,\n",
        "    remove_columns = dataset['train'].column_names,\n",
        "    with_indices   = True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0enAb0W9o25W",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "example = encoded_dataset['validation'][0]\n",
        "print(f\"example['labels']:  {type(example['labels'])} {len(example['labels'])}\\n{example['labels']}\")\n",
        "print(f\"example.keys(): {example.keys()}\")\n",
        "print(f\"example['input_ids']: {example['input_ids']}\")\n",
        "print(f\"example['token_type_ids']: {example['token_type_ids']}\")\n",
        "print(f\"example['attention_mask']: {example['attention_mask']}\")\n",
        "print(f\"example['labels']: {example['labels']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0McCtJ8HRJY",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(example['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4Dx95t2o6N9",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "example['labels']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LAyThO7Jnvj",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgpKXDfvKBxn"
      },
      "source": [
        "Finally, we set the format of our data to PyTorch tensors. This will turn the training, validation and test sets into standard PyTorch [datasets](https://pytorch.org/docs/stable/data.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4ENBTdulBEI",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "encoded_dataset.set_format(\"torch\")    # Ensures correctness and compatibility with PyTorch pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5qSmCgWefWs"
      },
      "source": [
        "## Define model\n",
        "\n",
        "Here we define a **model that includes a pre-trained base (i.e. the weights from bert-base-uncased) are loaded, with a random initialized classification head (linear layer) on top**. One should fine-tune this head, together with the pre-trained base on a labeled dataset.\n",
        "\n",
        "This is also printed by the warning.\n",
        "\n",
        "We set the `problem_type` to be \"multi_label_classification\", as this will make sure the appropriate loss function is used (namely [**BCEWithLogitsLoss**](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)). We also make sure the output layer has `len(labels)` output neurons, and we set the id2label and label2id mappings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XPL1Z_RegBF",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "### Define the model\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    problem_type=\"multi_label_classification\",\n",
        "    num_labels=len(labels),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjJGEXShp7te"
      },
      "source": [
        "## Train the model!\n",
        "\n",
        "We are going to train the model using HuggingFace's Trainer API. This requires us to define 2 things:\n",
        "\n",
        "* `TrainingArguments`, which specify training hyperparameters. All options can be found in the [docs](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments). Below, we for example specify that we want to evaluate after every epoch of training, we would like to save the model every epoch, we set the learning rate, the batch size to use for training/evaluation, how many epochs to train for, and so on.\n",
        "* a `Trainer` object (docs can be found [here](https://huggingface.co/transformers/main_classes/trainer.html#id1))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5a8_vIKqr7P"
      },
      "outputs": [],
      "source": [
        "batch_size  = 8\n",
        "metric_name = \"f1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3s32t3tFNsi"
      },
      "source": [
        "### TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dR2GmpvDqbuZ",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir                  = \"/content/results/output\",\n",
        "    overwrite_output_dir        = True,\n",
        "    logging_dir                 = \"/content/results/logs\",\n",
        "    logging_steps               = 50,\n",
        "    save_steps                  = 100,\n",
        "    save_total_limit            = 2,\n",
        "    eval_strategy               = \"epoch\",\n",
        "    save_strategy               = \"epoch\",\n",
        "    learning_rate               = 2e-5,\n",
        "    per_device_train_batch_size = batch_size,\n",
        "    per_device_eval_batch_size  = batch_size,\n",
        "    num_train_epochs            = 5,\n",
        "    weight_decay                = 0.01,\n",
        "    load_best_model_at_end      = True,\n",
        "    metric_for_best_model       = metric_name,\n",
        "    #push_to_hub                 = True,\n",
        "    run_name                   = \"BERT-multilabel-lr2e5-epochs5-datasetHF_128_18_54\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_v2fPFFJ3-v"
      },
      "source": [
        "We are also going to compute metrics while training. For this, we need to define a `compute_metrics` function, that returns a dictionary with the desired metric values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "7LaQCQoJFNsi"
      },
      "outputs": [],
      "source": [
        "### Metrics\n",
        "\n",
        "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
        "def multi_label_metrics(predictions, labels, threshold=0.2):\n",
        "    _average = 'micro'    # 'micro' or 'weighted'\n",
        "\n",
        "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs   = sigmoid(torch.Tensor(predictions))\n",
        "\n",
        "    # next, use threshold to turn them into integer predictions\n",
        "    y_pred = np.zeros(probs.shape)\n",
        "    y_pred[np.where(probs >= threshold)] = 1\n",
        "\n",
        "    # finally, compute metrics\n",
        "    y_true               = labels\n",
        "    f1                   = f1_score               (y_true=y_true, y_pred=y_pred, average=_average)    #, zero_division=1)\n",
        "    precision            = precision_score        (y_true=y_true, y_pred=y_pred, average=_average)    #, zero_division=1)\n",
        "    recall               = recall_score           (y_true=y_true, y_pred=y_pred, average=_average)    #, zero_division=1)\n",
        "    roc_auc              = roc_auc_score          (y_true=y_true, y_score=probs, average=_average)\n",
        "    precision_recall_auc = average_precision_score(y_true=y_true, y_score=probs, average=_average)\n",
        "    accuracy             = accuracy_score         (y_true=y_true, y_pred=y_pred)\n",
        "\n",
        "    # return as dictionary\n",
        "    metrics = {\n",
        "        'f1'                  : f1,\n",
        "        'precision'           : precision,\n",
        "        'recall'              : recall,\n",
        "        'roc_auc'             : roc_auc,\n",
        "        'precision_recall_auc': precision_recall_auc,\n",
        "        'accuracy'            : accuracy\n",
        "        }\n",
        "\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "797b2WHJqUgZ"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
        "    result = multi_label_metrics(\n",
        "        predictions = preds,\n",
        "        labels      = p.label_ids\n",
        "        )\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxNo4_TsvzDm"
      },
      "source": [
        "Let's verify a batch as well as a forward pass:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlOgGiojuWwG"
      },
      "outputs": [],
      "source": [
        "encoded_dataset['train'][0]['labels'].type()\n",
        "encoded_dataset['train']['input_ids'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adTwB7XvFNsj"
      },
      "outputs": [],
      "source": [
        "print(f\"inputids:       {type(encoded_dataset['train']['input_ids'][0])}      {encoded_dataset['train']['input_ids'][0].shape}\")\n",
        "print(f\"attention_mask: {type(encoded_dataset['train']['attention_mask'][0])} {encoded_dataset['train']['attention_mask'][0].shape}\")\n",
        "print(f\"labels:         {type(encoded_dataset['train'][0]['labels'])}         {encoded_dataset['train'][0]['labels'].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxWcnZ8ku12V",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "### Execute the forward pass\n",
        "\n",
        "outputs = model(\n",
        "    input_ids      = encoded_dataset['train']['input_ids'][0].unsqueeze(0),\n",
        "    attention_mask = encoded_dataset['train']['attention_mask'][0].unsqueeze(0),\n",
        "    labels         = encoded_dataset['train'][0]['labels'].unsqueeze(0)\n",
        "    )\n",
        "\n",
        "outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-X2brZcv0X6"
      },
      "source": [
        "Let's start training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chq_3nUz73ib"
      },
      "outputs": [],
      "source": [
        "### Create the trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXmFds8js6P8",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "### Train\n",
        "\n",
        "train_ouput = trainer.train()\n",
        "print(f\"train_ouput.global_step: {type(train_ouput.global_step)} {train_ouput.global_step}\")        # Total training steps\n",
        "print(f\"train_ouput.training_loss: {type(train_ouput.training_loss)} {train_ouput.training_loss}\")  # Final training loss\n",
        "print(f\"train_ouput.metrics: {type(train_ouput.metrics)} {train_ouput.metrics}\")                    # Training metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiloh9eMK91o"
      },
      "source": [
        "## Evaluate\n",
        "\n",
        "After training, we evaluate our model on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMlebJ83LRYG"
      },
      "outputs": [],
      "source": [
        "### Evaluate\n",
        "\n",
        "eval_results = trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api.upload_file(\n",
        "    path_or_fileobj = \"eval_results.json\",\n",
        "    path_in_repo    = \"eval_results.json\",\n",
        "    repo_id         = repo_id,\n",
        "    repo_type       = \"model\","
      ],
      "metadata": {
        "id": "LYmURgAz4Xk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfPiHh62FNsl"
      },
      "outputs": [],
      "source": [
        "### Save:\n",
        "### - the trained model and the tokenizer (saves the model weights, the tokenizer, the model configuration file (\"config.json\"))\n",
        "### - the train and evaluation metrics\n",
        "\n",
        "model_path         = \"skills_model\"\n",
        "train_metrics_path = \"train_metrics.json\"\n",
        "eval_metrics_path  = \"eval_metrics.json\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXlDMOCXFNsl"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(model_path)\n",
        "\n",
        "with open(train_metrics_path, 'w') as f:\n",
        "  json.dump(trainer.state.log_history,f)\n",
        "\n",
        "with open(eval_metrics_path, 'w') as f:\n",
        "  json.dump(eval_results, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSAz3pspFNsm"
      },
      "outputs": [],
      "source": [
        "### Upload the model and the tokenizer\n",
        "\n",
        "model     = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "model.push_to_hub(repo_id_model)\n",
        "tokenizer.push_to_hub(repo_id_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkB4b3qw-MxM"
      },
      "outputs": [],
      "source": [
        "raise Exception(\"STOP\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nmvJp0pLq-3"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Let's test the model on a new sentence:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8THm5-XgNHPm"
      },
      "source": [
        "The logits that come out of the model are of shape (batch_size, num_labels). As we are only forwarding a single sentence through the model, the `batch_size` equals 1. The logits is a tensor that contains the (unnormalized) scores for every individual label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC4XdDaHNVcd"
      },
      "source": [
        "To turn them into actual predicted labels, we first apply a sigmoid function independently to every score, such that every score is turned into a number between 0 and 1, that can be interpreted as a \"probability\" for how certain the model is that a given class belongs to the input text.\n",
        "\n",
        "Next, we use a threshold (typically, 0.5) to turn every probability into either a 1 (which means, we predict the label for the given example) or a 0 (which means, we don't predict the label for the given example)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58NIfp3yFNsn"
      },
      "source": [
        "## device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhioyh9aFNsn"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeTxxM--1QVL",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl7ZZsiaFNsn"
      },
      "source": [
        "Download labels.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc_G55oWFNsn"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from huggingface_hub import hf_hub_download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FOxICxfFNsn"
      },
      "outputs": [],
      "source": [
        "repo_id_labels = \"claudelepere/skills_labels\"\n",
        "repo_filename  = \"labels.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Cv5Z9dOxgPO",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "file_path = hf_hub_download(repo_id                = repo_id_labels,\n",
        "                            filename               = repo_filename,\n",
        "                            repo_type              = \"dataset\",\n",
        "                            local_dir              = \"/content\",\n",
        "                            local_dir_use_symlinks = False\n",
        "                           )\n",
        "with open(file_path, 'r') as f:\n",
        "    labels = json.load(f)\n",
        "print(f\"labels: {type(labels)} {len(labels)}\\n{labels}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBODuFNfFNsn"
      },
      "source": [
        "Upload skills.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7c-rttf8UE6",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "!ls -la"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cF2BSzptFNsn"
      },
      "source": [
        "## Filtered skills (those in labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dGfxsckFNsn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M12waP1I85BR",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "skill_df          = pd.read_csv(\"skills.csv\")\n",
        "skill_df['Id']    = skill_df['Id'].astype(str)\n",
        "skill_df['Value'] = skill_df['Value'].astype(str)\n",
        "filtered_skill_df = skill_df[skill_df['Id'].isin(labels)]\n",
        "print(f\"filtered_skill_df: {type(filtered_skill_df)} {filtered_skill_df.shape}\\n{filtered_skill_df}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wpxTxFAFNso"
      },
      "source": [
        "### Model and Tokenizer from the model repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSHYw3pxFNso"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfq9FL_pFNso"
      },
      "outputs": [],
      "source": [
        "repo_id_model = \"claudelepere/skills_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-qzTCqOTqHt",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# Load the trained model and tokenizer\n",
        "model     = AutoModelForSequenceClassification.from_pretrained(repo_id_model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(repo_id_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4m3ys8NFNso"
      },
      "source": [
        "### New texts to classify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCLg4f81FNso"
      },
      "source": [
        "text = \"I'm happy, I can finally train a model for multi-label classification.\"\n",
        "preds: none"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PltPt1Ny3VB"
      },
      "outputs": [],
      "source": [
        "#### in train or validation or test\n",
        "# id = 23553\n",
        "text = \"Cream Consulting - Business Analyst Insurance, Property&Casualty (P&C) Cream Consulting Today we are looking for Business Analyst to extend our team specialized in Business Process Improvement. You will join a multi-lingual team focusing on P&C business. In that position, you will define and clarify the role and responsibilities. You will be responsible for leading business process reviews where you will identify and implement innovative solutions. You will define and document the new processes and systems to meet our client's business objectives. What the job is all about? Implement new solutions for P&C insurance companies Write and validate the “as is” Plan the analysis of organization's strategic business needs Understand End Users needs Develop process modeling and design Follow up projects and monitor development Organize and conduct testing What Cream is looking for? At least 5 years of experience in Business Analysis in insurance business Analysis know-how Fluent editorial Good knowledge of Property & Casualty insurance business to interface with all stakeholders Functional knowledge of insurance products Good communication skills Fluent English, good knowledge in French and/or Dutch is a plus\"\n",
        "# \"356,168,149,795,802,137,139,138,353\"; SkillTypeId 7: true: 149, 168, 356 preds: 142, 170"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-hUyBnUFNso"
      },
      "source": [
        "### NL\n",
        "id = 323697\n",
        "text = \"Talencia Consulting - Senior Full Stack Developer (Java & Angular) Full Stack, DevOps, AKS, OAuth, API Talencia Consulting Voor een klant van Talencia ben ik opzoek naar een Senior Full Stack Developer (Java & Angular) Job beschrijving Als Developer zal je een bestaand team toevoegen en meewerken aan de buitbouw van webapplicaties op Azure. Dit is om bestaande applicaties te vervangen die end-of-live zijn. Het project is al in volle realisatie. Profiel Zeer goede kennis van Java en Angular Goede kennis van Azure DevOps, AKS,.. is een grote pluspunt Kennis van Docker/ SQL/ OAuth/PWA/ RESTful API is vereist Taal: Nederlands met kennis van Engels Extra informatie Teamspeler met ervaring in Agile methodiek is vereist. Als je meer informatie wilt en dit klinkt interessant voor u, aarzel dan niet om uw meest recente CV door te sturen. Het kan zijn dat ik niet beschik over uw meest recente CV en dat ik daarom u deze opportuniteit doorstuur dat niet geschikt is voor u. Als u iemand kent dat deze missie interessant zou vinden mag u deze vacature doorsturen. Met vriendelijke groeten,\"\n",
        "\"142,189,190,754,208,794,676,811,812,139,138\"; SkillTypeId: true: 142; preds: 142, 170"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76ge1lnSFNsp"
      },
      "outputs": [],
      "source": [
        "#### EN\n",
        "# id = 323611\n",
        "text = \"Atcon Global - Project Management Officer / PMO team management Atcon Global For one of our clients, we are looking for an experienced Project Management Officer (PMO) / Project Manager (PM) for permanent employment in the Flanders region. Your role? As a PMO, you will play a crucial role in setting up and improving our project management processes. You will not only be responsible for developing PM standards, but also for carrying out projects independently as a Project Manager. Your duties and responsibilities will include: Developing PMO and project management standards Executing and managing complex digital projects Oversee project progress and report to senior management Follow-up of project budgets, project selection, capacity planning and resource management Coaching and training project managers Identifying and managing project risks Promote continuous improvement in the project management domain Collaborate with stakeholders and external partners Who are we looking for? Bachelor's or master's degree 5+ years in a similar role in a dynamic organization Expertise in project management methods (Agile, Scrum, Lean, Kanban) Strong analytical and problem-solving skills Excellent communication and stakeholder management Experience in team management with clear objectives Proactive, Hands-on mentality and result-oriented Fluent in Dutch and English; French is a plus What's on offer? A dynamic and varied role in a growing, ambitious and innovative company Numerous opportunities for personal growth and career development A competitive salary with customizable benefits A friendly, collegial working atmosphere Flexible working hours, possibility to work from home\"\n",
        "# \"171,170,794,800,798,797,138,139,352\"; SkillTypeId 7: true: 170, 171; preds: 142, 170"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP6QtXKoFNsp"
      },
      "source": [
        "id = 323526\n",
        "text = \"Vivid Resourcing - Chief Technology Officer CTO, reliability, business goals Vivid Resourcing We're partnered with a leading sustainability-oriented company near Brussels, aiming to combat high pollution rates worldwide. They are currently working on a unique application that rewards workers for reducing their carbon footprint, whilst also maintaining and even improving profits. Together we are seeking a visionary Chief Technology Officer (CTO) who aligns with their mission and ambitions. The ideal candidate will possess a strong hands-on technical background, proven management experience, and strong business acumen. This role requires a strategic thinker who can drive technological direction and support the company's growth objectives of transitioning from a scale-up to an established business entity, so any past experience leading teams in this manner would go a long way. Key responsibilities Develop and execute the company's technological vision and strategy Lead and mentor a team of engineers and technologists Oversee all technical aspects of the company, ensuring alignment with business goals Drive innovation in regenerative sustainable technologies and carbon measurement systems Collaborate with cross-functional teams to integrate technology solutions Ensure the reliability, security, and scalability of technological infrastructures Foster a culture of continuous improvement and technical excellence Qualifications Experience leading a team within a small to medium sized company Strong technical background in software development/data analytics/system architecture Bachelor's or Master's degree in either an IT or Business related field Experience in the agriculture or environmental sectors is a plus Proven management skills with the ability to lead, communicate and inspire a diverse team Excellent business acumen and strategic thinking Strong problem-solving skills and the ability to make informed decisions in a fast-paced environment Offer Taking charge of a genuinely impactful product, using your direction for the good of the environment Complete responsibility over a technical team, with management responsibilities Up to 110,000 EUR gross for experienced applicants, which can then be increased further Full benefits package including mobility costs Flexible hybrid work Inclusive work environment If this role interests you, attach a CV and apply today!\"\n",
        "\"175,169,139,352\"; SkillTypeId 7: true: 169, 175 preds: 142, 170"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "-eLN9mXyFNsp"
      },
      "source": [
        "### FR\n",
        "id = 323517\n",
        "text = \"AG Insurance - Technical Architect REST API, DevOps AG Insurance Un lieu de travail où vous pouvez prendre des initiatives. Où tout le monde vous encourage à montrer ce dont vous êtes capable. Où on vous encourage à vous développer. Et où vous construisez l'avenir avec vos collègues. Vous avez de l'audace et le client est au centre de vos préoccupations ? Alors, à très bientôt chez AG ! Notre compagnie compte environ 700 spécialistes IT. Autant de collègues géniaux qui contribuent chaque jour à la (r)évolution technologique chez AG. Le département Information System (IS) est responsable du développement des applications supportant le business assurance, ainsi que des applications transversales dans les domaines suivants : In & Outbounds, Referentials & Financials, sans oublier des centres d'expertise technologique pour le développement des Front-Ends, pour la mise en place d'APIs et s... Et si c'était votre prochain job ? Nous recherchons un(e) .NET Solution Architect pour renforcer ce département. Vous proposez des solutions techniques adaptées aux besoins et aux contraintes des projets et conformes à la vision IT d'AG. En innovant et en améliorant ce qui existe déjà, vous créez également une valeur ajoutée pour AG. Pendant la phase architecturale, vous choisissez les solutions en concertation avec les autres architectes techniques et fonctionnels, et avec toutes les autres parties concernées (autres équipes, partenaires externes, collègues d'Infrastructure & Operations, etc.). Vous concevez l'architecture technique des applications sur la base des exigences fonctionnelles et non fonctionnelles. Nous comptons également sur vous pour le support technique, du développement à la production de l'application. Vous assurez le suivi de l'implémentation de vos solutions, en coopération avec les équipes de développement. Vous vous assurez ensuite que la solution mise en œuvre est conforme à l'architecture proposée. Si nécessaire, vous apportez des corrections. Vous accompagnez et coachez vos collègues pour les aider à monter en compétence dans le domaine. Ce poste vous intéresse ? Qu'attendez-vous pour postuler ? Nos recruteurs examineront avec vous quelle division de notre département IT et quelle équipe vous conviendront le mieux. Vous vous reconnaissez dans ce profil ? Les nouvelles technologies ? Vous en suivez de près l'évolution et les tendances en architecture IT. Vous avez une expérience solide dans les domaines suivants : Technical Architecture & Design Patterns REST API, .NET Framework & .NET 6 and up Source control tooling like Azure DevOps & GitHub Microsoft Azure Cloud SQL Server Vous êtes autonome et prenez les choses en main ? Bien sûr ! Mais vous savez remonter les informations ou demander du support si nécessaire. Team-player dans l'âme, vous partagez naturellement votre savoir-faire avec vos collègues. La qualité du travail, dans ses moindres détails, c'est votre cheval de bataille. Vous le documentez avec précision, tant pour le support technique de votre équipe que pour les utilisateurs d'autres teams. Communiquer en français et/ou en néerlandais, à l'oral et à l'écrit ? Un jeu d'enfant pour vous ! Votre anglais est à la hauteur ? Encore mieux ! Un diplôme supérieur en informatique et/ou plus de 2 ans d'expérience dans une fonction similaire ? Foncez ! Vous n'allez pas rater ça ? Un super job chez le leader sur le marché de l'assurance. Et comme nous cherchons à encore mieux servir nos clients, nous comptons sur vous pour nous y aider. Un environnement de travail moderne dans tous les sens du terme : physique, digital et organisationnel. En d'autres termes, des heures de travail flexibles, la possibilité de télétravailler jusqu'à 3 jours par semaine et le matériel IT adéquat pour travailler de la maison. Une équipe enthousiaste et dynamique, notée 10/10 pour l'ambiance et la convivialité. La possibilité de vous perfectionner en continu, grâce à un large éventail de formations. Idéal pour apprendre toutes les compétences qui vous aideront à faire évoluer votre carrière. Une véritable carrière. En effet, travailler chez AG, c'est bien plus qu'un job. Envie d'explorer de nouveaux horizons après quelque temps ? Nous vous guidons et vous encourageons à exploiter tous vos talents à fond. Des bonnes vibrations, grâce à un vaste programme de bien-être bourré d'activités sportives et d'ateliers inspirants. De quoi vous sentir (encore) plus épanoui(e) dans votre travail et votre vie. Un lieu où tout le monde se sent bienvenu et a des chances égales de s'épanouir. Où votre avenir n'est pas déterminé par votre origine, votre âge, votre genre, votre orientation sexuelle ou votre handicap, mais par votre talent et vos compétences. Et comme l'argent, ça compte aussi : un package salarial attrayant. Vous pouvez même composer vous-même une partie de votre package, car personne ne sait mieux que vous ce dont vous avez besoin. Lancez-vous ! Emballé(e) ? > Postulez aujourd'hui encore ! Nous nous ferons un plaisir de vous aider si vous avez besoin de soutien avant ou pendant le processus de sélection.\"\n",
        "\"146,636,676,668,670,300,812,138,137\"; SkillTypeId 7: true: 146; preds: 142, 170, 689"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUM_efcpFNsp"
      },
      "source": [
        "### Classify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaWtUaGIFNsp"
      },
      "outputs": [],
      "source": [
        "inputs    = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
        "threshold = 0.19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_W3wx1qFNsp"
      },
      "outputs": [],
      "source": [
        "# Perform the forward pass for inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZPDe1C8FNsp"
      },
      "outputs": [],
      "source": [
        "# Convert logits to probabilities\n",
        "probs = torch.sigmoid(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Gvl4j_wFNsp"
      },
      "outputs": [],
      "source": [
        "# Convert probabilities to predictions\n",
        "preds = (probs > threshold).int()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXIBqQXiUOo4",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "for label, Value, prob, pred in zip(filtered_skill_df['Id'], filtered_skill_df['Value'], probs.squeeze(), preds.squeeze()):\n",
        "    print(f\"label: {label} prob: {prob.item():.4f} pred: {int(pred.item())} {Value}\")\n",
        "print()\n",
        "for label, Value, prob, pred in zip(filtered_skill_df['Id'], filtered_skill_df['Value'], probs.squeeze(), preds.squeeze()):\n",
        "    if (pred == 1):\n",
        "        print(f\"label: {label} prob: {prob.item():.4f} pred: {int(pred.item())} {Value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHTCU7PQFNsq"
      },
      "source": [
        "### END #####################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJyFmUuczNYN"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8doFi8DFNsq"
      },
      "outputs": [],
      "source": [
        "#text = \"Voor een klant van Talencia ben ik opzoek naar een Senior Full Stack Developer (Java & Angular) Job beschrijving Als Developer zal je een bestaand team toevoegen en meewerken aan de buitbouw van webapplicaties op Azure. Dit is om bestaande applicaties te vervangen die end-of-live zijn. Het project is al in volle realisatie. Profiel Zeer goede kennis van Java en Angular Goede kennis van Azure DevOps, AKS,.. is een grote pluspunt Kennis van Docker/ SQL/ OAuth/PWA/ RESTful API is vereist Taal: Nederlands met kennis van Engels Extra informatie Teamspeler met ervaring in Agile methodiek is vereist. Als je meer informatie wilt en dit klinkt interessant voor u, aarzel dan niet om uw meest recente CV door te sturen. Het kan zijn dat ik niet beschik over uw meest recente CV en dat ik daarom u deze opportuniteit doorstuur dat niet geschikt is voor u. Als u iemand kent dat deze missie interessant zou vinden mag u deze vacature doorsturen. Met vriendelijke groeten\"\n",
        "text = \"Atcon Global - Project Management Officer / PMO team management Atcon Global For one of our clients, we are looking for an experienced Project Management Officer (PMO) / Project Manager (PM) for permanent employment in the Flanders region. Your role? As a PMO, you will play a crucial role in setting up and improving our project management processes. You will not only be responsible for developing PM standards, but also for carrying out projects independently as a Project Manager. Your duties and responsibilities will include: Developing PMO and project management standards Executing and managing complex digital projects Oversee project progress and report to senior management Follow-up of project budgets, project selection, capacity planning and resource management Coaching and training project managers Identifying and managing project risks Promote continuous improvement in the project management domain Collaborate with stakeholders and external partners Who are we looking for? Bachelor's or master's degree 5+ years in a similar role in a dynamic organization Expertise in project management methods (Agile, Scrum, Lean, Kanban) Strong analytical and problem-solving skills Excellent communication and stakeholder management Experience in team management with clear objectives Proactive, Hands-on mentality and result-oriented Fluent in Dutch and English; French is a plus What's on offer? A dynamic and varied role in a growing, ambitious and innovative company Numerous opportunities for personal growth and career development A competitive salary with customizable benefits A friendly, collegial working atmosphere Flexible working hours, possibility to work from home\"\n",
        "encoding = tokenizer(text, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0xEMzBsFNsq"
      },
      "outputs": [],
      "source": [
        "# Define the device based on availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhVjGw6XFNsq"
      },
      "outputs": [],
      "source": [
        "# Move the model to the device\n",
        "model.to(device)\n",
        "# Move encoding to the device of the model\n",
        "encoding = {k: v.to(device) for k,v in encoding.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCdOcGk7FNsq"
      },
      "outputs": [],
      "source": [
        "# Perform inference\n",
        "with torch.no_grad():    # no gradients needed for inference. Forward pass\n",
        "    outputs = model(**encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTTpdf7zFNsq"
      },
      "outputs": [],
      "source": [
        "# Get logits from the model's output\n",
        "logits = outputs.logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPlLfbZEFNsq"
      },
      "outputs": [],
      "source": [
        "# Apply softmax/sigmoid based on the type of classification\n",
        "if model.config.num_labels == 1:\n",
        "    probs = torch.sigmoid(logits.squeeze())\n",
        "else:\n",
        "    #probs = torch.softmax(logits, dim=1).squeeze()\n",
        "    probs = torch.sigmoid(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MswOPpF1FNsq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWAa2V5cFNsq"
      },
      "outputs": [],
      "source": [
        "# To get predictions\n",
        "threshold = 0.5\n",
        "#predictions = torch.where(probs >= threshold, torch.ones_like(probs), torch.zeros_like(probs))\n",
        "#predictions = torch.argmax(probs, dim=-1) if model.config.num_labels > 1 else torch.where(probs >= threshold, torch.ones_like(probs), torch.zeros_like(probs))\n",
        "predictions = (probs > threshold).float()\n",
        "print(\"Predictions:\", predictions)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uI-o4VSPFNsq"
      },
      "outputs": [],
      "source": [
        "# Turn predicted id's into actual label names\n",
        "print(\"Probabilites:\", probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT4Fh1_6FNsr"
      },
      "source": [
        "[id2label[idx] for idx, label in enumerate(predictions['labels']) if label == 1.0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hTzRZ4hFNsr"
      },
      "source": [
        "predicted_labels = [id2label[idx.item()] for idx in predictions]\n",
        "print(predicted_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzwcmVVvXgXR",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "for label, prob, pred in zip(labels, probs.squeeze(), predictions.squeeze()):\n",
        "  print(f\"Label: {label}: Probability: {prob.item():.4f} {int(pred.item())}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "jupytext": {
      "formats": "ipynb,py:nomarker"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}