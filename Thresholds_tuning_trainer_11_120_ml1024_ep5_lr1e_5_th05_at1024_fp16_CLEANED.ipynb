{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudelepere/ML_GitHub/blob/main/Thresholds_tuning_trainer_11_120_ml1024_ep5_lr1e_5_th05_at1024_fp16_CLEANED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OIjOPWowcJq"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate\n",
        "!pip install -q huggingface_hub\n",
        "!pip install -q scikit-learn\n",
        "\n",
        "# transformers and datasets are Hugging Face libraries\n",
        "!pip install -q transformers datasets\n",
        "\n",
        "!pip install -q wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ync6uXy-wcJs"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgQnuPvUFNsY"
      },
      "outputs": [],
      "source": [
        "from datasets              import DatasetDict\n",
        "from google.colab          import auth, drive, files, userdata\n",
        "from huggingface_hub       import create_repo, HfApi, login, upload_file, hf_hub_download, whoami\n",
        "from huggingface_hub.utils import RepositoryNotFoundError\n",
        "from sklearn.metrics       import accuracy_score, average_precision_score, classification_report, f1_score, precision_score, precision_recall_curve, precision_recall_fscore_support, recall_score, roc_auc_score\n",
        "from torch.utils.data      import DataLoader\n",
        "from tqdm.auto             import tqdm\n",
        "from transformers          import AdamW, EvalPrediction, LongformerTokenizerFast, LongformerForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.nn              import BCEWithLogitsLoss, Module"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face Hub (HF Hub) authenticate"
      ],
      "metadata": {
        "id": "98L73sydik95"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNxYo9txwcJs"
      },
      "outputs": [],
      "source": [
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")    # Store the key in os.environ\n",
        "hf_token               = os.environ.get('HF_TOKEN')\n",
        "\n",
        "login(token=hf_token)\n",
        "\n",
        "# Check\n",
        "user = whoami(token=hf_token)\n",
        "assert user['name'] == 'claudelepere', f\"{user['name']} is not claudelepere\"\n",
        "print(f\"user: {user}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPBlVmE3wcJv"
      },
      "source": [
        "#Create the model and dataset repos on HF Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1haRyaUwcJv"
      },
      "outputs": [],
      "source": [
        "repo_id         = \"claudelepere/skill_classification\"                                             # repo_id = my namespace/username_or_org/reponame\n",
        "model_repoUrl   = create_repo(repo_id=repo_id, repo_type=\"model\",   private=True, exist_ok=True)\n",
        "dataset_repoUrl = create_repo(repo_id=repo_id, repo_type=\"dataset\", private=True, exist_ok=True)\n",
        "\n",
        "print(f\"Model Repo Url: {model_repoUrl} created successfully as a private repo\")\n",
        "print(f\"Dataset Repo Url: {dataset_repoUrl} created successfully as a private repo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhAd6EZJwcJt"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Check the Python version\n",
        "print(sys.version)\n",
        "print()\n",
        "\n",
        "# Get the installed packages (you can see that conda is not installed (do not install it))\n",
        "!pip list\n",
        "print()\n",
        "\n",
        "# Check system information\n",
        "!cat /etc/os-release\n",
        "!uname -m\n",
        "print()\n",
        "\n",
        "# Check the GPU details (only if the runtime type is T4 GPU)\n",
        "#!nvidia-smi\n",
        "#print()\n",
        "\n",
        "# Check RAM\n",
        "!free -h\n",
        "print()\n",
        "\n",
        "# Check disk space\n",
        "!df -h\n",
        "print()\n",
        "\n",
        "# Get environment variables\n",
        "for key, value in os.environ.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\"\"\"\n",
        "!python -V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUTMN8SSwcJt"
      },
      "outputs": [],
      "source": [
        "print(f\"currentdir: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIUfehpkwcJt"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Out Of Memory (OOM)"
      ],
      "metadata": {
        "id": "pr4xZXGSXsEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OOM: reduce batch size\n",
        "      small sizes (1 to 32):            PROs: better generalization in some cases\n",
        "                                        CONs: may produce noisier gradients\n",
        "      large sizes (128, 256, or higer): PROs: gradients are smoother, leading to more stable training\n",
        "                                        CONs: poorer generalization (overfitting) in some cases\n",
        "      intermediate sizes (32, 64):      combines the benefits of small and large sizes"
      ],
      "metadata": {
        "id": "nvigdvJgp8V9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bq0JDT07wcJt"
      },
      "outputs": [],
      "source": [
        "batch_size = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OOM: enable gradient accumulation\n",
        "\n",
        "* compensate for smaller batch sizes by accumulating gradients over several steps\n",
        "* **effective batch size** = per-device batch size x gradient acumulation steps\n",
        "* in each iteration, the model computes the gradients, these gradients are immediately used to update the model parameters\n",
        "\n",
        "WARNING: gradient_accumulation_steps may not be None => comment it in TrainingArguments"
      ],
      "metadata": {
        "id": "rPERKMpOqa8X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nG0pflXGwcJt"
      },
      "outputs": [],
      "source": [
        "gradient_accumulation_steps = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OOM: use PYTORCH_CUDA_ALLOC_CONF to handle memory fragmentation"
      ],
      "metadata": {
        "id": "8yOaxNG3rzUt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXrPvSWAwcJt"
      },
      "outputs": [],
      "source": [
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OOM: check for and kill zombie processes"
      ],
      "metadata": {
        "id": "bmIswaWGsBBV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKeGyFYHwcJu"
      },
      "outputs": [],
      "source": [
        "!ps aux | grep python\n",
        "!kill -9 <PID>\n",
        "if torch.cuda.is_available():\n",
        "  !nvidia-smi\n",
        "  print(torch.cuda.memory_summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OOM: use fp16 (half precision) mixed precision training\n",
        "reduces memory requirements by up to 50%"
      ],
      "metadata": {
        "id": "TpKIEUjjsO-i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbyMivrEwcJu"
      },
      "outputs": [],
      "source": [
        "fp16 = True\n",
        "\n",
        "if fp16:\n",
        "  _fp = \"fp16\"\n",
        "else:\n",
        "  _fp = \"fp32\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVhoxNtUwcJu"
      },
      "source": [
        "## OOM: limit the number of GPU workers:\n",
        "* 0 (default) or 1\n",
        "* in Colab dataloader_num_workers = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OOM: reduce model size or input tokens\n",
        "* LongformerTokenizer.from_pretrained('allenai/longformer-base/large-4096'): large/base: 435M/149M parameters\n",
        "* max_length: 4096 max for Longformer\n",
        "* a single word can be equal to several tokens; stop words are **NOT discarded**!\n",
        "* word_text_length_counts_sorted:\n",
        "      jobs count                 : 50000\n",
        "      jobs count under  512 words: 44794  89.59%\n",
        "      jobs count under  640 words: 47894  95.79%\n",
        "      jobs count under  768 words: 49123  98.25%\n",
        "      jobs count under  896 words: 49691  99.38%\n",
        "      jobs count under 1024 words: 49917  99.83%\n",
        "      jobs count under 2048 words: 50000 100.00%\n",
        "      jobs count under 4096 words: 50000 100.00%"
      ],
      "metadata": {
        "id": "-elhl5Pkssq8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZOpDR5JwcJu"
      },
      "outputs": [],
      "source": [
        "#max_length =  768    #      37 min    #\n",
        "max_length = 1024    #      38 min    # GPU RAM: 12.2 / 40 GB\n",
        "#max_length = 2048    # 1 hr 10 min    # GPU RAM: 21.4 / 40 GB\n",
        "#max_length = 4096    # 2 hr 10 min    # GPU RAM: 39.5 / 40 GB => OutOfMemoryError"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OOM: free up GPU memory"
      ],
      "metadata": {
        "id": "R4v9XcFLuudV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJNB5_2HwcJu"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVi2_ymCwcJu"
      },
      "source": [
        "## OOM: reduce the number of transformers layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hidden_layers = 6  # default:12"
      ],
      "metadata": {
        "id": "VBu-UPkq0Rq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skills and datapoints"
      ],
      "metadata": {
        "id": "6ZIqJOYWagUh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWgIYo5DwcJt"
      },
      "outputs": [],
      "source": [
        "skills         = 11\n",
        "num_datapoints = 12000\n",
        "\n",
        "datasetDict_zip_file_name = f\"dataset_{skills}_{num_datapoints}.zip\"\n",
        "datasetDict_dir_name      = os.path.splitext(datasetDict_zip_file_name)[0]\n",
        "\n",
        "print(f\"datasetDict_zip_file_name: {datasetDict_zip_file_name}\")\n",
        "print(f\"datasetDict_dir_name     : {datasetDict_dir_name}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# epoch\n",
        "* 1 epoch is a complete pass through the entire training dataset\n",
        "* with n datapoints and batch size = b, n/b iterations to complete 1 epoch\n",
        "* 1 iteration is a single update of the model's parameters"
      ],
      "metadata": {
        "id": "qizOQRXNu54F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KORJtBl7wcJu"
      },
      "outputs": [],
      "source": [
        "epochs = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# learning rate\n",
        "* A common rule is to scale the learning rate proportionaly with the effective batch size\n",
        "* **note: get_linear_schedule_with_warmup**"
      ],
      "metadata": {
        "id": "lBRPtOsqvm2P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaoQWRXYwcJu"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-5  # 1e-5 x 32/8"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# threshold\n",
        "default: 0.5"
      ],
      "metadata": {
        "id": "_1j87eP0wCR7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2Dfm6lvwcJu"
      },
      "outputs": [],
      "source": [
        "threshold = 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# attention window size"
      ],
      "metadata": {
        "id": "xNDt1EVUwOWE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-CmoMf9wcJ0"
      },
      "outputs": [],
      "source": [
        "attention_window = 1024 #512"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# run_name"
      ],
      "metadata": {
        "id": "tRrvd-piqLJ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gweGUl--FNsZ",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "run_name = f\"{skills}_{num_datapoints}_ml{max_length}_ep{epochs}_lr{learning_rate}_th{threshold}_at{attention_window}_{_fp}\"\n",
        "\n",
        "if 'gradient_accumulation_steps' not in globals():\n",
        "  run_name = f\"{run_name}_ba{batch_size}\"\n",
        "else:\n",
        "  run_name = f\"{run_name}_ba{batch_size}x{gradient_accumulation_steps}\"\n",
        "\n",
        "print(f\"run_name: {run_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload and unzip job dataset"
      ],
      "metadata": {
        "id": "RPt8yoHMsksM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvZTXW-_ZAJ-",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def upload_unzip_dataset(file_name=datasetDict_zip_file_name):\n",
        "  # Check if the file exists\n",
        "  if not os.path.exists(file_name):\n",
        "    print(f\"'{file_name}' not found in /content. Uploading...\")\n",
        "    uploaded_files = files.upload()                              # Prompt file upload dialog\n",
        "    if file_name not in uploaded_files:\n",
        "      raise FileNotFoundError(f\"'{file_name}' was not uploaded. Please try again.\")\n",
        "    print(f\"'{file_name}' successfully uploaded to /content\")\n",
        "    uploaded_file_name = list(uploaded_files.keys())[0]          # Get the name of the uploaded file\n",
        "\n",
        "    !unzip {uploaded_file_name}\n",
        "\n",
        "    unzipped_dir_name = os.path.splitext(uploaded_file_name)[0]\n",
        "    assert unzipped_dir_name==datasetDict_dir_name, \"unzipped_dir_name != datasetDict_dir_name\"\n",
        "  else:\n",
        "    print(f\"'{datasetDict_dir_name}' already exists in /content.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0k70dawNeONH"
      },
      "outputs": [],
      "source": [
        "upload_unzip_dataset(datasetDict_zip_file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T90_cKa4wcJv"
      },
      "source": [
        "# W&B initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giGsdqlUwcJz"
      },
      "outputs": [],
      "source": [
        "os.environ[\"WANDB_API_KEY\"] = userdata.get(\"WANDB_API_KEY\")        # Store the key in os.environ\n",
        "wandb_api_key               = os.environ.get('WANDB_API_KEY')\n",
        "wandb.login(key=wandb_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYJMUPCONEiD"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  wandb.init(\n",
        "      project = \"skill_classification\",\n",
        "      name    = run_name,\n",
        "      entity  = \"claudelepere-c-cile-cy\",\n",
        "      config  = {\n",
        "          \"learning_rate\": learning_rate,\n",
        "          \"epochs\"       : epochs,\n",
        "          \"batch_size\"   : batch_size\n",
        "      }\n",
        "  )\n",
        "except wandb.errors.CommError as err:\n",
        "  print(f\"CommError: {err}\")\n",
        "except Exception as exc:\n",
        "  print(f\"Exception: {exc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfBhdTN9wcJz"
      },
      "source": [
        "# Create datasetDict (HF DatasetDict) = 3 HF Dataset, train, validation and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh_SgQD9wcJz"
      },
      "outputs": [],
      "source": [
        "datasetDict = DatasetDict.load_from_disk(datasetDict_dir_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcF4Gm8GFNsc"
      },
      "outputs": [],
      "source": [
        "print(f\"datasetDict: {type(datasetDict)} {datasetDict.shape}\\n{datasetDict}\")\n",
        "print(f\"datasetDict.keys(): {datasetDict.keys()}\")\n",
        "print(f\"datasetDict['train']:      {type(datasetDict['train'])}      {datasetDict['train'].shape}\")\n",
        "print(f\"datasetDict['validation']: {type(datasetDict['validation'])} {datasetDict['validation'].shape}\")\n",
        "print(f\"datasetDict['test']:       {type(datasetDict['test'])}       {datasetDict['test'].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unjuTtKUjZI3"
      },
      "outputs": [],
      "source": [
        "example = datasetDict['train'][0]\n",
        "print(f\"datasetDict['train'][0]: {type(example)} {example.keys()}\\n{example}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEgARWcywcJz"
      },
      "source": [
        "# Create labels (list), id2label (dict) and label2id (dict).\n",
        "* dataset 7_1000_125_125  ,  48 labels\n",
        "* dataset 7_128_18_54     ,  42 labels\n",
        "* dataset 8910_1087_68_204, 206 labels\n",
        "* dataset 11_1000         ,   6 labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vv6FWVzRwcJz"
      },
      "outputs": [],
      "source": [
        "labels = [label for label in datasetDict['train'].features.keys() if label not in ['id', 'text']]\n",
        "labels.sort()\n",
        "print(f\"labels: {type(labels)} {len(labels)}\\n{labels}\")\n",
        "num_labels = len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-xKopoHwcJz"
      },
      "outputs": [],
      "source": [
        "id2label = {idx: label for idx, label in enumerate(labels)}\n",
        "print(f\"id2label: {type(id2label)} {len(id2label)}\\n{id2label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5eYNbH5FNsd"
      },
      "outputs": [],
      "source": [
        "label2id = {label: idx for idx, label in enumerate(labels)}\n",
        "print(f\"label2id: {type(label2id)} {len(label2id)}\\n{label2id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_TdBZOGwcJ0"
      },
      "source": [
        "# Load the tokenizer and the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tL1IlzvfwcJ0"
      },
      "outputs": [],
      "source": [
        "model_name = \"allenai/longformer-base-4096\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwKcOR8GwcJ0"
      },
      "outputs": [],
      "source": [
        "tokenizer = LongformerTokenizerFast.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTZ61-RUwcJ0"
      },
      "outputs": [],
      "source": [
        "model = LongformerForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels   = num_labels,\n",
        "    id2label     = id2label,\n",
        "    label2id     = label2id,\n",
        "    problem_type = 'multi_label_classification'\n",
        ")\n",
        "\n",
        "# Configure attention window size\n",
        "model.config.attention_window = attention_window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjLO31SssqAM",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTYt0hM5wcJ0"
      },
      "source": [
        "## Tokenize ('input_ids' and 'attention_mask'), add 'global_attention_mask' (for Longformer), add 'labels'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFWlSsbZaRLc",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def preprocess_data(examples, indices):\n",
        "  # Step 1: Extract text and tokenize\n",
        "  text = examples['text']             # Batch of texts\n",
        "  encoding = tokenizer(\n",
        "      text,                           # Tokenize text\n",
        "      truncation     = True,\n",
        "      padding        = 'max_length',\n",
        "      max_length     = max_length,\n",
        "      return_tensors = 'pt'           # Return PyTorch tensors\n",
        "  )\n",
        "\n",
        "  # Step 2: Create and add the global attention mask\n",
        "  global_attention_mask             = torch.zeros_like(encoding['input_ids'])  # Initialize global attention mask with zeros (same shape as input_ids)\n",
        "  global_attention_mask[:, 0]       = 1                                        # Set global attention on the first token ([CLS], token ID=0) in each sequence\n",
        "  encoding['global_attention_mask'] = global_attention_mask                    # Add the global_attention_mask to the batch\n",
        "\n",
        "  # Step 3: Create and populate the label matrix\n",
        "  labels_matrix = torch.zeros((len(text), len(labels)), dtype=torch.float32)   # Create an empty label matrix\n",
        "  #print(f\"labels_matrix: {type(labels_matrix)} {labels_matrix.shape}\")\n",
        "  #---------Populate label matrix\n",
        "  for idx, label in enumerate(labels):\n",
        "    #print(f\"idx:{idx} label:{label}\")\n",
        "    if label in examples:\n",
        "      labels_matrix[:, idx] = torch.tensor(\n",
        "          [1.0 if val else 0.0 for val in examples[label]],\n",
        "          dtype=torch.float32\n",
        "          )\n",
        "  print(f\"labels_matrix: {type(labels_matrix)} {labels_matrix.shape}\")\n",
        "\n",
        "  encoding['labels'] = labels_matrix                                           # Add labels to the encoding\n",
        "  print(f\"encoding['labels']: {type(encoding['labels'])} {encoding['labels'].shape}\")\n",
        "\n",
        "  # encoding: <class 'transformers.tokenization_utils_base.BatchEncoding'> dict_keys(['input_ids', 'attention_mask', 'global_attention_mask', 'labels'])\n",
        "  #   'input_ids': tensor([[\n",
        "  #   'attention_mask': tensor([[\n",
        "  #   'global_attention_mask': tensor([[\n",
        "  #   'labels': tensor([[\n",
        "  #print(f\"1 preprocess_data call: encoding: {type(encoding)} {encoding.keys()}\")\n",
        "\n",
        "  return encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm9REDT_wcJ0"
      },
      "source": [
        "# Create encoded_dataset (datasets.dataset_dict.DatasetDict) = 3 encoded datasets.arrow_dataset.Dataset, train, validation and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxLcO0XDwcJ0"
      },
      "outputs": [],
      "source": [
        "encoded_dataset = datasetDict.map(\n",
        "    preprocess_data,\n",
        "    batched        = True,\n",
        "    remove_columns = datasetDict['train'].column_names,\n",
        "    with_indices   = True\n",
        ")\n",
        "\n",
        "print(f\"Zencoded_dataset: {type(encoded_dataset)} shape={encoded_dataset.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuft8rJe2Q03",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "encoded_dataset.set_format('torch')\n",
        "train_dataset      = encoded_dataset['train']\n",
        "validation_dataset = encoded_dataset['validation']\n",
        "test_dataset       = encoded_dataset['test']\n",
        "\n",
        "print(f\"train_dataset_tensor:                          {type(train_dataset)}                              {train_dataset.shape} {train_dataset.features}\\n{train_dataset}\")\n",
        "print(f\"train_dataset_tensor['input_ids']:             {type(train_dataset['input_ids'])}             len={len(train_dataset['input_ids'])}             shape={train_dataset['input_ids'].shape}            \") #\\n{train_dataset['input_ids']}\")\n",
        "print(f\"train_dataset_tensor['attention_mask']:        {type(train_dataset['attention_mask'])}        len={len(train_dataset['attention_mask'])}        shape={train_dataset['attention_mask'].shape}       \") #\\n{train_dataset['attention_mask']}\")\n",
        "print(f\"train_dataset_tensor['global_attention_mask']: {type(train_dataset['global_attention_mask'])} len={len(train_dataset['global_attention_mask'])} shape={train_dataset['global_attention_mask'].shape}\") #\\n{train_dataset['global_attention_mask']}\")\n",
        "print(f\"train_dataset_tensor['labels']:                {type(train_dataset['labels'])}                len={len(train_dataset['labels'])}                shape={train_dataset['labels'].shape}               \") #\\n{train_dataset['labels']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynwZnA55wcJ1"
      },
      "source": [
        "# Truncated part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brkRdqdjN-Ur",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def get_truncated_part(text):\n",
        "  tokens = tokenizer(\n",
        "      text,\n",
        "      truncation                = True,\n",
        "      padding                   = 'max_length',\n",
        "      max_length                = max_length,\n",
        "      return_overflowing_tokens = True,\n",
        "      return_tensors            = None\n",
        "      )\n",
        "  print(f\"tokens.keys(): {tokens.keys()}\")\n",
        "\n",
        "  # Get the truncated tokens\n",
        "  truncated_ids = tokens[\"input_ids\"][0]\n",
        "  print(f\"truncated_ids: {type(truncated_ids)} {truncated_ids}\")\n",
        "  #overflow_ids  = tokens[\"overflow_to_sample_mapping\"][0]\n",
        "  #print(f\"overflow_ids: {type(overflow_ids)} {overflow_ids}\")\n",
        "\n",
        "  # Decode the tokens back to text\n",
        "  truncated_text = tokenizer.decode(truncated_ids, skip_special_tokens=True)\n",
        "  #overflow_text  = tokenizer.decode(overflow_ids, skip_special_tokens=True)\n",
        "\n",
        "  print(f\"original_text :\\n{text}\")\n",
        "  print(f\"truncated_text:\\n{truncated_text}\")\n",
        "  #print(f\"overflow_text:\\n{overflow_text}\")\n",
        "\n",
        "  original_tokens  = tokenizer.tokenize(text)\n",
        "  truncated_tokens = tokenizer.tokenize(truncated_text)\n",
        "  #overflow_tokens  = tokenizer.tokenize(overflow_text)\n",
        "\n",
        "  print(f\"original_tokens count : {len(original_tokens)}\")\n",
        "  print(f\"truncated_tokens count: {len(truncated_tokens)}\")\n",
        "  #print(f\"overflow_tokens count: {len(overflow_tokens)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DD_bjwTfRiQO"
      },
      "outputs": [],
      "source": [
        "example_text = datasetDict['train'][0]['text']\n",
        "#get_truncated_part(example_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFiZ4mYmwcJ1"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(\n",
        "    example_text,\n",
        "    truncation     = True,\n",
        "    padding        = 'max_length',\n",
        "    max_length     = max_length,\n",
        "    return_tensors = 'pt'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTh2E_8iwcJ1"
      },
      "source": [
        "# Forward pass for multi-label classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_aIPMHuwcJ1"
      },
      "outputs": [],
      "source": [
        "outputs = model(\n",
        "    input_ids      = inputs.input_ids,\n",
        "    attention_mask = inputs.attention_mask\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxWcnZ8ku12V"
      },
      "outputs": [],
      "source": [
        "print(f\"outputs: {type(outputs)} {outputs.keys()}\\n{outputs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "islk-kFSs0t8"
      },
      "outputs": [],
      "source": [
        "# Logits (= raw model outputs)\n",
        "logits = outputs.logits\n",
        "print(f\"logits: {type(logits)} {logits.shape}\\n{logits}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMscqNTXuY8o"
      },
      "outputs": [],
      "source": [
        "# Convert logits to probabilities\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "probs   = sigmoid(logits)\n",
        "print(f\"probs: {type(probs)} {probs.shape}\\n{probs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZtO3uQkwcJ2"
      },
      "outputs": [],
      "source": [
        "example = encoded_dataset['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0enAb0W9o25W"
      },
      "outputs": [],
      "source": [
        "print(f\"example: {type(example)} {example.keys()}\\n{example}\")\n",
        "print()\n",
        "#print(f\"example['input_ids']: {type(example['input_ids'])} {len(example['input_ids'])}\\n{example['input_ids']}\")\n",
        "#print(f\"example['attention_mask']: {type(example['attention_mask'])} {len(example['attention_mask'])}\\n{example['attention_mask']}\")\n",
        "#print(f\"example['labels']:  {type(example['labels'])} {len(example['labels'])}\\n{example['labels']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0McCtJ8HRJY"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(example['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LAyThO7Jnvj"
      },
      "outputs": [],
      "source": [
        "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHCJcLKGwcJ2"
      },
      "source": [
        "# Set PyTorch format to ensures correctness and compatibility with PyTorch pipelines\n",
        "The 3 Hugging Face Dataset are formatted as PyTorch Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4ENBTdulBEI"
      },
      "outputs": [],
      "source": [
        "encoded_dataset.set_format('torch')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workflow\n",
        "\n",
        "# - 3 steps: training, evaluation, prediction/inference\n",
        "\n",
        "# - 3 datasets: train, validation, test\n",
        "\n",
        "# - 3 Trainer functions: train, evaluate, predict\n"
      ],
      "metadata": {
        "id": "o8aGgHN-BQrO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A unique commit_message for all uploads to HF Hub"
      ],
      "metadata": {
        "id": "1QZm-WyCSF5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "commit_message = f\"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
        "\n",
        "print(f\"commit_message: {commit_message}\")"
      ],
      "metadata": {
        "id": "A6bVXujFSOx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy7jlzubwcJ6"
      },
      "source": [
        "# Training step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5a8_vIKqr7P"
      },
      "outputs": [],
      "source": [
        "batch_size  = batch_size\n",
        "metric_name = \"f1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zt-m0QpwcJ3"
      },
      "source": [
        "## Metrics\n",
        "  source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LaQCQoJFNsi",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def multi_label_metrics(logits, true_labels):\n",
        "  \"\"\"\n",
        "  logits => sigmoid => probabilities => predictions\n",
        "\n",
        "  Args:\n",
        "    logits     : raw, unnormalized scores outputted by the model  (numpy array of shape (batch_size, num_labels))\n",
        "    true_labels: actual labels for the data                       (numpy array of shape (batch_size, num_labels))\n",
        "\n",
        "  Returns:\n",
        "    metrics: dictionary of scores\n",
        "  \"\"\"\n",
        "  average = 'micro'    # 'micro' or 'weighted'\n",
        "\n",
        "  sigmoid = torch.nn.Sigmoid()\n",
        "  probs   = sigmoid(torch.Tensor(logits))\n",
        "  # next, use threshold to turn them into integer predictions\n",
        "  preds = np.zeros(probs.shape)\n",
        "  preds[np.where(probs > threshold)] = 1\n",
        "\n",
        "  # compute metrics\n",
        "  f1                   = f1_score               (y_true=true_labels, y_pred=preds,  average=average)    #, zero_division=1)\n",
        "  precision            = precision_score        (y_true=true_labels, y_pred=preds,  average=average)    #, zero_division=1)\n",
        "  recall               = recall_score           (y_true=true_labels, y_pred=preds,  average=average)    #, zero_division=1)\n",
        "  roc_auc              = roc_auc_score          (y_true=true_labels, y_score=probs, average=average)\n",
        "  precision_recall_auc = average_precision_score(y_true=true_labels, y_score=probs, average=average)\n",
        "  accuracy             = accuracy_score         (y_true=true_labels, y_pred=preds)\n",
        "\n",
        "  metrics = {\n",
        "\n",
        "      'f1'                  : f1,\n",
        "      'precision'           : precision,\n",
        "      'recall'              : recall,\n",
        "      'roc_auc'             : roc_auc,\n",
        "      'precision_recall_auc': precision_recall_auc,\n",
        "      'accuracy'            : accuracy\n",
        "  }\n",
        "\n",
        "  return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "797b2WHJqUgZ",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def compute_metrics(p: EvalPrediction):\n",
        "  preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
        "  result = multi_label_metrics(logits=preds,true_labels=p.label_ids)   # true_labels=p.label_ids<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adTwB7XvFNsj"
      },
      "outputs": [],
      "source": [
        "print(f\"input_ids:              {type(encoded_dataset['train']['input_ids'][0])}\\t{encoded_dataset['train']['input_ids'][0].shape}\")\n",
        "print(f\"attention_mask:         {type(encoded_dataset['train']['attention_mask'][0])}\\t{encoded_dataset['train']['attention_mask'][0].shape}\")\n",
        "print(f\"global_attention_mask:  {type(encoded_dataset['train']['global_attention_mask'][0])}\\t{encoded_dataset['train']['global_attention_mask'][0].shape}\")\n",
        "print(f\"labels:                 {type(encoded_dataset['train'][0]['labels'])}\\t{encoded_dataset['train'][0]['labels'].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCGdbfNJwcJ4"
      },
      "source": [
        "## Execute a forward pass for debugging or verification purposes (cf. BERT_3_1 in Notion BERT database)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VN-sEDggwcJ4"
      },
      "outputs": [],
      "source": [
        "outputs = model(\n",
        "    input_ids      = encoded_dataset['train']['input_ids'][0].unsqueeze(0),\n",
        "    attention_mask = encoded_dataset['train']['attention_mask'][0].unsqueeze(0),\n",
        "    labels         = encoded_dataset['train'][0]['labels'].unsqueeze(0)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtEd6KmRwcJ4"
      },
      "outputs": [],
      "source": [
        "print(f\"outputs: {type(outputs)} {outputs.keys()}\\n{outputs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weighted loss function"
      ],
      "metadata": {
        "id": "EYiSTBw_1Kdj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "vqxhHkT1wcJ4"
      },
      "outputs": [],
      "source": [
        "\"\"\"# Define the weighted loss function\n",
        "\n",
        "class_weights = torch.tensor([7.68, 2.15, 0.61, 0.47, 0.68, 6.26], dtype=torch.float32).to(device)\n",
        "loss_fn       = BCEWithLogitsLoss(pos_weight=class_weights)  # For multi-label classification (binary classification per label)\n",
        "\n",
        "## Class supports, class weigths, weighted loss function\n",
        "\n",
        "Reminder:\n",
        "*   df_jobs      : <class 'pandas.core.frame.DataFrame'>\n",
        "*   df_jobs['id']: <class 'pandas.core.series.Series'>\n",
        "\n",
        "dataset = Dataset.from_pandas(df_jobs)\n",
        "*   dataset      : <class 'datasets.arrow_dataset.Dataset'>\n",
        "*   dataset['id']: <class 'list'>\n",
        "\n",
        "*   dataset_dict_jobs : <class 'datasets.dataset_dict.DatasetDict'>\n",
        "*   train_dataset     : <class 'datasets.arrow_dataset.Dataset'>\n",
        "*   validation_dataset: <class 'datasets.arrow_dataset.Dataset'>\n",
        "*   test_dataset      : <class 'datasets.arrow_dataset.Dataset'>\n",
        "\n",
        "\n",
        "We calculate the class supports for the train, validation and test datasets; the class weights and the weighted loss function are used for training only; the class supports of validation_dataset and test_dataset are calculated for information only.\n",
        "\n",
        "def get_train_class_weights(datasetDict, labels):\n",
        "  print(f\"datasetDict: {type(datasetDict)} shape={datasetDict.shape}\\n{datasetDict}\")\n",
        "  print(f\"labels: {type(labels)} len={len(labels)}\\n{labels}\")\n",
        "\n",
        "  dataset_train      = datasetDict['train']\n",
        "  dataset_validation = datasetDict['validation']\n",
        "  dataset_test       = datasetDict['test']\n",
        "\n",
        "  def calculate_class_supports(dataset, labels):\n",
        "    class_supports = dataset.map(\n",
        "        lambda example: {col: example[col] for col in labels},\n",
        "        batched=True\n",
        "    ).to_pandas()[labels].sum(axis=0)\n",
        "    return class_supports\n",
        "\n",
        "  class_supports = {}\n",
        "\n",
        "  for split_name, split_dataset in datasetDict.items():\n",
        "    class_supports[split_name] = calculate_class_supports(split_dataset, labels)\n",
        "\n",
        "  for split_name, split_class_supports in class_supports.items():\n",
        "    print(f\"{split_name}: {type(split_class_supports)} len={len(split_class_supports)}\\n{split_class_supports}\")\n",
        "\n",
        "  train_class_supports_list = class_supports['train'].tolist()\n",
        "  print(f\"train_class_supports_list: {type(train_class_supports_list)} len={len(train_class_supports_list)} {train_class_supports_list}\")\n",
        "\n",
        "  train_class_supports_tensor = torch.tensor(train_class_supports_list, dtype=torch.float32)\n",
        "  print(f\"train_class_supports_tensor: {type(train_class_supports_tensor)} len={len(train_class_supports_tensor)} {train_class_supports_tensor}\")\n",
        "\n",
        "  train_total_samples = dataset_train.num_rows\n",
        "  print(f\"train_total_samples: {train_total_samples}\")\n",
        "\n",
        "  number_of_classes = len(labels)\n",
        "  print(f\"number_of_classes: {number_of_classes}\")\n",
        "\n",
        "  train_class_weights = train_total_samples / (number_of_classes * train_class_supports_tensor)\n",
        "  print(f\"train_class_weights: {type(train_class_weights)} len={len(train_class_weights)} {train_class_weights}\")\n",
        "\n",
        "  train_class_weights_sum = train_class_weights.sum()\n",
        "  print(f\"train_class_weights_sum: {train_class_weights_sum}\")\n",
        "\n",
        "  normalized_train_class_weights = (train_class_weights / train_class_weights_sum) * number_of_classes\n",
        "  print(f\"normalized_train_class_weights: {type(normalized_train_class_weights)} len={len(normalized_train_class_weights)} {normalized_train_class_weights}\")\n",
        "\n",
        "  # Positives samples per label\n",
        "  supports = train_class_supports_tensor\n",
        "  print(f\"supports: {type(supports)} {len(supports)} {supports}\")\n",
        "\n",
        "  # Negatives samples per label\n",
        "  negatives = train_total_samples - supports\n",
        "  print(f\"negatives: {type(negatives)} {len(negatives)} {negatives}\")\n",
        "\n",
        "  # pos_weights = negative to positive ratios\n",
        "  pos_weights = negatives/supports\n",
        "  print(f\"pos_weights: {type(pos_weights)} {len(pos_weights)} {pos_weights}\")\n",
        "\n",
        "  # Normalize using min-max scaling\n",
        "  normalized_pos_weights_minmax = (pos_weights - pos_weights.min()) / (pos_weights.max() - pos_weights.min())\n",
        "  print(f\"normalized_pos_weights_minmax: {type(normalized_pos_weights_minmax)} {len(normalized_pos_weights_minmax)} {normalized_pos_weights_minmax}\")\n",
        "\n",
        "  # Normalize using z-score standardization\n",
        "  normalized_pos_weights_zscore = (pos_weights - pos_weights.mean()) / pos_weights.std()\n",
        "  print(f\"normalized_pos_weights_zscore: {type(normalized_pos_weights_zscore)} {len(normalized_pos_weights_zscore)} {normalized_pos_weights_zscore}\")\n",
        "\n",
        "  # Normalize using min-max scaling\n",
        "  normalized_pos_weights_minmax = (pos_weights - pos_weights.min()) / (pos_weights.max() - pos_weights.min())\n",
        "  print(f\"normalized_pos_weights_minmax: {type(normalized_pos_weights_minmax)} {len(normalized_pos_weights_minmax)} {normalized_pos_weights_minmax}\")\n",
        "\n",
        "  # Normalize using z-score standardization\n",
        "  normalized_pos_weights_zscore = (pos_weights - pos_weights.mean()) / pos_weights.std()\n",
        "  print(f\"normalized_pos_weights_zscore: {type(normalized_pos_weights_zscore)} {len(normalized_pos_weights_zscore)} {normalized_pos_weights_zscore}\")\n",
        "\n",
        "  # Normalize using sum-to-one\n",
        "  normalized_pos_weights_sum1 = pos_weights / pos_weights.sum()\n",
        "  print(f\"normalized_pos_weights_sum1: {type(normalized_pos_weights_sum1)} {len(normalized_pos_weights_sum1)} {normalized_pos_weights_sum1}\")\n",
        "\n",
        "  return normalized_pos_weights_minmax\n",
        "  #return normalized_pos_weights_zscore\n",
        "  #return normalized_pos_weights_sum1\n",
        "\n",
        "pos_weights = get_train_class_weights(datasetDict, labels)\n",
        "\n",
        "loss_fn = BCEWithLogitsLoss(pos_weight=pos_weights.to(device))  # For multi-label classification (binary classification per label)\n",
        "print(f\"loss_fn: {type(loss_fn)} {loss_fn}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "YqMipSpowcJ5"
      },
      "outputs": [],
      "source": [
        "def get_class_weights(labels=encoded_dataset['train']['labels']):\n",
        "  print(f\"labels: {type(labels)} len={len(labels)} shape={labels.shape}\\n{labels}\")\n",
        "\n",
        "  num_samples, num_labels = labels.shape\n",
        "  print(f\"num_samples: {type(num_samples)} {num_samples}\")\n",
        "  print(f\"num_labels:  {type(num_labels)}  {num_labels}\")\n",
        "\n",
        "  class_counts = labels.sum(dim=0)\n",
        "  print(f\"class_counts: {type(class_counts)} len={len(class_counts)}\\n{class_counts}\")\n",
        "\n",
        "  pos_weights = (num_samples-class_counts) / class_counts\n",
        "  print(f\"pos_weights: {type(pos_weights)} len={len(pos_weights)}\\n{pos_weights}\")\n",
        "\n",
        "  normalized_pos_weights_minmax = (pos_weights - pos_weights.min()) / (pos_weights.max() - pos_weights.min())\n",
        "  print(f\"normalized_pos_weights_minmax: {type(normalized_pos_weights_minmax)} {len(normalized_pos_weights_minmax)} {normalized_pos_weights_minmax}\")\n",
        "\n",
        "  normalized_pos_weights_zscore = (pos_weights - pos_weights.mean()) / pos_weights.std()\n",
        "  print(f\"normalized_pos_weights_zscore: {type(normalized_pos_weights_zscore)} {len(normalized_pos_weights_zscore)} {normalized_pos_weights_zscore}\")\n",
        "\n",
        "  normalized_pos_weights_sum1 = pos_weights / pos_weights.sum()\n",
        "  print(f\"normalized_pos_weights_sum1: {type(normalized_pos_weights_sum1)} {len(normalized_pos_weights_sum1)} {normalized_pos_weights_sum1}\")\n",
        "\n",
        "  #return pos_weights\n",
        "  #return normalized_pos_weights_minmax\n",
        "  #return normalized_pos_weights_zscore\n",
        "  return normalized_pos_weights_sum1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Bh-gAHGwcJ5"
      },
      "outputs": [],
      "source": [
        "pos_weights = get_class_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "4d36Km6_wcJ5"
      },
      "outputs": [],
      "source": [
        "loss_fn = BCEWithLogitsLoss(pos_weight=pos_weights.to(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FocalLoss"
      ],
      "metadata": {
        "id": "w_lLw3Er1WGj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "OPRpdL8fwcJ5"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(Module):\n",
        "  \"\"\"\n",
        "  Focal Loss implementation\n",
        "  \"\"\"\n",
        "  def __init__(self, alpha=1.0, gamma=2.0, logits=False, reduce=True):\n",
        "    super(FocalLoss, self).__init__()\n",
        "    self.alpha   = alpha\n",
        "    self.gamma   = gamma\n",
        "    self.logits  = logits  # This flag is to indicate whether input is logits or probability\n",
        "    self.reduce  = reduce\n",
        "\n",
        "  # inputs  = model's predictions: PyTorch tensor, shape=(batch_size, num_classes)\n",
        "  # targets = ground truth labels: PyTorch tensor, shape=same as inputs shape\n",
        "  def forward(self, inputs, targets):\n",
        "    # Here, we check if input is probability or logits\n",
        "    if self.logits:\n",
        "      # Input is logits\n",
        "      BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
        "    else:\n",
        "      # Input is probability\n",
        "      BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
        "    pt = torch.exp(-BCE_loss)\n",
        "    F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
        "\n",
        "    if self.reduce:\n",
        "      return torch.mean(F_loss)\n",
        "    else:\n",
        "      return F_loss\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"FocalLoss(alpha={self.alpha}, gamma={self.gamma}, logits={self.logits}, reduce={self.reduce})\"\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"FocalLoss(alpha={self.alpha}, gamma={self.gamma}, logits={self.logits}, reduce={self.reduce})\"\n",
        "\n",
        "  def __call__(self, inputs, targets):\n",
        "    return self.forward(inputs, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zknwQWsEb7oP",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "focal_loss_fn = FocalLoss(alpha=0.5, gamma=4.0, logits=True, reduce=True)\n",
        "print(f\"focal_loss_fn: {type(focal_loss_fn)} {focal_loss_fn}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2elPEoowcJ6"
      },
      "source": [
        "## HF transformer Trainer and CustomTrainer\n",
        "Abstracts the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dR2GmpvDqbuZ",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir                  = './training_results',  # where model predictions and checkpoints will be written during training\n",
        "    overwrite_output_dir        = True,\n",
        "    logging_dir                 = './logs',\n",
        "    logging_steps               = 50,\n",
        "    save_steps                  = 500,\n",
        "    save_total_limit            = 2,\n",
        "    eval_strategy               = 'epoch',\n",
        "    save_strategy               = 'epoch',\n",
        "    learning_rate               = learning_rate,\n",
        "    per_device_train_batch_size = batch_size,\n",
        "    per_device_eval_batch_size  = batch_size,\n",
        "    gradient_accumulation_steps = gradient_accumulation_steps,\n",
        "    num_train_epochs            = epochs,\n",
        "    weight_decay                = 0.01,\n",
        "    load_best_model_at_end      = True,\n",
        "    metric_for_best_model       = metric_name,\n",
        "    run_name                    = run_name,\n",
        "    fp16                        = fp16,\n",
        "    report_to                  = 'wandb'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Isb6aMV2bWJ8",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "class CustomTrainer(Trainer):\n",
        "\n",
        "  def __init__(self, *args, loss_fn=None, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.loss_fn = loss_fn\n",
        "\n",
        "  \"\"\"\n",
        "  # No print in compute_loss because out of memory because prints are batch per batch\n",
        "  def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "\n",
        "    #print(f\"inputs passed to compute_loss: {inputs.keys()}\")\n",
        "    #input_ids             = inputs['input_ids']                        # shape: batch_size, sequence_length\n",
        "    #attention_mask        = inputs['attention_mask']                   # shape: batch_size, sequence_length\n",
        "    #global_attention_mask = inputs.get('global_attention_mask', None)  # shape: batch_size, sequence_length; optional as LongFormer specific\n",
        "    labels                = inputs.pop('labels', None)                 # shape: batch_size, num_labels; needed for loss computation, not required by the model\n",
        "\n",
        "    #outputs = model(**inputs, global_attention_mask=global_attention_mask)  # Forward pass\n",
        "    # Forward pass\n",
        "    #outputs = model(\n",
        "    #    input_ids             = input_ids,\n",
        "    #    attention_mask        = attention_mask,\n",
        "    #    global_attention_mask = global_attention_mask,\n",
        "    #    labels                = labels\n",
        "    #)\n",
        "    outputs = model(**inputs, labels=labels)\n",
        "    #print(f\"outputs: {type(outputs)} {outputs.keys()}\\n{outputs}\")\n",
        "    logits = outputs.logits  # shape: (batch_size, num_labels)\n",
        "\n",
        "    # If labels are provided, compute loss\n",
        "    if labels is not None:\n",
        "      # Use the custom loss function if provided\n",
        "      if self.loss_fn is not None:\n",
        "        loss = self.loss_fn(logits, labels)  # Compute weighted loss\n",
        "      else:\n",
        "        # Default loss: BCEWithLogitsLoss\n",
        "        loss_fn = BCEWithLogitsLoss()\n",
        "        loss    = loss_fn(logits, labels)    # Compute loss\n",
        "      return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    # If no labels, return outputs only, for evaluation or inference\n",
        "    return outputs\n",
        "    \"\"\"\n",
        "  def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "    labels  = inputs.pop('labels', None)\n",
        "    outputs = model(**inputs, labels=labels)\n",
        "    logits  = outputs.logits\n",
        "\n",
        "    if labels is not None:\n",
        "      if self.loss_fn is not None:\n",
        "        loss = self.loss_fn(logits, labels)\n",
        "      else:\n",
        "        loss_fn = BCEWithLogitsLoss()\n",
        "        loss    = loss_fn(logits, labels)\n",
        "      return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chq_3nUz73ib"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "trainer = CustomTrainer(\n",
        "    model           = model,\n",
        "    args            = training_args,\n",
        "    train_dataset   = encoded_dataset[\"train\"],\n",
        "    eval_dataset    = encoded_dataset[\"validation\"],\n",
        "    compute_metrics = compute_metrics,                # custom metrics function\n",
        "    loss_fn         = focal_loss_fn,\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "trainer = Trainer(\n",
        "    model           = model,\n",
        "    args            = training_args,\n",
        "    train_dataset   = encoded_dataset[\"train\"],\n",
        "    eval_dataset    = encoded_dataset[\"validation\"],\n",
        "    compute_metrics = compute_metrics,                # custom metrics function\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "predictions_output: <class 'transformers.trainer_utils.PredictionOutput'> len=3:\n",
        "- predictions: np.ndarray          raw model outputs = logits\n",
        "- label_ids  : np.ndarray or None: true labels corresponding to the predictions, if available\n",
        "- metrics    : dict              : metrics computed during prediction step\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "predictions_output = trainer.predict(test_dataset)\n",
        "print(f\"predictions_output: {type(predictions_output)} len={len(predictions_output)}\\n{predictions_output}\")  # <class 'transformers.trainer_utils.PredictionOutput'> len=3\n",
        "logits = predictions_output.predictions\n",
        "print(f\"logits: {type(logits)} len={len(logits)} shape={logits.shape}\\n{logits}\")\n",
        "\n",
        "\n",
        "raise Exception(\"I'm here\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cVs8D9geXHqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## trainer.train"
      ],
      "metadata": {
        "id": "z3kPFNOG-3vL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_k5uxpKwcJ6"
      },
      "outputs": [],
      "source": [
        "trainer_train_results = trainer.train()\n",
        "\n",
        "print(f\"trainer_train_results: {type(trainer_train_results)} len={len(trainer_train_results)}\\n{trainer_train_results}\")\n",
        "print()\n",
        "print(f\"trainer_train_results.metrics: {type(trainer_train_results.metrics)} len={len(trainer_train_results.metrics)}\\n{json.dumps(trainer_train_results.metrics, indent=4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2SMR-lI8Y8r"
      },
      "outputs": [],
      "source": [
        "print(\"trainer.train successfully completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## trainer.train results: save locally and upload to HF Hub"
      ],
      "metadata": {
        "id": "Uai-d2z2IMaZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvhT1c_h_oul"
      },
      "outputs": [],
      "source": [
        "trainer_train_results_path = \"trainer_train_results.json\"\n",
        "with open(trainer_train_results_path, \"w\") as f:\n",
        "  json.dump(trainer_train_results, f)\n",
        "\n",
        "print(f\"trainer_train_results successfully saved locally to {trainer_train_results_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCxHpgmT5meA"
      },
      "outputs": [],
      "source": [
        "upload_file(\n",
        "    path_or_fileobj = trainer_train_results_path,\n",
        "    path_in_repo    = trainer_train_results_path,\n",
        "    repo_id         = repo_id,\n",
        "    repo_type       = 'dataset',\n",
        "    commit_message  = commit_message\n",
        ")\n",
        "\n",
        "print(f\"trainer_train_results successfully uploaded to HF Hub {repo_id} as {trainer_train_results_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## trainer.train results: check that the uploaded file can be downloaded\n",
        "File locally downloaded to:\n",
        "\n",
        "/root/.cache/huggingface/hub/datasets-claudelepere-skill_classification/snapshots/full_commit_hash/trainer_train_results.json"
      ],
      "metadata": {
        "id": "as2omZpPJNO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = hf_hub_download(repo_type=\"dataset\", repo_id=repo_id, filename=trainer_train_results_path)\n",
        "\n",
        "print(f\"file_path: {file_path}\")"
      ],
      "metadata": {
        "id": "PJ2D28X2-GfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation step"
      ],
      "metadata": {
        "id": "JVozoTnW1Mt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation: trainer.evaluate\n",
        "trainer.evaluate uses a fixed threshold of 0.5 to convert logits into binary labels, which is often suboptimal for imbalanced data."
      ],
      "metadata": {
        "id": "05FkezueB6_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_trainer_evaluate_results = trainer.evaluate(\n",
        "    #eval_dataset=encoded_dataset[\"validation\"],  # by default, trainer.evaluate() evaluates the dataset passed as eval_dataset during training\n",
        "    metric_key_prefix=\"eval\"                     # prefix for the evaluation metrics\n",
        ")\n",
        "\n",
        "print(f\"evaluation_trainer_evaluate_results: {type(evaluation_trainer_evaluate_results)} len={len(evaluation_trainer_evaluate_results)}\\n{json.dumps(evaluation_trainer_evaluate_results, indent=4)}\")\n"
      ],
      "metadata": {
        "id": "HMKEjsRk5ah6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"evaluation_trainer.evaluate successfully completed\")"
      ],
      "metadata": {
        "id": "_O-_tVHg-lUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation: trainer.evaluate results: save locally and upload to HF Hub"
      ],
      "metadata": {
        "id": "9w5BmumZ6tDS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMlebJ83LRYG"
      },
      "outputs": [],
      "source": [
        "evaluation_trainer_evaluate_results_path = \"evaluation_trainer_evaluate_results.json\"\n",
        "with open(evaluation_trainer_evaluate_results_path, \"w\") as f:\n",
        "  json.dump(evaluation_trainer_evaluate_results, f)\n",
        "\n",
        "print(f\"evaluation_trainer_evaluate_results successfully saved locally to {evaluation_trainer_evaluate_results_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "upload_file(\n",
        "    path_or_fileobj = evaluation_trainer_evaluate_results_path,\n",
        "    path_in_repo    = evaluation_trainer_evaluate_results_path,\n",
        "    repo_id         = repo_id,\n",
        "    repo_type       = 'dataset',\n",
        "    commit_message  = commit_message\n",
        ")\n",
        "\n",
        "print(f\"evaluation_trainer_evaluate_results successfully uploaded to HF Hub {repo_id} as {evaluation_trainer_evaluate_results_path}\")"
      ],
      "metadata": {
        "id": "PpfQ0S6LDj7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation: trainer.predict"
      ],
      "metadata": {
        "id": "nxYFDrB57nM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tune_thresholds(true_labels, probs, id2label):\n",
        "  \"\"\"\n",
        "  Tune thresholds for each label to maximize F1 alone, as F1 balances precision and recall into a single metric.\n",
        "\n",
        "  Args:\n",
        "    true_labels: actual labels for the data                                      (numpy array of shape (num_samples, num_labels))\n",
        "    probs      : predicted probabilities                                         (numpy array of shape (num_samples, num_labels))\n",
        "    id2label   : dictionary mapping label indices (int) to label names (string)\n",
        "\n",
        "  Returns:\n",
        "    best_thresholds: best threshold for each label                                                      (numpy array of shape (num_labels,))\n",
        "    best_metrics   : dictionary of best F1, precision_for_best_f1 and recall_for_best_f1 for each label (dictionary of numpy arrays)\n",
        "  \"\"\"\n",
        "  thresholds      = np.linspace(0.1, 0.9, 9)\n",
        "  best_thresholds = np.zeros(len(id2label))\n",
        "  best_metrics    = {label: {'f1': 0.0, 'precision': 0.0, 'recall': 0.0} for label in id2label.values()}\n",
        "\n",
        "  for label_idx, label in id2label.items():\n",
        "    for threshold in thresholds:\n",
        "      pred                     = (probs[:, label_idx] > threshold).astype(int)\n",
        "      precision, recall, f1, _ = precision_recall_fscore_support(true_labels[:, label_idx], pred, average='binary', zero_division=0)\n",
        "      if f1 > best_metrics[label]['f1']:\n",
        "        best_thresholds[label_idx]       = threshold\n",
        "        best_metrics[label]['f1']        = f1\n",
        "        best_metrics[label]['precision'] = precision\n",
        "        best_metrics[label]['recall']    = recall\n",
        "\n",
        "  return best_thresholds, best_metrics\n"
      ],
      "metadata": {
        "id": "NlcRxeCycBg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics_with_threshold(probs, label_ids, thresholds, id2label):\n",
        "  \"\"\"\n",
        "  Compute metrics during evaluation or test, by applying tuned thresholds\n",
        "  \"\"\"\n",
        "  #logits  = eval_preds.predictions\n",
        "  #labels  = eval_preds.label_ids\n",
        "  #sigmoid = torch.nn.Sigmoid  # Sigmoid or numpy?\n",
        "  #probs   = sigmoid(logits).cpu().numpy()\n",
        "  preds   = np.zeros_like(probs)\n",
        "\n",
        "  # Apply threshold per label\n",
        "  for label_idx in id2label.keys():\n",
        "    preds[:, label_idx] = (probs[:, label_idx] > thresholds[label_idx]).astype(int)\n",
        "\n",
        "  # Compute metrics\n",
        "  f1                    = f1_score               (label_ids, preds, average='micro')\n",
        "  precision             = precision_score        (label_ids, preds, average='micro')\n",
        "  recall                = recall_score           (label_ids, preds, average='micro')\n",
        "  accuracy              = accuracy_score         (label_ids, preds)\n",
        "  roc_auc               = roc_auc_score          (label_ids, probs, average='micro')\n",
        "  precision_recall_auc  = average_precision_score(label_ids, probs, average='micro')\n",
        "\n",
        "  # Use id2label for target_names\n",
        "  report = classification_report(label_ids, preds, target_names=id2label.values(), zero_division=0)\n",
        "\n",
        "  metrics = {\n",
        "      'f1'                   : f1,\n",
        "      'precision'            : precision,\n",
        "      'recall'               : recall,\n",
        "      'accuracy'             : accuracy,\n",
        "      'roc_auc'              : roc_auc,\n",
        "      'precision_recall_auc' : precision_recall_auc,\n",
        "      'thresholds'           : thresholds.tolist(),\n",
        "      'classification_report': report\n",
        "  }\n",
        "\n",
        "  return metrics\n"
      ],
      "metadata": {
        "id": "8_-dGUEkJqDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "def predict_with_thresholds(trainer, dataset, id2label, threshold_tuning=False, thresholds=None):\n",
        "    \"\"\"\n",
        "    Predicts using trainer.predict(), with optional thresholds tuning.\n",
        "\n",
        "    Parameters:\n",
        "      - trainer         : Hugging Face Trainer or CustomTrainer instance\n",
        "      - dataset         : dataset to predict on\n",
        "      - id2label        : dictionary mapping label indices (int) to label names (string)\n",
        "      - threshold_tuning: boolean to enable threshold tuning\n",
        "      - thresholds      : custom thresholds for classification (optional)\n",
        "\n",
        "    Returns:\n",
        "      - predictions                               : final binary predictions\n",
        "      - true_labels                               : ground truth labels from the dataset\n",
        "      - best_thresholds (if threshold_tuning=True): optimized threshold for each label\n",
        "      - best_metrics    (if threshold_tuning=True): dictionary of best F1, precision_for_best_f1 and recall_for_best_f1 for each label\n",
        "    \"\"\"\n",
        "    # Predict\n",
        "    predictions_output = trainer.predict(dataset)        # <class 'transformers.trainer_utils.PredictionOutput'> len   = 3\n",
        "    predictions        = predictions_output.predictions  # <class 'numpy.ndarray'>                               shape = (12, 6) TODO\n",
        "    label_ids          = predictions_output.label_ids    # <class 'numpy.ndarray'>                               shape = (12, 6) TODO\n",
        "    metrics            = predictions_output.metrics      # <class 'dict'                                         len   = 10\n",
        "\n",
        "    print(f\"predictions: {type(predictions)} shape={predictions.shape}\\n{predictions}\")\n",
        "    print(f\"label_ids  : {type(label_ids)}   shape={label_ids.shape}  \\n{label_ids}\")\n",
        "    print(f\"metrics    : {type(metrics)}     len={len(metrics)}       \\n{json.dumps(metrics, indent=4)}\")\n",
        "\n",
        "    logits             = predictions\n",
        "    true_labels        = label_ids\n",
        "\n",
        "    # Convert logits to probabilities\n",
        "    probabilities = 1 / (1 + np.exp(-logits))            # <class 'numpy.ndarray'>                               shape = (12, 6) TODO\n",
        "    print(f\"probabilities: {type(probabilities)} shape={probabilities.shape}\")\n",
        "\n",
        "    best_thresholds, best_metrics = tune_thresholds(true_labels, probabilities, id2label)\n",
        "    print(f\"best_thresholds: {type(best_thresholds)} shape={best_thresholds.shape}\\n{best_thresholds}\")\n",
        "    print(f\"best_metrics   : {type(best_metrics)}    len={len(best_metrics)}\\n{best_metrics}\")\n",
        "\n",
        "    metrics = compute_metrics_with_threshold(probabilities, label_ids, best_thresholds, id2label)\n",
        "\n",
        "    return metrics"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "22JFVtdWhnss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_trainer_predict_results = predict_with_thresholds(trainer, validation_dataset, id2label, threshold_tuning=True, thresholds=None)\n",
        "\n",
        "except_report = {k: v for k, v in evaluation_trainer_predict_results.items() if k!='classification_report'}\n",
        "report        = evaluation_trainer_predict_results['classification_report']\n",
        "print(f\"evaluation_trainer_predict_results: {type(except_report)} len={len(except_report)}\\n{json.dumps(except_report, indent=4)}\")\n",
        "print(f\"evaluation_trainer_predict_results['classification_report']: {type(report)} len={len(report)}\\n{report}\")\n"
      ],
      "metadata": {
        "id": "TphO3ifmBLDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"trainer.predict successfully completed\")"
      ],
      "metadata": {
        "id": "oKuHwTP-_OmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation: trainer.predict results: save locally and upload to HF Hub"
      ],
      "metadata": {
        "id": "JtwSQZ89__I9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_trainer_predict_results_path = \"evaluation_trainer_predict_results.json\"\n",
        "with open(evaluation_trainer_predict_results_path, \"w\") as f:\n",
        "  json.dump(evaluation_trainer_predict_results, f)\n",
        "\n",
        "print(f\"evaluation_trainer_predict_results successfully saved locally to {evaluation_trainer_predict_results_path}\")"
      ],
      "metadata": {
        "id": "aTDx-S2UDsyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upload_file(\n",
        "    path_or_fileobj = evaluation_trainer_predict_results_path,\n",
        "    path_in_repo    = evaluation_trainer_predict_results_path,\n",
        "    repo_id         = repo_id,\n",
        "    repo_type       = 'dataset',\n",
        "    commit_message  = commit_message\n",
        ")\n",
        "\n",
        "print(f\"evaluation_trainer_evaluate_results successfully uploaded to HF Hub {repo_id} as {evaluation_trainer_evaluate_results_path}\")"
      ],
      "metadata": {
        "id": "1p4v5_KEDzvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYmURgAz4Xk-"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "upload_file(\n",
        "    path_or_fileobj = f\"{phase_evaluate}_results.csv\",\n",
        "    path_in_repo    = f\"{phase_evaluate}_results.csv\",\n",
        "    repo_id         = HF_name,\n",
        "    repo_type       = 'dataset'\n",
        ")\n",
        "upload_file(\n",
        "    path_or_fileobj = f\"{phase_evaluate}_metrics.json\",\n",
        "    path_in_repo    = f\"{phase_evaluate}_metrics.json\",\n",
        "    repo_id         = HF_name,\n",
        "    repo_type       = 'dataset'\n",
        ")\n",
        "upload_file(\n",
        "    path_or_fileobj = 'trainer_evaluate.json',\n",
        "    path_in_repo    = 'trainer_evaluate.json',\n",
        "    repo_id         = HF_name,\n",
        "    repo_type       = 'dataset'\n",
        ")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raise Exception(\"I'm here\")"
      ],
      "metadata": {
        "id": "yPc7wMNpVe8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction/Inference step"
      ],
      "metadata": {
        "id": "sYSVoKZDAq4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction/Inference: trainer.evaluate"
      ],
      "metadata": {
        "id": "0lCQusokDRda"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction/Inference: trainer.predict"
      ],
      "metadata": {
        "id": "ZJNMb3idDapI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwO6_V_lYcJg"
      },
      "source": [
        "First test results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X0r_TALwcJ9"
      },
      "outputs": [],
      "source": [
        "phase_test = 'test_model_eval'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPHcZrdsEARs"
      },
      "outputs": [],
      "source": [
        "get_results(\n",
        "    model      = model,\n",
        "    dataset    = test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    threshold  = threshold,\n",
        "    phase      = phase_test\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yibAPgTaVljm"
      },
      "outputs": [],
      "source": [
        "print(\"First test successfully completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMtMjR-WwcJ9"
      },
      "source": [
        "Second test results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX1sWMZywcJ9"
      },
      "outputs": [],
      "source": [
        "trainer_predict = trainer.predict(test_dataset)\n",
        "print(f\"trainer_predict: {type(trainer_predict)} len={len(trainer_predict)}\\n{trainer_predict}\")\n",
        "print(f\"trainer_predict.predictions: {type(trainer_predict.predictions)} shape={trainer_predict.predictions.shape}\")  # Model logits\n",
        "print(f\"trainer_predict.label_ids: {type(trainer_predict.label_ids)} shape={trainer_predict.label_ids.shape}\")        # Ground truth labels\n",
        "print(f\"trainer_predict.metrics: {type(trainer_predict.metrics)} len={len(trainer_predict.metrics)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY42YauKwcJ9"
      },
      "outputs": [],
      "source": [
        "trainer_predict_json_serializable = {\n",
        "    'predictions': trainer_predict.predictions.tolist(),  # Convert Numpy array to list\n",
        "    'label_ids'  : trainer_predict.label_ids.tolist(),    # Convert Numpy array to list\n",
        "    'metrics'    : trainer_predict.metrics                # Dictionary is already serializable\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39vYrXt4HSaO"
      },
      "outputs": [],
      "source": [
        "file_path = \"trainer_predict.json\"\n",
        "with open(file_path, \"w\") as f:\n",
        "  json.dump(trainer_predict_json_serializable, f)\n",
        "print(f\"Test output successfully saved to {file_path}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_F1Y8Hve8qsa"
      },
      "outputs": [],
      "source": [
        "print(\"Second test successfully completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4ckvP9rwcJ9"
      },
      "source": [
        "Upload Test Results to HF repo_id_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSy9v4OL7iPa"
      },
      "outputs": [],
      "source": [
        "upload_file(\n",
        "    path_or_fileobj = f\"{phase_test}_results.csv\",\n",
        "    path_in_repo    = f\"{phase_test}_results.csv\",\n",
        "    repo_id         = HF_name,\n",
        "    repo_type       = 'dataset'\n",
        ")\n",
        "upload_file(\n",
        "    path_or_fileobj = f\"{phase_test}_metrics.json\",\n",
        "    path_in_repo    = f\"{phase_test}_metrics.json\",\n",
        "    repo_id         = HF_name,\n",
        "    repo_type       = 'dataset'\n",
        ")\n",
        "upload_file(\n",
        "    path_or_fileobj = 'trainer_predict.json',\n",
        "    path_in_repo    = 'trainer_predict.json',\n",
        "    repo_id         = HF_name,\n",
        "    repo_type       = 'dataset'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVQXo7SvwcJ7"
      },
      "source": [
        "# Upload the tokenizer and the model to HF Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSAz3pspFNsm"
      },
      "outputs": [],
      "source": [
        "tokenizer.push_to_hub(repo_id=repo_id, revision=run_name, commit_message=commit_message)\n",
        "model.push_to_hub(repo_id=repo_id, revision=run_name, commit_message=commit_message)\n",
        "\n",
        "print(f\"tokenizer and model successfully uploaded to HF Hub at {repo_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the tokenizer and the model\n",
        "It's also a check whether the upload was successful."
      ],
      "metadata": {
        "id": "T3W9-Xdo_egG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokenizer\")\n",
        "tokenizer = LongformerTokenizerFast.from_pretrained(repo_id, revision=run_name)\n",
        "print(\"Model\")\n",
        "model     = LongformerForSequenceClassification.from_pretrained(repo_id, revision=run_name)\n",
        "\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\", truncation=True, padding=True)\n",
        "outputs = model(**inputs)\n",
        "\n",
        "print()\n",
        "print(f\"outputs: {type(outputs)} {outputs.keys()}\\n{outputs}\")"
      ],
      "metadata": {
        "id": "Vrs9lNbS-uh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvh_jDvdBt26"
      },
      "outputs": [],
      "source": [
        "print(\"It's the end\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "jupytext": {
      "formats": "ipynb,py:nomarker"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}