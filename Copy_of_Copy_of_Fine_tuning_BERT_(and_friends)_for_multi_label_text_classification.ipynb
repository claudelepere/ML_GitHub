{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudelepere/ML_GitHub/blob/main/Copy_of_Copy_of_Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "print(f\"Current directory: {current_dir}\")\n"
      ],
      "metadata": {
        "id": "JffgoBWJrPhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLB3I4FKZ5Lr"
      },
      "source": [
        "# My fine-tuning BERT (and friends) for multi-label text classification\n",
        "\n",
        "In this notebook, we are going to fine-tune BERT to predict one or more labels for a given piece of text. Note that this notebook illustrates how to fine-tune a bert-base-uncased model, but you can also fine-tune a RoBERTa, DeBERTa, DistilBERT, CANINE, ... checkpoint in the same way.\n",
        "\n",
        "All of those work in the same way: they add a **linear layer on top of the base model, which is used to produce a tensor of shape (batch_size, num_labels)**, indicating the unnormalized scores for a number of labels for every example in the batch.\n",
        "\n",
        "\n",
        "\n",
        "## Set-up environment\n",
        "\n",
        "First, we install the libraries which we'll use: HuggingFace Transformers and Datasets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets\n"
      ],
      "metadata": {
        "id": "Rxwimm_9ipw-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIH9NP0MZ6-O"
      },
      "source": [
        "## Load dataset\n",
        "\n",
        "Next, let's download a multi-label text classification dataset from the [hub](https://huggingface.co/).\n",
        "\n",
        "At the time of writing, I picked a random one as follows:   \n",
        "\n",
        "* first, go to the \"datasets\" tab on huggingface.co\n",
        "* next, select the \"multi-label-classification\" tag on the left as well as the the \"1k<10k\" tag (fo find a relatively small dataset).\n",
        "\n",
        "Note that you can also easily load your local data (i.e. csv files, txt files, Parquet files, JSON, ...) as explained [here](https://huggingface.co/docs/datasets/loading.html#local-and-remote-files).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "sd1LiXGjZ420",
        "outputId": "2a10f41c-c0d6-4cb4-978c-f3ed4a1323c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current directory: /content\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fd5373fd-4676-4ef3-9235-0c259f778141\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fd5373fd-4676-4ef3-9235-0c259f778141\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 240\n",
            "drwxr-xr-x 1 root root   4096 Nov 15 16:31 .\n",
            "drwxr-xr-x 1 root root   4096 Nov 15 16:00 ..\n",
            "drwxr-xr-x 4 root root   4096 Nov 12 14:24 .config\n",
            "drwxr-xr-x 5 root root   4096 Nov 15 16:31 datasetHF_128_18_54\n",
            "-rw-r--r-- 1 root root 213003 Nov 15 16:31 datasetHF_128_18_54.zip\n",
            "drwxr-xr-x 1 root root   4096 Nov 12 14:25 sample_data\n",
            "-rw-r--r-- 1 root root   7805 Nov 15 16:31 skills.csv\n"
          ]
        }
      ],
      "source": [
        "dir_1000_125_125 = False\n",
        "dir_128_18_54    = False\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()         # upload datasetHF_128_18_54.zip or datasetHF_1000_125_125.zip, and skills.csv\n",
        "\n",
        "!ls -la\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azw3Om6lrUWE",
        "outputId": "79ad9b93-0af3-4951-e809-27690af3a7cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasetHF_128_18_54.zip exists\n",
            "Archive:  datasetHF_128_18_54.zip\n",
            "   creating: datasetHF_128_18_54/test/\n",
            "   creating: datasetHF_128_18_54/train/\n",
            "   creating: datasetHF_128_18_54/validation/\n",
            "  inflating: datasetHF_128_18_54/dataset_dict.json  \n",
            "  inflating: datasetHF_128_18_54/test/data-00000-of-00001.arrow  \n",
            "  inflating: datasetHF_128_18_54/test/dataset_info.json  \n",
            "  inflating: datasetHF_128_18_54/test/state.json  \n",
            "  inflating: datasetHF_128_18_54/train/data-00000-of-00001.arrow  \n",
            "  inflating: datasetHF_128_18_54/train/dataset_info.json  \n",
            "  inflating: datasetHF_128_18_54/train/state.json  \n",
            "  inflating: datasetHF_128_18_54/validation/data-00000-of-00001.arrow  \n",
            "  inflating: datasetHF_128_18_54/validation/dataset_info.json  \n",
            "  inflating: datasetHF_128_18_54/validation/state.json  \n"
          ]
        }
      ],
      "source": [
        "### Unzip the datasetHF zip file\n",
        "\n",
        "if os.path.isfile(\"datasetHF_1000_125_125.zip\"):\n",
        "    print(\"datasetHF_1000_125_125.zip exists\")\n",
        "    !unzip datasetHF_1000_125_125.zip -d datasetHF_1000_125_125\n",
        "    dir_1000_125_125 = True\n",
        "elif os.path.isfile(\"datasetHF_128_18_54.zip\"):\n",
        "    print(\"datasetHF_128_18_54.zip exists\")\n",
        "    !unzip datasetHF_128_18_54.zip -d datasetHF_128_18_54\n",
        "    dir_128_18_54 = True\n",
        "else:\n",
        "  print(\"Neither datasetHF_1000_125_125.zip nor datasetHF_128_18_54.zip exists\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8CEWz878rl_8"
      },
      "outputs": [],
      "source": [
        "### dataset\n",
        "\n",
        "from datasets import DatasetDict\n",
        "\n",
        "if dir_1000_125_125:\n",
        "    dataset = DatasetDict.load_from_disk('datasetHF_1000_125_125')\n",
        "elif dir_128_18_54:\n",
        "    dataset = DatasetDict.load_from_disk('datasetHF_128_18_54')\n",
        "else:\n",
        "    print(\"Neither dir datasetHF_1000_125_125 nor datasetHF_128_18_54 exists\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCL02vQgxYTO"
      },
      "source": [
        "As we can see, the dataset contains 3 splits: one for training, one for validation and one for testing."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"dataset: {type(dataset)} {dataset.shape}\\n{dataset}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ea9aTDY0nLys",
        "outputId": "46984967-a789-44a3-8b3b-d15ac24b7b3d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset: <class 'datasets.dataset_dict.DatasetDict'> {'train': (128, 44), 'validation': (18, 44), 'test': (54, 44)}\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'text', '394', '142', '146', '147', '148', '149', '150', '151', '408', '409', '153', '154', '155', '156', '157', '158', '160', '152', '162', '165', '167', '168', '169', '170', '171', '685', '174', '686', '176', '689', '173', '356', '360', '361', '362', '364', '760', '756', '758', '375', '376', '761'],\n",
            "        num_rows: 128\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'text', '394', '142', '146', '147', '148', '149', '150', '151', '408', '409', '153', '154', '155', '156', '157', '158', '160', '152', '162', '165', '167', '168', '169', '170', '171', '685', '174', '686', '176', '689', '173', '356', '360', '361', '362', '364', '760', '756', '758', '375', '376', '761'],\n",
            "        num_rows: 18\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'text', '394', '142', '146', '147', '148', '149', '150', '151', '408', '409', '153', '154', '155', '156', '157', '158', '160', '152', '162', '165', '167', '168', '169', '170', '171', '685', '174', '686', '176', '689', '173', '356', '360', '361', '362', '364', '760', '756', '758', '375', '376', '761'],\n",
            "        num_rows: 54\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test the first example of the training split:"
      ],
      "metadata": {
        "id": "j5qgMAhrlql-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unjuTtKUjZI3",
        "outputId": "dbee0ad2-5c5b-48ff-f018-e838681a6822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example: <class 'dict'> dict_keys(['id', 'text', '394', '142', '146', '147', '148', '149', '150', '151', '408', '409', '153', '154', '155', '156', '157', '158', '160', '152', '162', '165', '167', '168', '169', '170', '171', '685', '174', '686', '176', '689', '173', '356', '360', '361', '362', '364', '760', '756', '758', '375', '376', '761'])\n",
            "{'id': 140409, 'text': \"Inetum-Realdolmen - Azure Cloud Engineer Azure Cloud Inetum-Realdolmen YOUR FUNCTION To support the exponential growth of our Azure practice, we are looking for several Azure Cloud Engineers. Here's how you'll make impact: For larger projects you work in team with other Azure Cloud Engineers, Cloud Solution Architects and Project Managers to write a new success story. We can count on you for the professional implementation of the tasks entrusted to you. For smaller cloud projects you are in charge for the full engagement: you advise your client from the design until his solution is fully operational. In addition to consultancy, you also offer third-line support to customers and colleagues. Automation is in your genes. You always strive to make your life and that of the operation engineers easier by automating everything. Infrastructure-as-Code does not sound weird to you. You test thoroughly, and you don't compromise on quality. You adhere to Realdolmens best practices and play an active role in the continuous improvement by making suggestions and modifications based on your daily hands-on experience. You will be involved during PoCs, pilots and proactive services Inetum-Realdolmen offers as a Cloud Managed Services Provider (MSP), such a cost optimization, lifecycle management, architecture optimization, ... You keep your work well-documented, clear and to the point. YOUR PROFILE You are experienced in Azure as a system engineer You have a pretty good understanding of the Azure Infrastructure-as-a-Service platform and can assess the added value of IaaS components for clients' respective situations. You are eager to learn and ambitious. You also invest your own time in keeping your competencies up-to-date. You are a real team player, but you also get on just fine on your own. You are socially skilled and always willing to share knowledge with colleagues. You are flexible and resistant to stress. You work accurately and on a project basis, and you are not prepared to make compromises when it comes to quality. You speak fluently Dutch and English or French and English. If that's you, come in and have a talk with us! OUR OFFER The opportunity to have a meaningful job where you can make a difference; The chance to continuously evolve as a professional, coupled with a variety of training opportunities; Inetum-Realdolmen wants you to find a balance between work and private life by offering flexible hours, satellite offices and home working; 32 days of annual leave, because life isn't all about working; Forget about the miles: we provide you with a company car and a national fuel card; Group insurance and hospitalization insurance, because we care about you; And of course, we also offer a gross salary. One which is optimized from a net perspective for our employees!\", '394': False, '142': False, '146': False, '147': False, '148': False, '149': False, '150': False, '151': False, '408': False, '409': False, '153': False, '154': False, '155': False, '156': False, '157': False, '158': False, '160': True, '152': False, '162': False, '165': False, '167': False, '168': False, '169': False, '170': False, '171': False, '685': False, '174': False, '686': False, '176': False, '689': False, '173': False, '356': False, '360': False, '361': False, '362': False, '364': False, '760': False, '756': False, '758': False, '375': False, '376': False, '761': False}\n"
          ]
        }
      ],
      "source": [
        "example = dataset['train'][0]\n",
        "\n",
        "print(f\"example: {type(example)} {example.keys()}\\n{example}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset consists of texts, labeled with one or more skills.\n",
        "\n",
        "Let's create a list that contains the labels, as well as 2 dictionaries that map labels to integers and back."
      ],
      "metadata": {
        "id": "fdkFBRCcl5dU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5vZhQpvkE8s",
        "outputId": "6aa1cf3b-6860-473c-c6b1-456e68580408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels: <class 'list'> 42\n",
            "['142', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '160', '162', '165', '167', '168', '169', '170', '171', '173', '174', '176', '356', '360', '361', '362', '364', '375', '376', '394', '408', '409', '685', '686', '689', '756', '758', '760', '761']\n",
            "id2label: <class 'dict'> 42\n",
            "{0: '142', 1: '146', 2: '147', 3: '148', 4: '149', 5: '150', 6: '151', 7: '152', 8: '153', 9: '154', 10: '155', 11: '156', 12: '157', 13: '158', 14: '160', 15: '162', 16: '165', 17: '167', 18: '168', 19: '169', 20: '170', 21: '171', 22: '173', 23: '174', 24: '176', 25: '356', 26: '360', 27: '361', 28: '362', 29: '364', 30: '375', 31: '376', 32: '394', 33: '408', 34: '409', 35: '685', 36: '686', 37: '689', 38: '756', 39: '758', 40: '760', 41: '761'}\n",
            "label2id: <class 'dict'> 42\n",
            "{'142': 0, '146': 1, '147': 2, '148': 3, '149': 4, '150': 5, '151': 6, '152': 7, '153': 8, '154': 9, '155': 10, '156': 11, '157': 12, '158': 13, '160': 14, '162': 15, '165': 16, '167': 17, '168': 18, '169': 19, '170': 20, '171': 21, '173': 22, '174': 23, '176': 24, '356': 25, '360': 26, '361': 27, '362': 28, '364': 29, '375': 30, '376': 31, '394': 32, '408': 33, '409': 34, '685': 35, '686': 36, '689': 37, '756': 38, '758': 39, '760': 40, '761': 41}\n"
          ]
        }
      ],
      "source": [
        "### if dataset 1000_125_125, 48 labels\n",
        "### if dataset 128_18_54   , 42 labels\n",
        "\n",
        "labels = [label for label in dataset['train'].features.keys() if label not in ['id', 'text']]\n",
        "labels.sort()\n",
        "print(f\"labels: {type(labels)} {len(labels)}\\n{labels}\")\n",
        "\n",
        "id2label = {idx:label for idx, label in enumerate(labels)}\n",
        "print(f\"id2label: {type(id2label)} {len(id2label)}\\n{id2label}\")\n",
        "\n",
        "label2id = {label:idx for idx, label in enumerate(labels)}\n",
        "print(f\"label2id: {type(label2id)} {len(label2id)}\\n{label2id}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### dataset of filtered skills, only those in labels\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "skill_df          = pd.read_csv(\"skills.csv\")\n",
        "skill_df['Id']    = skill_df['Id'].astype(str)\n",
        "skill_df['Value'] = skill_df['Value'].astype(str)\n",
        "filtered_skill_df = skill_df[skill_df['Id'].isin(labels)]\n",
        "\n",
        "print(f\"filtered_skill_df: {type(filtered_skill_df)} {filtered_skill_df.shape}\\n{filtered_skill_df}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_6Pzj_vn3Ph",
        "outputId": "7ec14080-1b8c-41fa-c1da-d7625f0cf433"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "filtered_skill_df: <class 'pandas.core.frame.DataFrame'> (42, 3)\n",
            "     Id  SkillTypeId                             Value\n",
            "0   142            7    Developer / Analyst Programmer\n",
            "2   146            7  Application / Solution Architect\n",
            "3   147            7          Infrastructure Architect\n",
            "4   148            7                 Technical Analyst\n",
            "5   149            7                Functional Analyst\n",
            "6   150            7        Test / Validation Engineer\n",
            "7   151            7         Test / Validation Manager\n",
            "8   152            7                  Technical Writer\n",
            "9   153            7                Database Developer\n",
            "10  154            7            Database Administrator\n",
            "11  155            7                Database Architect\n",
            "12  156            7                Helpdesk / Support\n",
            "13  157            7                          Operator\n",
            "14  158            7      Field / Maintenance Engineer\n",
            "15  160            7   System Engineer / Administrator\n",
            "16  162            7                 Security Engineer\n",
            "17  165            7        Network / Telecom Engineer\n",
            "18  167            7                   Trainer / Coach\n",
            "19  168            7           Consultant (Specialist)\n",
            "20  169            7                       Team Leader\n",
            "21  170            7     Project Manager / Coordinator\n",
            "22  171            7        Project Mgmt Officer (PMO)\n",
            "23  173            7          Change / Release Manager\n",
            "24  174            7          Service Delivery Manager\n",
            "26  176            7           Sales / Account Manager\n",
            "27  356            7                  Business Analyst\n",
            "28  360            7                         Recruiter\n",
            "29  361            7                Quality Specialist\n",
            "30  362            7            Software Administrator\n",
            "31  364            7                   Process Analyst\n",
            "35  375            7           Product Owner / Manager\n",
            "36  376            7             Governance Specialist\n",
            "37  394            7         Risk & Compliance Manager\n",
            "38  408            7                   Program Manager\n",
            "39  409            7                    SOA Specialist\n",
            "41  685            7        Incident & Problem Manager\n",
            "42  686            7                UX / UI Specialist\n",
            "45  689            7                 (Big) Data Expert\n",
            "56  756            7             GDPR Specialist / DPO\n",
            "58  758            7        Presales / Technical Sales\n",
            "59  760            7              Enterprise Architect\n",
            "60  761            7                   DevOps Engineer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ3Teyjmank2"
      },
      "source": [
        "## Preprocess data\n",
        "\n",
        "As models like BERT don't expect text as direct input, but rather **`input_ids`**, etc., we tokenize the text using the tokenizer. Here I'm using the `AutoTokenizer` API, which will automatically load the appropriate tokenizer based on the checkpoint on the hub.\n",
        "\n",
        "What's a bit tricky is that we also need to provide labels to the model. For multi-label text classification, this is a **matrix of shape (batch_size, num_labels)**. Also important: this should be a tensor of floats rather than integers, otherwise PyTorch' **BCEWithLogitsLoss** (which the model will use) will complain, as explained [here](https://discuss.pytorch.org/t/multi-label-binary-classification-result-type-float-cant-be-cast-to-the-desired-output-type-long/117915/3)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFWlSsbZaRLc"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "### preprocess function: examples, not example, because batched=True => examples is a batch\n",
        "def preprocess_data(examples, indices):\n",
        "  text = examples['text']    # Batch of texts\n",
        "\n",
        "  encoding = tokenizer(        # Tokenize text\n",
        "      text,\n",
        "      truncation=True,\n",
        "      padding='max_length',\n",
        "      max_length=512,\n",
        "      return_tensors='pt'      # Return PyTorch tensors\n",
        "  )\n",
        "\n",
        "  # Create an empty label matrix\n",
        "  labels_matrix = torch.zeros((len(text), len(labels)), dtype=torch.float32)\n",
        "\n",
        "  #print(f\"labels_matrix: {type(labels_matrix)} {labels_matrix.shape}\")\n",
        "\n",
        "  # Populate label matrix\n",
        "  for idx, label in enumerate(labels):\n",
        "\n",
        "    #print(f\"idx:{idx} label:{label}\")\n",
        "\n",
        "    if label in examples:\n",
        "      labels_matrix[:, idx] = torch.tensor(\n",
        "          [1.0 if val else 0.0 for val in examples[label]],\n",
        "          dtype=torch.float32\n",
        "      )\n",
        "\n",
        "  #print(f\"labels_matrix: {type(labels_matrix)} {labels_matrix.shape}\")\n",
        "\n",
        "  # Add labels to the encoding\n",
        "  encoding['labels'] = labels_matrix\n",
        "\n",
        "  #print(f\"encoding['labels']: {encoding['labels']}\")\n",
        "\n",
        "  return encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_dataset = dataset.map(\n",
        "    preprocess_data,\n",
        "    batched=True,\n",
        "    remove_columns=dataset['train'].column_names,\n",
        "    with_indices=True\n",
        ")"
      ],
      "metadata": {
        "id": "kuft8rJe2Q03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = encoded_dataset['validation'][0]\n",
        "\n",
        "print(f\"example['labels']:  {type(example['labels'])} {example['labels'].shape}\\n{example['labels']}\")\n"
      ],
      "metadata": {
        "id": "HnRFf2pj2ee2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0enAb0W9o25W"
      },
      "outputs": [],
      "source": [
        "example = encoded_dataset['validation'][0]\n",
        "\n",
        "print(f\"example.keys(): {example.keys()}\")\n",
        "print(f\"example['input_ids']: {example['input_ids']}\")\n",
        "print(f\"example['token_type_ids']: {example['token_type_ids']}\")\n",
        "print(f\"example['attention_mask']: {example['attention_mask']}\")\n",
        "print(f\"example['labels']: {example['labels']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0McCtJ8HRJY"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(example['input_ids'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4Dx95t2o6N9"
      },
      "outputs": [],
      "source": [
        "example['labels']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LAyThO7Jnvj"
      },
      "outputs": [],
      "source": [
        "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgpKXDfvKBxn"
      },
      "source": [
        "Finally, we set the format of our data to PyTorch tensors. This will turn the training, validation and test sets into standard PyTorch [datasets](https://pytorch.org/docs/stable/data.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4ENBTdulBEI"
      },
      "outputs": [],
      "source": [
        "encoded_dataset.set_format(\"torch\")    # Ensures correctness and compatibility with PyTorch pipelines\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5qSmCgWefWs"
      },
      "source": [
        "## Define model\n",
        "\n",
        "Here we define a **model that includes a pre-trained base (i.e. the weights from bert-base-uncased) are loaded, with a random initialized classification head (linear layer) on top**. One should fine-tune this head, together with the pre-trained base on a labeled dataset.\n",
        "\n",
        "This is also printed by the warning.\n",
        "\n",
        "We set the `problem_type` to be \"multi_label_classification\", as this will make sure the appropriate loss function is used (namely [**BCEWithLogitsLoss**](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)). We also make sure the output layer has `len(labels)` output neurons, and we set the id2label and label2id mappings."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### device\n",
        "\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"device: {device}\")\n"
      ],
      "metadata": {
        "id": "b-PRy5qsYpKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XPL1Z_RegBF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
        "                                                           problem_type=\"multi_label_classification\",\n",
        "                                                           num_labels=len(labels),\n",
        "                                                           id2label=id2label,\n",
        "                                                           label2id=label2id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjJGEXShp7te"
      },
      "source": [
        "## Train the model!\n",
        "\n",
        "We are going to train the model using HuggingFace's Trainer API. This requires us to define 2 things:\n",
        "\n",
        "* `TrainingArguments`, which specify training hyperparameters. All options can be found in the [docs](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments). Below, we for example specify that we want to evaluate after every epoch of training, we would like to save the model every epoch, we set the learning rate, the batch size to use for training/evaluation, how many epochs to train for, and so on.\n",
        "* a `Trainer` object (docs can be found [here](https://huggingface.co/transformers/main_classes/trainer.html#id1))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5a8_vIKqr7P"
      },
      "outputs": [],
      "source": [
        "batch_size  = 8\n",
        "metric_name = \"f1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dR2GmpvDqbuZ"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir                  = r'C:\\tmp\\BERT_results\\output',\n",
        "    overwrite_output_dir        = True,\n",
        "    logging_dir                 = r'C:\\tmp\\BERT_results\\logs',\n",
        "    logging_steps               = 50,\n",
        "    save_steps                  = 100,\n",
        "    save_total_limit            = 2,\n",
        "    eval_strategy               = \"epoch\",\n",
        "    save_strategy               = \"epoch\",\n",
        "    learning_rate               = 2e-5,\n",
        "    per_device_train_batch_size = batch_size,\n",
        "    per_device_eval_batch_size  = batch_size,\n",
        "    num_train_epochs            = 5,\n",
        "    weight_decay                = 0.01,\n",
        "    load_best_model_at_end      = True,\n",
        "    metric_for_best_model       = metric_name,\n",
        "    #push_to_hub                 = True,\n",
        "    run_name                   = \"BERT-multilabel-lr2e5-epochs5-datasetHF_128_18_54\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_v2fPFFJ3-v"
      },
      "source": [
        "We are also going to compute metrics while training. For this, we need to define a `compute_metrics` function, that returns a dictionary with the desired metric values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "797b2WHJqUgZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, average_precision_score, accuracy_score\n",
        "from transformers import EvalPrediction\n",
        "\n",
        "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
        "def multi_label_metrics(predictions, labels, threshold=0.2):\n",
        "    _average = 'micro'    # 'micro' or 'weighted'\n",
        "\n",
        "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs   = sigmoid(torch.Tensor(predictions))\n",
        "\n",
        "    # next, use threshold to turn them into integer predictions\n",
        "    y_pred = np.zeros(probs.shape)\n",
        "    y_pred[np.where(probs >= threshold)] = 1\n",
        "\n",
        "    # finally, compute metrics\n",
        "    y_true               = labels\n",
        "    f1                   = f1_score               (y_true=y_true, y_pred=y_pred, average=_average)    #, zero_division=1)\n",
        "    precision            = precision_score        (y_true=y_true, y_pred=y_pred, average=_average)    #, zero_division=1)\n",
        "    recall               = recall_score           (y_true=y_true, y_pred=y_pred, average=_average)    #, zero_division=1)\n",
        "    roc_auc              = roc_auc_score          (y_true=y_true, y_score=probs, average=_average)\n",
        "    precision_recall_auc = average_precision_score(y_true=y_true, y_score=probs, average=_average)\n",
        "    accuracy             = accuracy_score         (y_true=y_true, y_pred=y_pred)\n",
        "\n",
        "    # return as dictionary\n",
        "    metrics = {'f1'                  : f1,\n",
        "               'precision'           : precision,\n",
        "               'recall'              : recall,\n",
        "               'roc_auc'             : roc_auc,\n",
        "               'precision_recall_auc': precision_recall_auc,\n",
        "               'accuracy'            : accuracy}\n",
        "    return metrics\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
        "    result = multi_label_metrics(\n",
        "        predictions=preds,\n",
        "        labels=p.label_ids)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxNo4_TsvzDm"
      },
      "source": [
        "Let's verify a batch as well as a forward pass:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlOgGiojuWwG"
      },
      "outputs": [],
      "source": [
        "encoded_dataset['train'][0]['labels'].type()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y41Kre_jvD7x"
      },
      "outputs": [],
      "source": [
        "encoded_dataset['train']['input_ids'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxWcnZ8ku12V"
      },
      "outputs": [],
      "source": [
        "#forward pass\n",
        "print(f\"inputids:       {type(encoded_dataset['train']['input_ids'][0])}      {encoded_dataset['train']['input_ids'][0].shape}\")\n",
        "print(f\"attention_mask: {type(encoded_dataset['train']['attention_mask'][0])} {encoded_dataset['train']['attention_mask'][0].shape}\")\n",
        "print(f\"labels:         {type(encoded_dataset['train'][0]['labels'])}         {encoded_dataset['train'][0]['labels'].shape}\")\n",
        "\n",
        "outputs = model(input_ids      = encoded_dataset['train']['input_ids'][0].unsqueeze(0),\n",
        "                attention_mask = encoded_dataset['train']['attention_mask'][0].unsqueeze(0),\n",
        "                labels         = encoded_dataset['train'][0]['labels'].unsqueeze(0)\n",
        "          )\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-X2brZcv0X6"
      },
      "source": [
        "Let's start training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chq_3nUz73ib"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXmFds8js6P8"
      },
      "outputs": [],
      "source": [
        "train_ouput = trainer.train()\n",
        "\n",
        "print(f\"train_ouput.global_step: {type(train_ouput.global_step)} {train_ouput.global_step}\")          # Total training steps\n",
        "print(f\"train_ouput.training_loss: {type(train_ouput.training_loss)} {train_ouput.training_loss}\")    # Final training loss\n",
        "print(f\"train_ouput.metrics: {type(train_ouput.metrics)} {train_ouput.metrics}\")                      # Training metrics\n",
        "print(f\"train_ouput.state_dict: {type(train_ouput.state_dict)} {train_ouput.state_dict}\")             # Model state dictionary\n",
        "print(f\"train_ouput.log_history: {type(train_ouput.log_history)} {train_ouput.log_history}\")         # Log history\n",
        "print(f\"train_ouput.prediction_step: {type(train_ouput.prediction_step)} {train_ouput.prediction_step}\") # Prediction step\n",
        "print(f\"train_ouput.optimizer: {type(train_ouput.optimizer)} {train_ouput.optimizer}\")                 # Optimizer state\n",
        "print(f\"train_ouput.lr_scheduler: {type(train_ouput.lr_scheduler)} {train_ouput.lr_scheduler}\")         # Learning rate scheduler state\n",
        "print(f\"train_ouput.epoch: {type(train_ouput.epoch)} {train_ouput.epoch}\")                             # Current epoch\n",
        "print(f\"train_ouput.state: {type(train_ouput.state)} {train_ouput.state}\")                             # Trainer state\n",
        "print(f\"train_ouput.world_size: {type(train_ouput.world_size)} {train_ouput.world_size}\")               # World size\n",
        "print(f\"train_ouput.name: {type(train_ouput.name)} {train_ouput.name}\")                                 # Name of the trainer\n",
        "print(f\"train_ouput.args: {type(train_ouput.args)} {train_ouput.args}\")                                 # Training arguments\n",
        "print(f\"train_ouput.train_dataset: {type(train_ouput.train_dataset)} {train_ouput.train_dataset}\")     # Training dataset\n",
        "print(f\"train_ouput.eval_dataset: {type(train_ouput.eval_dataset)} {train_ouput.eval_dataset}\")         # Evaluation dataset\n",
        "print(f\"train_ouput.data_collator: {type(train_ouput.data_collator)} {train_ouput.data_collator}\")     # Data collator\n",
        "print(f\"train_ouput.compute_metrics: {type(train_ouput.compute_metrics)} {train_ouput.compute_metrics}\") # Compute metrics function\n",
        "print(f\"train_ouput.callbacks: {type(train_ouput.callbacks)} {train_ouput.callbacks}\")                   # Callbacks\n",
        "print(f\"train_ouput.optimizers: {type(train_ouput.optimizers)} {train_ouput.optimizers}\")                 # Optimizers\n",
        "print(f\"train_ouput.lr_schedulers: {type(train_ouput.lr_schedulers)} {train_ouput.lr_schedulers}\")       # Learning rate schedulers\n",
        "print(f\"train_ouput.label_names: {type(train_ouput.label_names)} {train_ouput.label_names}\")             # Label names\n",
        "print(f\"train_ouput.model_class: {type(train_ouput.model_class)} {train_ouput.model_class}\")             # Model class\n",
        "print(f\"train_ouput.model_init_args: {type(train_ouput.model_init_args)} {train_ouput.model_init_args}\") # Model initialization arguments\n",
        "print(f\"train_ouput.model_name_or_path: {type(train_ouput.model_name_or_path)} {train_ouput.model_name_or_path}\") # Model name or path\n",
        "print(f\"train_ouput.tokenizer: {type(train_ouput.tokenizer)} {train_ouput.tokenizer}\")                     # Tokenizer\n",
        "print(f\"train_ouput.train_dataloader: {type(train_ouput.train_dataloader)} {train_ouput.train_dataloader}\") # Training dataloader\n",
        "print(f\"train_ouput.eval_dataloader: {type(train_ouput.eval_dataloader)} {train_ouput.eval_dataloader}\")     # Evaluation dataloader\n",
        "print(f\"train_ouput.train_dataloader_batch_size: {type(train_ouput.train_dataloader_batch_size)} {train_ouput.train_dataloader_batch_size}\") # Training dataloader batch size\n",
        "print(f\"train_ouput.eval_dataloader_batch_size: {type(train_ouput.eval_dataloader_batch_size)} {train_ouput.eval_dataloader_batch_size}\")     # Evaluation dataloader batch size\n",
        "print(f\"train_ouput.train_dataloader_drop_last: {type(train_ouput.train_dataloader_drop_last)} {train_ouput.train_dataloader_drop_last}\") # Training dataloader drop last\n",
        "print(f\"train_ouput.eval_dataloader_drop_last: {type(train_ouput.eval_dataloader_drop_last)} {train_ouput.eval_dataloader_drop_last}\")     # Evaluation dataloader drop last\n",
        "print(f\"train_ouput.train_dataloader_num_workers: {type(train_ouput.train_dataloader_num_workers)} {train_ouput.train_dataloader_num_workers}\") # Training dataloader num workers\n",
        "print(f\"train_ouput.eval_dataloader_num_workers: {type(train_ouput.eval_dataloader_num_workers)} {train_ouput.eval_dataloader_num_workers}\")     # Evaluation dataloader num workers\n",
        "print(f\"train_ouput.train_dataloader_pin_memory: {type(train_ouput.train_dataloader_pin_memory)} {train_ouput.train_dataloader_pin_memory}\") # Training dataloader pin memory\n",
        "print(f\"train_ouput.eval_dataloader_pin_memory: {type(train_ouput.eval_dataloader_pin_memory)} {train_ouput.eval_dataloader_pin_memory}\")     # Evaluation dataloader pin memory\n",
        "print(f\"train_ouput.train_dataloader_persistent_workers: {type(train_ouput.train_dataloader_persistent_workers)} {train_ouput.train_dataloader_persistent_workers}\") # Training dataloader persistent workers\n",
        "print(f\"train_ouput.eval_dataloader_persistent_workers: {type(train_ouput.eval_dataloader_persistent_workers)} {train_ouput.eval_dataloader_persistent_workers}\")     # Evaluation dataloader persistent workers\n",
        "print(f\"train_ouput.train_dataloader_prefetch_factor: {type(train_ouput.train_dataloader_prefetch_factor)} {train_ouput.train_dataloader_prefetch_factor}\") # Training dataloader prefetch factor\n",
        "print(f\"train_ouput.eval_dataloader_prefetch_factor: {type(train_ouput.eval_dataloader_prefetch_factor)} {train_ouput.eval_dataloader_prefetch_factor}\")     # Evaluation dataloader prefetch factor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiloh9eMK91o"
      },
      "source": [
        "## Evaluate\n",
        "\n",
        "After training, we evaluate our model on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMlebJ83LRYG"
      },
      "outputs": [],
      "source": [
        "eval_results = trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raise Exception(\"STOP\")"
      ],
      "metadata": {
        "id": "BkB4b3qw-MxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nmvJp0pLq-3"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Let's test the model on a new sentence:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHY6-zJ1YVDb"
      },
      "source": [
        "id: 323697\n",
        "\"Voor een klant van Talencia ben ik opzoek naar een Senior Full Stack Developer (Java & Angular) Job beschrijving Als Developer zal je een bestaand team toevoegen en meewerken aan de buitbouw van webapplicaties op Azure. Dit is om bestaande applicaties te vervangen die end-of-live zijn. Het project is al in volle realisatie. Profiel Zeer goede kennis van Java en Angular Goede kennis van Azure DevOps, AKS,.. is een grote pluspunt Kennis van Docker/ SQL/ OAuth/PWA/ RESTful API is vereist Taal: Nederlands met kennis van Engels Extra informatie Teamspeler met ervaring in Agile methodiek is vereist. Als je meer informatie wilt en dit klinkt interessant voor u, aarzel dan niet om uw meest recente CV door te sturen. Het kan zijn dat ik niet beschik over uw meest recente CV en dat ik daarom u deze opportuniteit doorstuur dat niet geschikt is voor u. Als u iemand kent dat deze missie interessant zou vinden mag u deze vacature doorsturen. Met vriendelijke groeten,\"\n",
        "\n",
        "['142', '147', '149', '154', '156', '157', '173', '409', '685', '689']\n",
        "\n",
        "---\n",
        "\n",
        "id: 323611,\"Atcon Global - Project Management Officer / PMO team management Atcon Global For one of our clients, we are looking for an experienced Project Management Officer (PMO) / Project Manager (PM) for permanent employment in the Flanders region. Your role? As a PMO, you will play a crucial role in setting up and improving our project management processes. You will not only be responsible for developing PM standards, but also for carrying out projects independently as a Project Manager. Your duties and responsibilities will include: Developing PMO and project management standards Executing and managing complex digital projects Oversee project progress and report to senior management Follow-up of project budgets, project selection, capacity planning and resource management Coaching and training project managers Identifying and managing project risks Promote continuous improvement in the project management domain Collaborate with stakeholders and external partners Who are we looking for? Bachelor's or master's degree 5+ years in a similar role in a dynamic organization Expertise in project management methods (Agile, Scrum, Lean, Kanban) Strong analytical and problem-solving skills Excellent communication and stakeholder management Experience in team management with clear objectives Proactive, Hands-on mentality and result-oriented Fluent in Dutch and English; French is a plus What's on offer? A dynamic and varied role in a growing, ambitious and innovative company Numerous opportunities for personal growth and career development A competitive salary with customizable benefits A friendly, collegial working atmosphere Flexible working hours, possibility to work from home\",\"171,170,794,800,798,797,138,139,352\"\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fxjfr8PLD42"
      },
      "outputs": [],
      "source": [
        "#text = \"Voor een klant van Talencia ben ik opzoek naar een Senior Full Stack Developer (Java & Angular) Job beschrijving Als Developer zal je een bestaand team toevoegen en meewerken aan de buitbouw van webapplicaties op Azure. Dit is om bestaande applicaties te vervangen die end-of-live zijn. Het project is al in volle realisatie. Profiel Zeer goede kennis van Java en Angular Goede kennis van Azure DevOps, AKS,.. is een grote pluspunt Kennis van Docker/ SQL/ OAuth/PWA/ RESTful API is vereist Taal: Nederlands met kennis van Engels Extra informatie Teamspeler met ervaring in Agile methodiek is vereist. Als je meer informatie wilt en dit klinkt interessant voor u, aarzel dan niet om uw meest recente CV door te sturen. Het kan zijn dat ik niet beschik over uw meest recente CV en dat ik daarom u deze opportuniteit doorstuur dat niet geschikt is voor u. Als u iemand kent dat deze missie interessant zou vinden mag u deze vacature doorsturen. Met vriendelijke groeten\"\n",
        "#text = \"Atcon Global - Project Management Officer / PMO team management Atcon Global For one of our clients, we are looking for an experienced Project Management Officer (PMO) / Project Manager (PM) for permanent employment in the Flanders region. Your role? As a PMO, you will play a crucial role in setting up and improving our project management processes. You will not only be responsible for developing PM standards, but also for carrying out projects independently as a Project Manager. Your duties and responsibilities will include: Developing PMO and project management standards Executing and managing complex digital projects Oversee project progress and report to senior management Follow-up of project budgets, project selection, capacity planning and resource management Coaching and training project managers Identifying and managing project risks Promote continuous improvement in the project management domain Collaborate with stakeholders and external partners Who are we looking for? Bachelor's or master's degree 5+ years in a similar role in a dynamic organization Expertise in project management methods (Agile, Scrum, Lean, Kanban) Strong analytical and problem-solving skills Excellent communication and stakeholder management Experience in team management with clear objectives Proactive, Hands-on mentality and result-oriented Fluent in Dutch and English; French is a plus What's on offer? A dynamic and varied role in a growing, ambitious and innovative company Numerous opportunities for personal growth and career development A competitive salary with customizable benefits A friendly, collegial working atmosphere Flexible working hours, possibility to work from home\"\n",
        "#encoding = tokenizer(text, return_tensors=\"pt\")\n",
        "#encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
        "\n",
        "#outputs = trainer.model(**encoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8THm5-XgNHPm"
      },
      "source": [
        "The logits that come out of the model are of shape (batch_size, num_labels). As we are only forwarding a single sentence through the model, the `batch_size` equals 1. The logits is a tensor that contains the (unnormalized) scores for every individual label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOBosj4UL2tU"
      },
      "outputs": [],
      "source": [
        "#logits = outputs.logits\n",
        "#logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC4XdDaHNVcd"
      },
      "source": [
        "To turn them into actual predicted labels, we first apply a sigmoid function independently to every score, such that every score is turned into a number between 0 and 1, that can be interpreted as a \"probability\" for how certain the model is that a given class belongs to the input text.\n",
        "\n",
        "Next, we use a threshold (typically, 0.5) to turn every probability into either a 1 (which means, we predict the label for the given example) or a 0 (which means, we don't predict the label for the given example)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2ojrqxoerup"
      },
      "outputs": [],
      "source": [
        "# apply sigmoid + threshold\n",
        "#import torch\n",
        "\n",
        "#sigmoid = torch.nn.Sigmoid()\n",
        "#probs = sigmoid(logits.squeeze().cpu())\n",
        "#predictions = np.zeros(probs.shape)\n",
        "#predictions[np.where(probs >= 0.2)] = 1\n",
        "# turn predicted id's into actual label names\n",
        "#predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
        "#print(predicted_labels)\n",
        "\n",
        "#raise Exception(\"STOP\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bdg0HQ5LcKkE"
      },
      "source": [
        "**id**: 323697\n",
        "\n",
        "**MySQL**: \"142,189,190,754,208,794,676,811,812,139,138\" (only 142=\"Developer / Analyst Programmer\") is a 7-skill)\n",
        "\n",
        "**predicted_labels**: ['148', '152', '154', '409'] : all are 7-skills: 148=\"Technical Analyst\", 152=\"Technical Writer\", 154=\"Database Admininistrator\"\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**id**: 323611\n",
        "\n",
        "**MySQL**: \"171,170,794,800,798,797,138,139,352\"            \n",
        "           171: Project Mgmt Officer (PMO)  \n",
        "           170: Project Manager / Coordinator\n",
        "\n",
        "**predicted labels**: 409:  \n",
        "                      409: \"SOA Specialist\" (SOA: Service Oriented Architecture)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZJG-hcc_dzD"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"skills_model\")    # Save locally the trained model and tokenizer: saves the model weights, the tokenizer, the model configuration file (\"config.json\")\n",
        "\n",
        "import json\n",
        "\n",
        "with open(\"training_metrics.json\", 'w') as f:\n",
        "    json.dump(trainer.state.log_history,f)\n",
        "\n",
        "with open(\"eval_metrics.json\", 'w') as f:\n",
        "    json.dump(eval_results, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers huggingface_hub\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "Kz6KuaNQMWVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import create_repo, HfApi\n",
        "from huggingface_hub.utils import RepositoryNotFoundError\n",
        "\n",
        "repo_id = 'claudelepere/skills_model'\n",
        "api     = HfApi()\n",
        "try:\n",
        "    api.repo_info(repo_id)\n",
        "    print(f\"repo_id: {repo_id}\")\n",
        "except RepositoryNotFoundError:\n",
        "    create_repo(repo_id, private=True)\n",
        "    print(f\"Repo {repo_id} created succesfully as a private repo.\")"
      ],
      "metadata": {
        "id": "RGYrmqYZM3OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"skills_model\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"skills_model\")\n",
        "\n",
        "model.push_to_hub(repo_id)\n",
        "tokenizer.push_to_hub(repo_id)"
      ],
      "metadata": {
        "id": "0C2F-c6MTGHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Z3mQMr9pojx"
      },
      "outputs": [],
      "source": [
        "raise Exception(\"STOP\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "model     = AutoModelForSequenceClassification.from_pretrained(repo_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(repo_id)"
      ],
      "metadata": {
        "id": "m-qzTCqOTqHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text      = \"Sample text for prediction\"\n",
        "#text = \"Voor een klant van Talencia ben ik opzoek naar een Senior Full Stack Developer (Java & Angular) Job beschrijving Als Developer zal je een bestaand team toevoegen en meewerken aan de buitbouw van webapplicaties op Azure. Dit is om bestaande applicaties te vervangen die end-of-live zijn. Het project is al in volle realisatie. Profiel Zeer goede kennis van Java en Angular Goede kennis van Azure DevOps, AKS,.. is een grote pluspunt Kennis van Docker/ SQL/ OAuth/PWA/ RESTful API is vereist Taal: Nederlands met kennis van Engels Extra informatie Teamspeler met ervaring in Agile methodiek is vereist. Als je meer informatie wilt en dit klinkt interessant voor u, aarzel dan niet om uw meest recente CV door te sturen. Het kan zijn dat ik niet beschik over uw meest recente CV en dat ik daarom u deze opportuniteit doorstuur dat niet geschikt is voor u. Als u iemand kent dat deze missie interessant zou vinden mag u deze vacature doorsturen. Met vriendelijke groeten\"\n",
        "text = \"Atcon Global - Project Management Officer / PMO team management Atcon Global For one of our clients, we are looking for an experienced Project Management Officer (PMO) / Project Manager (PM) for permanent employment in the Flanders region. Your role? As a PMO, you will play a crucial role in setting up and improving our project management processes. You will not only be responsible for developing PM standards, but also for carrying out projects independently as a Project Manager. Your duties and responsibilities will include: Developing PMO and project management standards Executing and managing complex digital projects Oversee project progress and report to senior management Follow-up of project budgets, project selection, capacity planning and resource management Coaching and training project managers Identifying and managing project risks Promote continuous improvement in the project management domain Collaborate with stakeholders and external partners Who are we looking for? Bachelor's or master's degree 5+ years in a similar role in a dynamic organization Expertise in project management methods (Agile, Scrum, Lean, Kanban) Strong analytical and problem-solving skills Excellent communication and stakeholder management Experience in team management with clear objectives Proactive, Hands-on mentality and result-oriented Fluent in Dutch and English; French is a plus What's on offer? A dynamic and varied role in a growing, ambitious and innovative company Numerous opportunities for personal growth and career development A competitive salary with customizable benefits A friendly, collegial working atmosphere Flexible working hours, possibility to work from home\"\n",
        "#text = \"Vivid Resourcing - Chief Technology Officer CTO, reliability, business goals Vivid Resourcing We're partnered with a leading sustainability-oriented company near Brussels, aiming to combat high pollution rates worldwide. They are currently working on a unique application that rewards workers for reducing their carbon footprint, whilst also maintaining and even improving profits. Together we are seeking a visionary Chief Technology Officer (CTO) who aligns with their mission and ambitions. The ideal candidate will possess a strong hands-on technical background, proven management experience, and strong business acumen. This role requires a strategic thinker who can drive technological direction and support the company's growth objectives of transitioning from a scale-up to an established business entity, so any past experience leading teams in this manner would go a long way. Key responsibilities Develop and execute the company's technological vision and strategy Lead and mentor a team of engineers and technologists Oversee all technical aspects of the company, ensuring alignment with business goals Drive innovation in regenerative sustainable technologies and carbon measurement systems Collaborate with cross-functional teams to integrate technology solutions Ensure the reliability, security, and scalability of technological infrastructures Foster a culture of continuous improvement and technical excellence Qualifications Experience leading a team within a small to medium sized company Strong technical background in software development/data analytics/system architecture Bachelor's or Master's degree in either an IT or Business related field Experience in the agriculture or environmental sectors is a plus Proven management skills with the ability to lead, communicate and inspire a diverse team Excellent business acumen and strategic thinking Strong problem-solving skills and the ability to make informed decisions in a fast-paced environment Offer Taking charge of a genuinely impactful product, using your direction for the good of the environment Complete responsibility over a technical team, with management responsibilities Up to 110,000 EUR gross for experienced applicants, which can then be increased further Full benefits package including mobility costs Flexible hybrid work Inclusive work environment If this role interests you, attach a CV and apply today!\"\n",
        "inputs    = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
        "threshold = 0.5\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    probs = torch.sigmoid(logits)\n",
        "    #predictions = torch.where(probs >= threshold, torch.ones_like(probs), torch.zeros_like(probs))\n",
        "    #predictions = torch.argmax(probs, dim=-1) if model.config.num_labels > 1 else torch.where(probs >= threshold, torch.ones_like(probs), torch.zeros_like(probs))\n",
        "    preds = (probs > threshold).int()\n",
        "    print(f\"probs: {probs} preds: {preds}\")\n",
        "    print()\n",
        "    for label, Value, prob, pred in zip(filtered_skill_df['Id'], filtered_skill_df['Value'], probs.squeeze(), preds.squeeze()):\n",
        "      #if (pred == 1):\n",
        "      print(f\"label: {label} logits: {logits} prob: {prob.item():.4f} pred: {int(pred.item())} {Value}\")\n"
      ],
      "metadata": {
        "id": "pXIBqQXiUOo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNJ8nrkP5dz3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Copy files to your Google Drive\n",
        "!cp -r my_model /content/drive/MyDrive/\n",
        "!cp training_metrics.json /content/drive/MyDrive/\n",
        "!cp eval_metrics.json /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGdHS6CHCoWH"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "#!zip -r my_model.zip my_model\n",
        "#!split -b 100M my_model.zip my_model_part_\n",
        "#!unzip - my_model.zip\n",
        "\n",
        "# for part in ['my_model_part_aa', 'my_model_part_ab', 'my_model_part_ac']:  # Adjust based on number of parts\n",
        "#    files.download(part)\n",
        "#files.download(\"my_model_part_aa\")\n",
        "#files.download(\"my_model_part_ab\")\n",
        "#files.download(\"my_model_part_ac\")\n",
        "#files.download(\"my_model_part_ad\")\n",
        "#!md5sum my_model.zip\n",
        "#files.download(\"training_metrics.json\")\n",
        "#files.download(\"eval_metrics.json\")\n",
        "\n",
        "#uploaded = files.upload()\n",
        "#!md5sum my_model.zip\n",
        "#!md5sum training_metrics.json\n",
        "#!md5sum eval_metrics.json\n",
        "\n",
        "#!unzip my_model.zip -d my_model_unzip\n",
        "\n",
        "#raise Exception(\"STOP\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNNg-HJE93lq"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"/content/my_model_unzip/my_model\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/my_model_unzip/my_model\")\n",
        "\n",
        "import json\n",
        "with open(\"/content/training_metrics.json\", 'r') as f:\n",
        "    training_metrics = json.load(f)\n",
        "with open(\"/content/eval_metrics.json\", 'r') as f:\n",
        "    eval_metrics = json.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzwcmVVvXgXR"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "#text = \"Voor een klant van Talencia ben ik opzoek naar een Senior Full Stack Developer (Java & Angular) Job beschrijving Als Developer zal je een bestaand team toevoegen en meewerken aan de buitbouw van webapplicaties op Azure. Dit is om bestaande applicaties te vervangen die end-of-live zijn. Het project is al in volle realisatie. Profiel Zeer goede kennis van Java en Angular Goede kennis van Azure DevOps, AKS,.. is een grote pluspunt Kennis van Docker/ SQL/ OAuth/PWA/ RESTful API is vereist Taal: Nederlands met kennis van Engels Extra informatie Teamspeler met ervaring in Agile methodiek is vereist. Als je meer informatie wilt en dit klinkt interessant voor u, aarzel dan niet om uw meest recente CV door te sturen. Het kan zijn dat ik niet beschik over uw meest recente CV en dat ik daarom u deze opportuniteit doorstuur dat niet geschikt is voor u. Als u iemand kent dat deze missie interessant zou vinden mag u deze vacature doorsturen. Met vriendelijke groeten\"\n",
        "text = \"Atcon Global - Project Management Officer / PMO team management Atcon Global For one of our clients, we are looking for an experienced Project Management Officer (PMO) / Project Manager (PM) for permanent employment in the Flanders region. Your role? As a PMO, you will play a crucial role in setting up and improving our project management processes. You will not only be responsible for developing PM standards, but also for carrying out projects independently as a Project Manager. Your duties and responsibilities will include: Developing PMO and project management standards Executing and managing complex digital projects Oversee project progress and report to senior management Follow-up of project budgets, project selection, capacity planning and resource management Coaching and training project managers Identifying and managing project risks Promote continuous improvement in the project management domain Collaborate with stakeholders and external partners Who are we looking for? Bachelor's or master's degree 5+ years in a similar role in a dynamic organization Expertise in project management methods (Agile, Scrum, Lean, Kanban) Strong analytical and problem-solving skills Excellent communication and stakeholder management Experience in team management with clear objectives Proactive, Hands-on mentality and result-oriented Fluent in Dutch and English; French is a plus What's on offer? A dynamic and varied role in a growing, ambitious and innovative company Numerous opportunities for personal growth and career development A competitive salary with customizable benefits A friendly, collegial working atmosphere Flexible working hours, possibility to work from home\"\n",
        "encoding = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# Define the device based on availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move the model to the device\n",
        "model.to(device)\n",
        "# Move encoding to the device of the model\n",
        "encoding = {k: v.to(device) for k,v in encoding.items()}\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():    # no gradients needed for inference. Forward pass\n",
        "    outputs = model(**encoding)\n",
        "\n",
        "# Get logits from the model's output\n",
        "logits = outputs.logits\n",
        "\n",
        "# Apply softmax/sigmoid based on the type of classification\n",
        "if model.config.num_labels == 1:\n",
        "    probs = torch.sigmoid(logits.squeeze())\n",
        "else:\n",
        "    #probs = torch.softmax(logits, dim=1).squeeze()\n",
        "    probs = torch.sigmoid(logits)\n",
        "\n",
        "\n",
        "\n",
        "# To get predictions\n",
        "threshold = 0.5\n",
        "#predictions = torch.where(probs >= threshold, torch.ones_like(probs), torch.zeros_like(probs))\n",
        "#predictions = torch.argmax(probs, dim=-1) if model.config.num_labels > 1 else torch.where(probs >= threshold, torch.ones_like(probs), torch.zeros_like(probs))\n",
        "predictions = (probs > threshold).float()\n",
        "print(\"Predictions:\", predictions)\n",
        "print()\n",
        "\n",
        "# Turn predicted id's into actual label names\n",
        "print(\"Probabilites:\", probs)\n",
        "\n",
        "#[id2label[idx] for idx, label in enumerate(predictions['labels']) if label == 1.0]\n",
        "\n",
        "#predicted_labels = [id2label[idx.item()] for idx in predictions]\n",
        "#print(predicted_labels)\n",
        "\n",
        "for label, prob, pred in zip(labels, probs.squeeze(), predictions.squeeze()):\n",
        "  print(f\"Label: {label}: Probability: {prob.item():.4f} {int(pred.item())}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvipL3S0g8EL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}