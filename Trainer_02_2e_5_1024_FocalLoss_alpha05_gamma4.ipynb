{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudelepere/ML_GitHub/blob/main/Trainer_02_2e_5_1024_FocalLoss_alpha05_gamma4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OIjOPWowcJq",
        "outputId": "b52552ee-ec8e-4cde-c983-b855bf982e6c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: The following features are always enabled: truststore. \n",
            "WARNING: The following features are always enabled: truststore. \n",
            "WARNING: The following features are always enabled: truststore. \n",
            "WARNING: The following features are always enabled: truststore. \n",
            "WARNING: The following features are always enabled: truststore. \n"
          ]
        }
      ],
      "source": [
        "!pip install -q accelerate\n",
        "!pip install -q huggingface_hub\n",
        "!pip install -q scikit-learn\n",
        "\n",
        "# transformers and datasets are Hugging Face libraries\n",
        "!pip install -q transformers datasets\n",
        "\n",
        "!pip install -q wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ync6uXy-wcJs"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgQnuPvUFNsY"
      },
      "outputs": [],
      "source": [
        "from datasets              import DatasetDict\n",
        "from google.colab          import auth, drive, files, userdata\n",
        "from huggingface_hub       import create_repo, login, upload_file, hf_hub_download\n",
        "from huggingface_hub.utils import RepositoryNotFoundError\n",
        "from sklearn.metrics       import accuracy_score, average_precision_score, classification_report, f1_score, precision_score, precision_recall_curve, precision_recall_fscore_support, recall_score, roc_auc_score\n",
        "from torch.utils.data      import DataLoader\n",
        "from tqdm.auto             import tqdm\n",
        "from transformers          import AdamW, EvalPrediction, LongformerTokenizerFast, LongformerForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.nn              import BCEWithLogitsLoss, Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dro6dkjyHNA7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab    import userdata\n",
        "from huggingface_hub import login, hf_hub_download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNxYo9txwcJs"
      },
      "outputs": [],
      "source": [
        "# Hugging Face Authenticate\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")    # Store the key in os.environ\n",
        "hf_token               = os.environ.get('HF_TOKEN')\n",
        "login(token=hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDimlzpeMDyl"
      },
      "outputs": [],
      "source": [
        "# Verify\n",
        "!huggingface-cli whoami"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtCrlJwnwcJs"
      },
      "outputs": [],
      "source": [
        "file_path = hf_hub_download(\n",
        "    repo_id   =\"claudelepere/skill_classification\",\n",
        "    repo_type = \"dataset\",\n",
        "    filename  = \"test_model_eval_results.csv\"\n",
        ")\n",
        "print(f\"file_path: {file_path}\")  # /root/.cache/huggingface/hub/datasets--claudelepere--skill_classification/snapshots/51ead81f69b1689fc19694b3f034585cde9f56e1/test_model_eval_results.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwEbNhC_wcJs"
      },
      "source": [
        "Next, open in Colab or download to local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhAd6EZJwcJt"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Check the Python version\n",
        "print(sys.version)\n",
        "print()\n",
        "\n",
        "# Get the installed packages (you can see that conda is not installed (do not install it))\n",
        "!pip list\n",
        "print()\n",
        "\n",
        "# Check system information\n",
        "!cat /etc/os-release\n",
        "!uname -m\n",
        "print()\n",
        "\n",
        "# Check the GPU details (only if the runtime type is T4 GPU)\n",
        "#!nvidia-smi\n",
        "#print()\n",
        "\n",
        "# Check RAM\n",
        "!free -h\n",
        "print()\n",
        "\n",
        "# Check disk space\n",
        "!df -h\n",
        "print()\n",
        "\n",
        "# Get environment variables\n",
        "for key, value in os.environ.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\"\"\"\n",
        "!python -V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUTMN8SSwcJt"
      },
      "outputs": [],
      "source": [
        "print(f\"currentdir: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIUfehpkwcJt"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWgIYo5DwcJt"
      },
      "outputs": [],
      "source": [
        "datasetDict_zip_file_name = \"dataset_11_12000.zip\"\n",
        "datasetDict_dir_name      = os.path.splitext(datasetDict_zip_file_name)[0]\n",
        "print(f\"datasetDict_zip_file_name: {datasetDict_zip_file_name}\")\n",
        "print(f\"datasetDict_dir_name     : {datasetDict_dir_name}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bq0JDT07wcJt"
      },
      "outputs": [],
      "source": [
        "# OOM: reduce batch size\n",
        "#      small sizes (1 to 32):            PROs: better generalization in some cases\n",
        "#                                        CONs: may produce noisier gradients\n",
        "#      large sizes (128, 256, or higer): PROs: gradients are smoother, leading to more stable training\n",
        "#                                        CONs: poorer generalization (overfitting) in some cases\n",
        "#      intermediate sizes (32, 64):      combines the benefits of small and large sizes\n",
        "batch_size = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nG0pflXGwcJt"
      },
      "outputs": [],
      "source": [
        "# OOM: enable gradient accumulation to compensate for smaller batch sizes by accumulating gradients over several steps\n",
        "#      effective batch size = per-device batch size x gradient accumulation steps;\n",
        "#      in each iteration, the model computes the gradients, these gradients are immediately used to update the model parameters\n",
        "gradient_accumulation_steps = 4  #<<<<<<<<<<<<<<<<<<< gradient_accumulation_steps may not be None => comment it in TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXrPvSWAwcJt"
      },
      "outputs": [],
      "source": [
        "# OOM: use PYTORCH_CUDA_ALLOC_CONF to handle memory fragmentation\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKeGyFYHwcJu"
      },
      "outputs": [],
      "source": [
        "# OOM: check for zombie processes\n",
        "if torch.cuda.is_available():\n",
        "  !nvidia-smi\n",
        "  torch.cuda.memory_summary()\n",
        "!ps aux | grep python\n",
        "!kill -9 <PID>\n",
        "!nvidia-smi     # Checked if killed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbyMivrEwcJu"
      },
      "outputs": [],
      "source": [
        "# OOM: use fp16 (half precision) mixed precision training\n",
        "#      reduces memory requirements by up to 50%\n",
        "fp16 = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVhoxNtUwcJu"
      },
      "source": [
        "OOM: limit the number of GPU workers: 0 (default) or 1 in Colab\n",
        "dataloader_num_workers = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZOpDR5JwcJu"
      },
      "outputs": [],
      "source": [
        "# OOM: reduce model size or input tokens\n",
        "#      1) LongformerTokenizer.from_pretrained('allenai/longformer-base/large-4096'): large/base: 435M/149M parameters\n",
        "#      2) max_length: 4096 max for Longformer; 1 word can give several tokens, stop words are NOT discarded!\n",
        "#         word_text_length_counts_sorted: jobs count                 : 50000\n",
        "#                                         jobs count under  512 words: 44794  89.59%\n",
        "#                                         jobs count under  640 words: 47894  95.79%\n",
        "#                                         jobs count under  768 words: 49123  98.25%\n",
        "#                                         jobs count under  896 words: 49691  99.38%\n",
        "#                                         jobs count under 1024 words: 49917  99.83%\n",
        "#                                         jobs count under 2048 words: 50000 100.00%\n",
        "#                                         jobs count under 4096 words: 50000 100.00%\n",
        "#max_length =  768    #      37 min    #\n",
        "max_length = 1024    #      38 min    # GPU RAM: 12.2 / 40 GB\n",
        "#max_length = 2048    # 1 hr 10 min    # GPU RAM: 21.4 / 40 GB\n",
        "#max_length = 4096    # 2 hr 10 min    # GPU RAM: 39.5 / 40 GB => OutOfMemoryError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJNB5_2HwcJu"
      },
      "outputs": [],
      "source": [
        "# OOM: free up GPU memory\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YgjmWL7wcJu"
      },
      "outputs": [],
      "source": [
        "# OOM: monitor GPU memory usage\n",
        "!nividia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KORJtBl7wcJu"
      },
      "outputs": [],
      "source": [
        "# 1 epoch is a complete pass through the entire training dataset;\n",
        "# with n datapoints and batch size = b, n/b iterations to complete 1 epoch;\n",
        "# 1 iteration is a single update of the model's parameters\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaoQWRXYwcJu"
      },
      "outputs": [],
      "source": [
        "# A common rule is to scale the learning rate proportionaly with the effective batch size\n",
        "# note: get_linear_schedule_with_warmup <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "learning_rate = 2e-5  # 1e-5 x 32/8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVi2_ymCwcJu"
      },
      "source": [
        "Reduce the number of transformers layers\n",
        "hidden_layers = 12    # 12 (default) or 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2Dfm6lvwcJu"
      },
      "outputs": [],
      "source": [
        "# Threshold: 0.5 (default)\n",
        "threshold = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIihId9jwcJu"
      },
      "outputs": [],
      "source": [
        "if fp16:\n",
        "  _fp = \"fp16\"\n",
        "else:\n",
        "  _fp = \"fp32\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gweGUl--FNsZ",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "if 'gradient_accumulation_steps' not in globals():\n",
        "  run_name = f\"Longformer-multilabel-{datasetDict_dir_name}-length{max_length}-batch{batch_size}-epochs{epochs}-lr{learning_rate}-{_fp}-threshold{threshold}\"\n",
        "else:\n",
        "  run_name = f\"Longformer-multilabel-{datasetDict_dir_name}-length{max_length}-batch{batch_size}x{gradient_accumulation_steps}-epochs{epochs}-lr{learning_rate}-{_fp}-threshold{threshold}\"\n",
        "print(f\"run_name                 : {run_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvZTXW-_ZAJ-",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def upload_unzip_dataset(file_name=datasetDict_zip_file_name):\n",
        "  # Check if the file exists\n",
        "  if not os.path.exists(file_name):\n",
        "    print(f\"'{file_name}' not found in /content. Uploading...\")\n",
        "    uploaded_files = files.upload()                              # Prompt file upload dialog\n",
        "    if file_name not in uploaded_files:\n",
        "      raise FileNotFoundError(f\"'{file_name}' was not uploaded. Please try again.\")\n",
        "    print(f\"'{file_name}' successfully uploaded to /content\")\n",
        "    uploaded_file_name = list(uploaded_files.keys())[0]          # Get the name of the uploaded file\n",
        "\n",
        "    !unzip {uploaded_file_name}\n",
        "\n",
        "    unzipped_dir_name = os.path.splitext(uploaded_file_name)[0]\n",
        "    assert unzipped_dir_name==datasetDict_dir_name, \"unzipped_dir_name != datasetDict_dir_name\"\n",
        "  else:\n",
        "    print(f\"'{datasetDict_dir_name}' already exists in /content.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0k70dawNeONH"
      },
      "outputs": [],
      "source": [
        "upload_unzip_dataset(datasetDict_zip_file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX-YLguewcJv"
      },
      "source": [
        "Hugging Face Authenticate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6KDXp3OwcJv"
      },
      "outputs": [],
      "source": [
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")    # Store the key in os.environ\n",
        "hf_token               = os.environ.get('HF_TOKEN')\n",
        "login(token=hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5Ziyt5IwcJv"
      },
      "outputs": [],
      "source": [
        "# Verify\n",
        "!huggingface-cli whoami"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPBlVmE3wcJv"
      },
      "source": [
        "Create the skill_classification repo on the Hugging Face Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1haRyaUwcJv"
      },
      "outputs": [],
      "source": [
        "HF_name         = \"claudelepere/skill_classification\"\n",
        "repo_id_model   = HF_name\n",
        "repo_id_dataset = HF_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB2OYD2ywcJv"
      },
      "outputs": [],
      "source": [
        "repo_model_url = create_repo(\n",
        "    repo_id   = repo_id_model,\n",
        "    repo_type = \"model\",\n",
        "    private   = True,\n",
        "    exist_ok  = True\n",
        ")\n",
        "print(f\"Repo model url: {repo_model_url} created successfully as a private repo.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoWP5yVgwcJv"
      },
      "outputs": [],
      "source": [
        "repo_dataset_url = create_repo(\n",
        "    repo_id   = repo_id_dataset,\n",
        "    repo_type = \"dataset\",\n",
        "    private   = True,\n",
        "    exist_ok  = True\n",
        ")\n",
        "print(f\"Repo datasets url: {repo_dataset_url} created successfully as a private repo.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OUzSg4AwcJv"
      },
      "outputs": [],
      "source": [
        "repo_id_dataset = f\"datasets/{HF_name}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lvx8Nh_Ms_c"
      },
      "outputs": [],
      "source": [
        "print(f\"repo_id_model: {repo_id_model}\")\n",
        "print(f\"repo_id_dataset: {repo_id_dataset}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T90_cKa4wcJv"
      },
      "source": [
        "W&B initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giGsdqlUwcJz"
      },
      "outputs": [],
      "source": [
        "os.environ[\"WANDB_API_KEY\"] = userdata.get(\"WANDB_API_KEY\")        # Store the key in os.environ\n",
        "wandb_api_key               = os.environ.get('WANDB_API_KEY')\n",
        "wandb.login(key=wandb_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYJMUPCONEiD"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  wandb.init(\n",
        "      project = \"skill_classification\",\n",
        "      name    = run_name,\n",
        "      entity  = \"claudelepere-c-cile-cy\",\n",
        "      config  = {\n",
        "          \"learning_rate\": learning_rate,\n",
        "          \"epochs\"       : 5,\n",
        "          \"batch_size\"   : batch_size\n",
        "      }\n",
        "  )\n",
        "except wandb.errors.CommError as err:\n",
        "  print(f\"CommError: {err}\")\n",
        "except Exception as exc:\n",
        "  print(f\"Exception: {exc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfBhdTN9wcJz"
      },
      "source": [
        "Create the dataset: 3 Hugging Face Dataset in a Hugging Face DatasetDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh_SgQD9wcJz"
      },
      "outputs": [],
      "source": [
        "datasetDict = DatasetDict.load_from_disk(datasetDict_dir_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcF4Gm8GFNsc"
      },
      "outputs": [],
      "source": [
        "print(f\"datasetDict: {type(datasetDict)} {datasetDict.shape}\\n{datasetDict}\")\n",
        "print(f\"datasetDict.keys(): {datasetDict.keys()}\")\n",
        "print(f\"datasetDict['train']: {type(datasetDict['train'])} {datasetDict['train'].shape}\")\n",
        "print(f\"datasetDict['validation']: {type(datasetDict['validation'])} {datasetDict['validation'].shape}\")\n",
        "print(f\"datasetDict['test']: {type(datasetDict['test'])} {datasetDict['test'].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unjuTtKUjZI3"
      },
      "outputs": [],
      "source": [
        "example = datasetDict['train'][0]\n",
        "print(f\"datasetDict['train'][0]: {type(example)} {example.keys()}\\n{example}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEgARWcywcJz"
      },
      "source": [
        "Create the label list and the id2label and label2id mappings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKbNkGo_wcJz"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "dataset 7_1000_125_125  ,  48 labels\n",
        "dataset 7_128_18_54     ,  42 labels\n",
        "dataset 8910_1087_68_204, 206 labels\n",
        "dataset 11_1000         ,   6 labels\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vv6FWVzRwcJz"
      },
      "outputs": [],
      "source": [
        "labels = [label for label in datasetDict['train'].features.keys() if label not in ['id', 'text']]\n",
        "labels.sort()\n",
        "print(f\"labels: {type(labels)} {len(labels)}\\n{labels}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-xKopoHwcJz"
      },
      "outputs": [],
      "source": [
        "id2label = {idx:label for idx, label in enumerate(labels)}\n",
        "print(f\"id2label: {type(id2label)} {len(id2label)}\\n{id2label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5eYNbH5FNsd"
      },
      "outputs": [],
      "source": [
        "label2id = {label:idx for idx, label in enumerate(labels)}\n",
        "print(f\"label2id: {type(label2id)} {len(label2id)}\\n{label2id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_TdBZOGwcJ0"
      },
      "source": [
        "Load tokenizer and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tL1IlzvfwcJ0"
      },
      "outputs": [],
      "source": [
        "model_name = \"allenai/longformer-base-4096\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwKcOR8GwcJ0"
      },
      "outputs": [],
      "source": [
        "tokenizer = LongformerTokenizerFast.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTZ61-RUwcJ0"
      },
      "outputs": [],
      "source": [
        "model = LongformerForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels        = len(labels),\n",
        "#    num_hidden_layers = hidden_layers,\n",
        "    problem_type      = 'multi_label_classification')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-CmoMf9wcJ0"
      },
      "outputs": [],
      "source": [
        "# Configure attention window size\n",
        "model.config.attention_window = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjLO31SssqAM",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTYt0hM5wcJ0"
      },
      "source": [
        "Tokenize ('input_ids' and 'attention_mask'), add 'global_attention_mask' (for Longformer), add 'labels'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFWlSsbZaRLc",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def preprocess_data(examples, indices):\n",
        "  # Step 1: Extract text and tokenize\n",
        "  text = examples['text']             # Batch of texts\n",
        "  encoding = tokenizer(\n",
        "      text,                           # Tokenize text\n",
        "      truncation     = True,\n",
        "      padding        = 'max_length',\n",
        "      max_length     = max_length,\n",
        "      return_tensors = 'pt'           # Return PyTorch tensors\n",
        "  )\n",
        "\n",
        "  # Step 2: Create and add the global attention mask\n",
        "  global_attention_mask             = torch.zeros_like(encoding['input_ids'])  # Initialize global attention mask with zeros (same shape as input_ids)\n",
        "  global_attention_mask[:, 0]       = 1                                        # Set global attention on the first token ([CLS], token ID=0) in each sequence\n",
        "  encoding['global_attention_mask'] = global_attention_mask                    # Add the global_attention_mask to the batch\n",
        "\n",
        "  # Step 3: Create and populate the label matrix\n",
        "  labels_matrix = torch.zeros((len(text), len(labels)), dtype=torch.float32)   # Create an empty label matrix\n",
        "  #print(f\"labels_matrix: {type(labels_matrix)} {labels_matrix.shape}\")\n",
        "  #---------Populate label matrix\n",
        "  for idx, label in enumerate(labels):\n",
        "    #print(f\"idx:{idx} label:{label}\")\n",
        "    if label in examples:\n",
        "      labels_matrix[:, idx] = torch.tensor(\n",
        "          [1.0 if val else 0.0 for val in examples[label]],\n",
        "          dtype=torch.float32\n",
        "          )\n",
        "  print(f\"labels_matrix: {type(labels_matrix)} {labels_matrix.shape}\")\n",
        "\n",
        "  encoding['labels'] = labels_matrix                                           # Add labels to the encoding\n",
        "  print(f\"encoding['labels']: {type(encoding['labels'])} {encoding['labels'].shape}\")\n",
        "\n",
        "  # encoding: <class 'transformers.tokenization_utils_base.BatchEncoding'> dict_keys(['input_ids', 'attention_mask', 'global_attention_mask', 'labels'])\n",
        "  #   'input_ids': tensor([[\n",
        "  #   'attention_mask': tensor([[\n",
        "  #   'global_attention_mask': tensor([[\n",
        "  #   'labels': tensor([[\n",
        "  #print(f\"1 preprocess_data call: encoding: {type(encoding)} {encoding.keys()}\")\n",
        "\n",
        "  return encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm9REDT_wcJ0"
      },
      "source": [
        "Create the 3 encoded datasets, train, validation and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxLcO0XDwcJ0"
      },
      "outputs": [],
      "source": [
        "encoded_dataset = datasetDict.map(\n",
        "    preprocess_data,\n",
        "    batched        = True,\n",
        "    remove_columns = datasetDict['train'].column_names,\n",
        "    with_indices   = True\n",
        ")\n",
        "#train_dataset      = encoded_dataset['train']\n",
        "#validation_dataset = encoded_dataset['validation']\n",
        "#test_dataset       = encoded_dataset['test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2-_oy2YwcJ0"
      },
      "source": [
        "train_labels_list_of_lists = train_dataset['labels'].tolist()\n",
        "print(\"=============================================\")\n",
        "print(f\"encoded_dataset: {type(encoded_dataset)} {encoded_dataset.shape}\\n{encoded_dataset}\")\n",
        "print(f\"train_dataset: {type(train_dataset)} {train_dataset.shape} {train_dataset.features}\\n{train_dataset}\")\n",
        "print(f\"validation_dataset: {type(validation_dataset)} {validation_dataset.shape} {validation_dataset.features}\")\n",
        "print(f\"test_dataset: {type(test_dataset)} {test_dataset.shape} {test_dataset.features}\")\n",
        "print(\"---\")\n",
        "print(f\"train_dataset['labels']: {type(train_dataset['labels'])} len={len(train_dataset['labels'])}\\n{train_dataset['labels']}\")\n",
        "print(\"---\")\n",
        "print(f\"train_dataset[0]['input_ids']: {type(train_dataset[0]['input_ids'])} {len(train_dataset[0]['input_ids'])}\\n{train_dataset['input_ids'][0]}\")\n",
        "print(f\"train_dataset[0]['attention_mask']: {type(train_dataset[0]['attention_mask'])} {len(train_dataset[0]['attention_mask'])}\\n{train_dataset['attention_mask'][0]}\")\n",
        "print(f\"train_dataset[0]['global_attention_mask']: {type(train_dataset[0]['global_attention_mask'])} {len(train_dataset[0]['global_attention_mask'])}\\n{train_dataset['global_attention_mask'][0]}\")\n",
        "print(f\"train_dataset[0]['labels']: {type(train_dataset[0]['labels'])} {len(train_dataset[0]['labels'])} {train_dataset[0]['labels']}\")\n",
        "print(f\"train_dataset['labels'][0]: {type(train_dataset['labels'][0])} {len(train_dataset['labels'][0])}\\n{train_dataset['labels'][0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuft8rJe2Q03",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "encoded_dataset.set_format('torch')\n",
        "train_dataset      = encoded_dataset['train']\n",
        "validation_dataset = encoded_dataset['validation']\n",
        "test_dataset       = encoded_dataset['test']\n",
        "print(f\"train_dataset_tensor: {type(train_dataset)} {train_dataset.shape} {train_dataset.features}\\n{train_dataset}\")\n",
        "print(f\"train_dataset_tensor['input_ids']:             {type(train_dataset['input_ids'])}             len={len(train_dataset['input_ids'])}             shape={train_dataset['input_ids'].shape}            \") #\\n{train_dataset['input_ids']}\")\n",
        "print(f\"train_dataset_tensor['attention_mask']:        {type(train_dataset['attention_mask'])}        len={len(train_dataset['attention_mask'])}        shape={train_dataset['attention_mask'].shape}       \") #\\n{train_dataset['attention_mask']}\")\n",
        "print(f\"train_dataset_tensor['global_attention_mask']: {type(train_dataset['global_attention_mask'])} len={len(train_dataset['global_attention_mask'])} shape={train_dataset['global_attention_mask'].shape}\") #\\n{train_dataset['global_attention_mask']}\")\n",
        "print(f\"train_dataset_tensor['labels']:                {type(train_dataset['labels'])}                len={len(train_dataset['labels'])}                shape={train_dataset['labels'].shape}               \") #\\n{train_dataset['labels']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynwZnA55wcJ1"
      },
      "source": [
        "Truncated part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brkRdqdjN-Ur",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def get_truncated_part(text):\n",
        "  tokens = tokenizer(\n",
        "      text,\n",
        "      truncation                = True,\n",
        "      padding                   = 'max_length',\n",
        "      max_length                = max_length,\n",
        "      return_overflowing_tokens = True,\n",
        "      return_tensors            = None\n",
        "      )\n",
        "  print(f\"tokens.keys(): {tokens.keys()}\")\n",
        "\n",
        "  # Get the truncated tokens\n",
        "  truncated_ids = tokens[\"input_ids\"][0]\n",
        "  print(f\"truncated_ids: {type(truncated_ids)} {truncated_ids}\")\n",
        "  #overflow_ids  = tokens[\"overflow_to_sample_mapping\"][0]\n",
        "  #print(f\"overflow_ids: {type(overflow_ids)} {overflow_ids}\")\n",
        "\n",
        "  # Decode the tokens back to text\n",
        "  truncated_text = tokenizer.decode(truncated_ids, skip_special_tokens=True)\n",
        "  #overflow_text  = tokenizer.decode(overflow_ids, skip_special_tokens=True)\n",
        "\n",
        "  print(f\"original_text :\\n{text}\")\n",
        "  print(f\"truncated_text:\\n{truncated_text}\")\n",
        "  #print(f\"overflow_text:\\n{overflow_text}\")\n",
        "\n",
        "  original_tokens  = tokenizer.tokenize(text)\n",
        "  truncated_tokens = tokenizer.tokenize(truncated_text)\n",
        "  #overflow_tokens  = tokenizer.tokenize(overflow_text)\n",
        "\n",
        "  print(f\"original_tokens count : {len(original_tokens)}\")\n",
        "  print(f\"truncated_tokens count: {len(truncated_tokens)}\")\n",
        "  #print(f\"overflow_tokens count: {len(overflow_tokens)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DD_bjwTfRiQO"
      },
      "outputs": [],
      "source": [
        "example_text = datasetDict['train'][0]['text']\n",
        "#get_truncated_part(example_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFiZ4mYmwcJ1"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(\n",
        "    example_text,\n",
        "    truncation     = True,\n",
        "    padding        = 'max_length',\n",
        "    max_length     = max_length,\n",
        "    return_tensors = 'pt'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUNknGBLwcJ1"
      },
      "source": [
        "inputs: <class 'transformers.tokenization_utils_base.BatchEncoding'> dict_keys(['input_ids', 'attention_mask'])\n",
        "  {'input_ids': tensor([[\n",
        "  'attention_mask': tensor([[\n",
        "print(f\"inputs: {type(inputs)} {inputs.keys()}\") #\\n{inputs}\")\n",
        "print(f\"inputs_ids: {type(inputs.input_ids)} {inputs.input_ids.shape}\\n{inputs.input_ids}\")\n",
        "print(f\"attention_mask: {type(inputs.attention_mask)} {inputs.attention_mask.shape}\\n{inputs.attention_mask}\")\n",
        "print(f\"labels: {inputs.labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTh2E_8iwcJ1"
      },
      "source": [
        "4. Forward pass for multi-label classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_aIPMHuwcJ1"
      },
      "outputs": [],
      "source": [
        "outputs = model(\n",
        "    input_ids      = inputs.input_ids,\n",
        "    attention_mask = inputs.attention_mask\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxWcnZ8ku12V"
      },
      "outputs": [],
      "source": [
        "print(f\"outputs: {type(outputs)} {outputs.keys()}\\n{outputs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "islk-kFSs0t8"
      },
      "outputs": [],
      "source": [
        "# Logits (= raw model outputs)\n",
        "logits = outputs.logits\n",
        "print(f\"logits: {type(logits)} {logits.shape}\\n{logits}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMscqNTXuY8o"
      },
      "outputs": [],
      "source": [
        "# Convert logits to probabilities\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "probs   = sigmoid(logits)\n",
        "print(f\"probs: {type(probs)} {probs.shape}\\n{probs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZtO3uQkwcJ2"
      },
      "outputs": [],
      "source": [
        "example = encoded_dataset['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0enAb0W9o25W"
      },
      "outputs": [],
      "source": [
        "print(f\"example: {type(example)} {example.keys()}\\n{example}\")\n",
        "print()\n",
        "#print(f\"example['input_ids']: {type(example['input_ids'])} {len(example['input_ids'])}\\n{example['input_ids']}\")\n",
        "#print(f\"example['attention_mask']: {type(example['attention_mask'])} {len(example['attention_mask'])}\\n{example['attention_mask']}\")\n",
        "#print(f\"example['labels']:  {type(example['labels'])} {len(example['labels'])}\\n{example['labels']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0McCtJ8HRJY"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(example['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LAyThO7Jnvj"
      },
      "outputs": [],
      "source": [
        "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHCJcLKGwcJ2"
      },
      "source": [
        "Set PyTorch format to ensures correctness and compatibility with PyTorch pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4ENBTdulBEI"
      },
      "outputs": [],
      "source": [
        "# The 3 Hugging Face Dataset are formatted as PyTorch Dataset\n",
        "encoded_dataset.set_format('torch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5a8_vIKqr7P"
      },
      "outputs": [],
      "source": [
        "batch_size  = batch_size\n",
        "metric_name = \"f1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dR2GmpvDqbuZ",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir                  = './training_results',  # where model predictions and checkpoints will be written during training\n",
        "    overwrite_output_dir        = True,\n",
        "    logging_dir                 = './logs',\n",
        "    logging_steps               = 50,\n",
        "    save_steps                  = 500,\n",
        "    save_total_limit            = 2,\n",
        "    eval_strategy               = 'epoch',\n",
        "    save_strategy               = 'epoch',\n",
        "    learning_rate               = learning_rate,\n",
        "    per_device_train_batch_size = batch_size,\n",
        "    per_device_eval_batch_size  = batch_size,\n",
        "    gradient_accumulation_steps = gradient_accumulation_steps,\n",
        "    num_train_epochs            = epochs,\n",
        "    weight_decay                = 0.01,\n",
        "    load_best_model_at_end      = True,\n",
        "    metric_for_best_model       = metric_name,\n",
        "    run_name                    = run_name,\n",
        "    fp16                        = fp16,\n",
        "    #dataloader_num_workers      = dataloader_num_workers,\n",
        "    report_to                  = 'wandb'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zt-m0QpwcJ3"
      },
      "source": [
        "Metrics\n",
        "  source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LaQCQoJFNsi",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def multi_label_metrics(predictions, labels):\n",
        "    average = 'micro'    # 'micro' or 'weighted'\n",
        "\n",
        "    # first, apply sigmoid on predictions whose shape is (batch_size, num_labels)\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs   = sigmoid(torch.Tensor(predictions))\n",
        "\n",
        "    # next, use threshold to turn them into integer predictions\n",
        "    y_pred = np.zeros(probs.shape)\n",
        "    y_pred[np.where(probs >= threshold)] = 1\n",
        "\n",
        "    # finally, compute metrics\n",
        "    y_true               = labels\n",
        "    f1                   = f1_score               (y_true=y_true, y_pred=y_pred, average=average)    #, zero_division=1)\n",
        "    precision            = precision_score        (y_true=y_true, y_pred=y_pred, average=average)    #, zero_division=1)\n",
        "    recall               = recall_score           (y_true=y_true, y_pred=y_pred, average=average)    #, zero_division=1)\n",
        "    roc_auc              = roc_auc_score          (y_true=y_true, y_score=probs, average=average)\n",
        "    precision_recall_auc = average_precision_score(y_true=y_true, y_score=probs, average=average)\n",
        "    accuracy             = accuracy_score         (y_true=y_true, y_pred=y_pred)\n",
        "\n",
        "    # return as dictionary\n",
        "    metrics = {\n",
        "        'f1'                  : f1,\n",
        "        'precision'           : precision,\n",
        "        'recall'              : recall,\n",
        "        'roc_auc'             : roc_auc,\n",
        "        'precision_recall_auc': precision_recall_auc,\n",
        "        'accuracy'            : accuracy\n",
        "        }\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "797b2WHJqUgZ",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
        "    result = multi_label_metrics(\n",
        "        predictions = preds,\n",
        "        labels      = p.label_ids\n",
        "        )\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STMWGSJDwcJ3"
      },
      "outputs": [],
      "source": [
        "\"\"\"Let's verify a batch as well as a forward pass:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adTwB7XvFNsj"
      },
      "outputs": [],
      "source": [
        "print(f\"input_ids:              {type(encoded_dataset['train']['input_ids'][0])}\\t{encoded_dataset['train']['input_ids'][0].shape}\")\n",
        "print(f\"attention_mask:         {type(encoded_dataset['train']['attention_mask'][0])}\\t{encoded_dataset['train']['attention_mask'][0].shape}\")\n",
        "print(f\"global_attention_mask:  {type(encoded_dataset['train']['global_attention_mask'][0])}\\t{encoded_dataset['train']['global_attention_mask'][0].shape}\")\n",
        "print(f\"labels:                 {type(encoded_dataset['train'][0]['labels'])}\\t{encoded_dataset['train'][0]['labels'].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCGdbfNJwcJ4"
      },
      "source": [
        "Execute a forward pass for debugging or verification purposes (cf. BERT_3_1 in Notion BERT database)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VN-sEDggwcJ4"
      },
      "outputs": [],
      "source": [
        "outputs = model(\n",
        "    input_ids      = encoded_dataset['train']['input_ids'][0].unsqueeze(0),\n",
        "    attention_mask = encoded_dataset['train']['attention_mask'][0].unsqueeze(0),\n",
        "    labels         = encoded_dataset['train'][0]['labels'].unsqueeze(0)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtEd6KmRwcJ4"
      },
      "outputs": [],
      "source": [
        "print(f\"outputs: {type(outputs)} {outputs.keys()}\\n{outputs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "vqxhHkT1wcJ4"
      },
      "outputs": [],
      "source": [
        "\"\"\"# Define the weighted loss function\n",
        "\n",
        "class_weights = torch.tensor([7.68, 2.15, 0.61, 0.47, 0.68, 6.26], dtype=torch.float32).to(device)\n",
        "loss_fn       = BCEWithLogitsLoss(pos_weight=class_weights)  # For multi-label classification (binary classification per label)\n",
        "\n",
        "## Class supports, class weigths, weighted loss function\n",
        "\n",
        "Reminder:\n",
        "*   df_jobs      : <class 'pandas.core.frame.DataFrame'>\n",
        "*   df_jobs['id']: <class 'pandas.core.series.Series'>\n",
        "\n",
        "dataset = Dataset.from_pandas(df_jobs)\n",
        "*   dataset      : <class 'datasets.arrow_dataset.Dataset'>\n",
        "*   dataset['id']: <class 'list'>\n",
        "\n",
        "*   dataset_dict_jobs : <class 'datasets.dataset_dict.DatasetDict'>\n",
        "*   train_dataset     : <class 'datasets.arrow_dataset.Dataset'>\n",
        "*   validation_dataset: <class 'datasets.arrow_dataset.Dataset'>\n",
        "*   test_dataset      : <class 'datasets.arrow_dataset.Dataset'>\n",
        "\n",
        "\n",
        "We calculate the class supports for the train, validation and test datasets; the class weights and the weighted loss function are used for training only; the class supports of validation_dataset and test_dataset are calculated for information only.\n",
        "\n",
        "def get_train_class_weights(datasetDict, labels):\n",
        "  print(f\"datasetDict: {type(datasetDict)} shape={datasetDict.shape}\\n{datasetDict}\")\n",
        "  print(f\"labels: {type(labels)} len={len(labels)}\\n{labels}\")\n",
        "\n",
        "  dataset_train      = datasetDict['train']\n",
        "  dataset_validation = datasetDict['validation']\n",
        "  dataset_test       = datasetDict['test']\n",
        "\n",
        "  def calculate_class_supports(dataset, labels):\n",
        "    class_supports = dataset.map(\n",
        "        lambda example: {col: example[col] for col in labels},\n",
        "        batched=True\n",
        "    ).to_pandas()[labels].sum(axis=0)\n",
        "    return class_supports\n",
        "\n",
        "  class_supports = {}\n",
        "\n",
        "  for split_name, split_dataset in datasetDict.items():\n",
        "    class_supports[split_name] = calculate_class_supports(split_dataset, labels)\n",
        "\n",
        "  for split_name, split_class_supports in class_supports.items():\n",
        "    print(f\"{split_name}: {type(split_class_supports)} len={len(split_class_supports)}\\n{split_class_supports}\")\n",
        "\n",
        "  train_class_supports_list = class_supports['train'].tolist()\n",
        "  print(f\"train_class_supports_list: {type(train_class_supports_list)} len={len(train_class_supports_list)} {train_class_supports_list}\")\n",
        "\n",
        "  train_class_supports_tensor = torch.tensor(train_class_supports_list, dtype=torch.float32)\n",
        "  print(f\"train_class_supports_tensor: {type(train_class_supports_tensor)} len={len(train_class_supports_tensor)} {train_class_supports_tensor}\")\n",
        "\n",
        "  train_total_samples = dataset_train.num_rows\n",
        "  print(f\"train_total_samples: {train_total_samples}\")\n",
        "\n",
        "  number_of_classes = len(labels)\n",
        "  print(f\"number_of_classes: {number_of_classes}\")\n",
        "\n",
        "  train_class_weights = train_total_samples / (number_of_classes * train_class_supports_tensor)\n",
        "  print(f\"train_class_weights: {type(train_class_weights)} len={len(train_class_weights)} {train_class_weights}\")\n",
        "\n",
        "  train_class_weights_sum = train_class_weights.sum()\n",
        "  print(f\"train_class_weights_sum: {train_class_weights_sum}\")\n",
        "\n",
        "  normalized_train_class_weights = (train_class_weights / train_class_weights_sum) * number_of_classes\n",
        "  print(f\"normalized_train_class_weights: {type(normalized_train_class_weights)} len={len(normalized_train_class_weights)} {normalized_train_class_weights}\")\n",
        "\n",
        "  # Positives samples per label\n",
        "  supports = train_class_supports_tensor\n",
        "  print(f\"supports: {type(supports)} {len(supports)} {supports}\")\n",
        "\n",
        "  # Negatives samples per label\n",
        "  negatives = train_total_samples - supports\n",
        "  print(f\"negatives: {type(negatives)} {len(negatives)} {negatives}\")\n",
        "\n",
        "  # pos_weights = negative to positive ratios\n",
        "  pos_weights = negatives/supports\n",
        "  print(f\"pos_weights: {type(pos_weights)} {len(pos_weights)} {pos_weights}\")\n",
        "\n",
        "  # Normalize using min-max scaling\n",
        "  normalized_pos_weights_minmax = (pos_weights - pos_weights.min()) / (pos_weights.max() - pos_weights.min())\n",
        "  print(f\"normalized_pos_weights_minmax: {type(normalized_pos_weights_minmax)} {len(normalized_pos_weights_minmax)} {normalized_pos_weights_minmax}\")\n",
        "\n",
        "  # Normalize using z-score standardization\n",
        "  normalized_pos_weights_zscore = (pos_weights - pos_weights.mean()) / pos_weights.std()\n",
        "  print(f\"normalized_pos_weights_zscore: {type(normalized_pos_weights_zscore)} {len(normalized_pos_weights_zscore)} {normalized_pos_weights_zscore}\")\n",
        "\n",
        "  # Normalize using min-max scaling\n",
        "  normalized_pos_weights_minmax = (pos_weights - pos_weights.min()) / (pos_weights.max() - pos_weights.min())\n",
        "  print(f\"normalized_pos_weights_minmax: {type(normalized_pos_weights_minmax)} {len(normalized_pos_weights_minmax)} {normalized_pos_weights_minmax}\")\n",
        "\n",
        "  # Normalize using z-score standardization\n",
        "  normalized_pos_weights_zscore = (pos_weights - pos_weights.mean()) / pos_weights.std()\n",
        "  print(f\"normalized_pos_weights_zscore: {type(normalized_pos_weights_zscore)} {len(normalized_pos_weights_zscore)} {normalized_pos_weights_zscore}\")\n",
        "\n",
        "  # Normalize using sum-to-one\n",
        "  normalized_pos_weights_sum1 = pos_weights / pos_weights.sum()\n",
        "  print(f\"normalized_pos_weights_sum1: {type(normalized_pos_weights_sum1)} {len(normalized_pos_weights_sum1)} {normalized_pos_weights_sum1}\")\n",
        "\n",
        "  return normalized_pos_weights_minmax\n",
        "  #return normalized_pos_weights_zscore\n",
        "  #return normalized_pos_weights_sum1\n",
        "\n",
        "pos_weights = get_train_class_weights(datasetDict, labels)\n",
        "\n",
        "loss_fn = BCEWithLogitsLoss(pos_weight=pos_weights.to(device))  # For multi-label classification (binary classification per label)\n",
        "print(f\"loss_fn: {type(loss_fn)} {loss_fn}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "YqMipSpowcJ5"
      },
      "outputs": [],
      "source": [
        "def get_class_weights(labels=encoded_dataset['train']['labels']):\n",
        "  print(f\"labels: {type(labels)} len={len(labels)} shape={labels.shape}\\n{labels}\")\n",
        "\n",
        "  num_samples, num_labels = labels.shape\n",
        "  print(f\"num_samples: {type(num_samples)} {num_samples}\")\n",
        "  print(f\"num_labels:  {type(num_labels)}  {num_labels}\")\n",
        "\n",
        "  class_counts = labels.sum(dim=0)\n",
        "  print(f\"class_counts: {type(class_counts)} len={len(class_counts)}\\n{class_counts}\")\n",
        "\n",
        "  pos_weights = (num_samples-class_counts) / class_counts\n",
        "  print(f\"pos_weights: {type(pos_weights)} len={len(pos_weights)}\\n{pos_weights}\")\n",
        "\n",
        "  normalized_pos_weights_minmax = (pos_weights - pos_weights.min()) / (pos_weights.max() - pos_weights.min())\n",
        "  print(f\"normalized_pos_weights_minmax: {type(normalized_pos_weights_minmax)} {len(normalized_pos_weights_minmax)} {normalized_pos_weights_minmax}\")\n",
        "\n",
        "  normalized_pos_weights_zscore = (pos_weights - pos_weights.mean()) / pos_weights.std()\n",
        "  print(f\"normalized_pos_weights_zscore: {type(normalized_pos_weights_zscore)} {len(normalized_pos_weights_zscore)} {normalized_pos_weights_zscore}\")\n",
        "\n",
        "  normalized_pos_weights_sum1 = pos_weights / pos_weights.sum()\n",
        "  print(f\"normalized_pos_weights_sum1: {type(normalized_pos_weights_sum1)} {len(normalized_pos_weights_sum1)} {normalized_pos_weights_sum1}\")\n",
        "\n",
        "  #return pos_weights\n",
        "  #return normalized_pos_weights_minmax\n",
        "  #return normalized_pos_weights_zscore\n",
        "  return normalized_pos_weights_sum1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Bh-gAHGwcJ5"
      },
      "outputs": [],
      "source": [
        "pos_weights = get_class_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "4d36Km6_wcJ5"
      },
      "outputs": [],
      "source": [
        "loss_fn = BCEWithLogitsLoss(pos_weight=pos_weights.to(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izbAeCoewcJ5"
      },
      "source": [
        "raise Exception(\"Stop here\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "OPRpdL8fwcJ5"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(Module):\n",
        "  \"\"\"\n",
        "  Focal Loss implementation\n",
        "  \"\"\"\n",
        "  def __init__(self, alpha=1.0, gamma=2.0, logits=False, reduce=True):\n",
        "    super(FocalLoss, self).__init__()\n",
        "    self.alpha   = alpha\n",
        "    self.gamma   = gamma\n",
        "    self.logits  = logits  # This flag is to indicate whether input is logits or probability\n",
        "    self.reduce  = reduce\n",
        "\n",
        "  # inputs  = model's predictions: PyTorch tensor, shape=(batch_size, num_classes)\n",
        "  # targets = ground truth labels: PyTorch tensor, shape=same as inputs shape\n",
        "  def forward(self, inputs, targets):\n",
        "    # Here, we check if input is probability or logits\n",
        "    if self.logits:\n",
        "      # Input is logits\n",
        "      BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
        "    else:\n",
        "      # Input is probability\n",
        "      BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
        "    pt = torch.exp(-BCE_loss)\n",
        "    F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
        "\n",
        "    if self.reduce:\n",
        "      return torch.mean(F_loss)\n",
        "    else:\n",
        "      return F_loss\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"FocalLoss(alpha={self.alpha}, gamma={self.gamma}, logits={self.logits}, reduce={self.reduce})\"\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"FocalLoss(alpha={self.alpha}, gamma={self.gamma}, logits={self.logits}, reduce={self.reduce})\"\n",
        "\n",
        "  def __call__(self, inputs, targets):\n",
        "    return self.forward(inputs, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zknwQWsEb7oP",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "focal_loss_fn = FocalLoss(alpha=0.5, gamma=4.0, logits=True, reduce=True)\n",
        "print(f\"focal_loss_fn: {type(focal_loss_fn)} {focal_loss_fn}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CpbfGBwwcJ5"
      },
      "source": [
        "CustomTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Isb6aMV2bWJ8",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "class CustomTrainer(Trainer):\n",
        "\n",
        "  def __init__(self, *args, loss_fn=None, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.loss_fn = loss_fn\n",
        "\n",
        "  \"\"\"\n",
        "  # No print in compute_loss because out of memory because prints are batch per batch\n",
        "  def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "\n",
        "    #print(f\"inputs passed to compute_loss: {inputs.keys()}\")\n",
        "    #input_ids             = inputs['input_ids']                        # shape: batch_size, sequence_length\n",
        "    #attention_mask        = inputs['attention_mask']                   # shape: batch_size, sequence_length\n",
        "    #global_attention_mask = inputs.get('global_attention_mask', None)  # shape: batch_size, sequence_length; optional as LongFormer specific\n",
        "    labels                = inputs.pop('labels', None)                 # shape: batch_size, num_labels; needed for loss computation, not required by the model\n",
        "\n",
        "    #outputs = model(**inputs, global_attention_mask=global_attention_mask)  # Forward pass\n",
        "    # Forward pass\n",
        "    #outputs = model(\n",
        "    #    input_ids             = input_ids,\n",
        "    #    attention_mask        = attention_mask,\n",
        "    #    global_attention_mask = global_attention_mask,\n",
        "    #    labels                = labels\n",
        "    #)\n",
        "    outputs = model(**inputs, labels=labels)\n",
        "    #print(f\"outputs: {type(outputs)} {outputs.keys()}\\n{outputs}\")\n",
        "    logits = outputs.logits  # shape: (batch_size, num_labels)\n",
        "\n",
        "    # If labels are provided, compute loss\n",
        "    if labels is not None:\n",
        "      # Use the custom loss function if provided\n",
        "      if self.loss_fn is not None:\n",
        "        loss = self.loss_fn(logits, labels)  # Compute weighted loss\n",
        "      else:\n",
        "        # Default loss: BCEWithLogitsLoss\n",
        "        loss_fn = BCEWithLogitsLoss()\n",
        "        loss    = loss_fn(logits, labels)    # Compute loss\n",
        "      return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    # If no labels, return outputs only, for evaluation or inference\n",
        "    return outputs\n",
        "    \"\"\"\n",
        "  def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "    labels  = inputs.pop('labels', None)\n",
        "    outputs = model(**inputs, labels=labels)\n",
        "    logits  = outputs.logits\n",
        "\n",
        "    if labels is not None:\n",
        "      if self.loss_fn is not None:\n",
        "        loss = self.loss_fn(logits, labels)\n",
        "      else:\n",
        "        loss_fn = BCEWithLogitsLoss()\n",
        "        loss    = loss_fn(logits, labels)\n",
        "      return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2elPEoowcJ6"
      },
      "source": [
        "Create a Hugging Face's transformers trainer (which abstracts the training loop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chq_3nUz73ib"
      },
      "outputs": [],
      "source": [
        "trainer = CustomTrainer(\n",
        "#trainer = Trainer(\n",
        "    model           = model,\n",
        "    args            = training_args,\n",
        "    train_dataset   = encoded_dataset[\"train\"],\n",
        "    eval_dataset    = encoded_dataset[\"validation\"],\n",
        "    compute_metrics = compute_metrics,                # Optional: custom metrics function\n",
        "    loss_fn         = focal_loss_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy7jlzubwcJ6"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_k5uxpKwcJ6"
      },
      "outputs": [],
      "source": [
        "trainer_train = trainer.train()\n",
        "print(f\"trainer_train: {type(trainer_train)} len={len(trainer_train)}\\n{trainer_train}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvhT1c_h_oul"
      },
      "outputs": [],
      "source": [
        "file_path = \"trainer_train.json\"\n",
        "with open(file_path, \"w\") as f:\n",
        "  json.dump(trainer_train, f)\n",
        "print(f\"Train output successfully saved to {file_path}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2SMR-lI8Y8r"
      },
      "outputs": [],
      "source": [
        "print(\"Training successfully completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzSkwPcXwcJ6"
      },
      "outputs": [],
      "source": [
        "\"\"\"## Upload model, tokenizer, train results, evaluate results\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a55QdBEkYhJv"
      },
      "source": [
        "Save model to /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfPiHh62FNsl"
      },
      "outputs": [],
      "source": [
        "model_path = \"model\"\n",
        "trainer.save_model(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVQXo7SvwcJ7"
      },
      "source": [
        "Upload model and tokenizer to HF repo_id_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1ra8cu4wcJ7"
      },
      "outputs": [],
      "source": [
        "tokenizer = LongformerTokenizerFast.from_pretrained(model_path)\n",
        "model     = LongformerForSequenceClassification.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSAz3pspFNsm"
      },
      "outputs": [],
      "source": [
        "tokenizer.push_to_hub(repo_id_model)\n",
        "model.push_to_hub(repo_id_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd6B9IXFwcJ7"
      },
      "source": [
        "Upload Train results to HF repo_id_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCxHpgmT5meA"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "upload_file(\n",
        "    path_or_fileobj = 'trainer_train.json',\n",
        "    path_in_repo    = 'trainer_train.json',\n",
        "    repo_id         = HF_name,\n",
        "    repo_type       = 'dataset'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "W3tCujNQwcJ7"
      },
      "outputs": [],
      "source": [
        "\"\"\"To Get Results of Evaluation and Test\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gS9EbjVmVpjg",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def get_results(model, dataset, batch_size, threshold, phase):\n",
        "  # Clear GPU cache\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  # Set the model to evaluation mode to disable dropout and other training-specific behaviors\n",
        "  model.eval()\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "\n",
        "  dataLoader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  all_preds       = []\n",
        "  all_probs       = []\n",
        "  all_true_labels = []\n",
        "\n",
        "  for batch in tqdm(dataLoader):\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "      outputs = model(**batch)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Convert logits to probabilities and probabilities to predictions\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs   = sigmoid(logits).cpu().numpy()    # Convert to Numpy\n",
        "    preds   = (probs > threshold).astype(int)  # Convert to binary Numpy array; convert the boolean result to int (0 or 1)\n",
        "\n",
        "    # Accumulate probabilities, predictions and labels\n",
        "    all_probs.append(probs)\n",
        "    all_preds.append(preds)\n",
        "    all_true_labels.append(batch['labels'].cpu().numpy())\n",
        "\n",
        "  # Concatenate results from all batches\n",
        "  all_probs       = np.concatenate(all_probs, axis=0)        # shape: [num_samples, num_labels]\n",
        "  all_preds       = np.concatenate(all_preds, axis=0)        # shape: [num_samples, num_labels]\n",
        "  all_true_labels = np.concatenate(all_true_labels, axis=0)  # shape: [num_samples, num_labels]\n",
        "\n",
        "  print(f\"all_probs:       {type(all_probs)} {all_probs.shape}\")\n",
        "  print(f\"all_preds:       {type(all_preds)} {all_preds.shape}\")\n",
        "  print(f\"all_true_labels: {type(all_true_labels)} {all_true_labels.shape}\")\n",
        "  results_df = pd.DataFrame({\n",
        "      'predictions'  : [list(pred)  for pred  in all_preds],\n",
        "      'probabilities': [list(prob)  for prob  in all_probs],\n",
        "      'true_labels'  : [list(label) for label in all_true_labels]\n",
        "  })\n",
        "  results_file_path = f\"{phase}_results.csv\"\n",
        "  results_df.to_csv(results_file_path, index=False)\n",
        "\n",
        "  # Classification report for precision, recall, F1 score\n",
        "  report = classification_report(\n",
        "      y_true        = all_true_labels,\n",
        "      y_pred        = all_preds,\n",
        "      target_names  = labels,\n",
        "      zero_division = 0,\n",
        "      output_dict   = True\n",
        "  )\n",
        "  print(f\"Classification Report:\\n{report}\")\n",
        "\n",
        "  # ROC AUC for multi-label classification\n",
        "  roc_auc = roc_auc_score(\n",
        "      y_true  = all_true_labels,\n",
        "      y_score = all_probs,\n",
        "      average = 'micro'\n",
        "  )\n",
        "  print(f\"ROC AUC: {roc_auc}\")\n",
        "\n",
        "  metrics = {\n",
        "      'classification_report': report,\n",
        "      'roc_auc'              : roc_auc\n",
        "  }\n",
        "  metrics_file_path = f\"{phase}_metrics.json\"\n",
        "  with open(metrics_file_path, \"w\") as f:\n",
        "    json.dump(metrics, f, indent=4)\n",
        "\n",
        "  print(f\"{phase} Results Saved to {results_file_path}\")\n",
        "  print(f\"{phase} Metrics Saved to {metrics_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_results_with_threshold_tuning(model, dataset, batch_size, tune_thresholds=False, average, phase='eval'):\n",
        "  \"\"\"\n",
        "  Evaluates a model on a given dataset and optionally tunes thresholds for multi-label classification.\n",
        "\n",
        "  Args:\n",
        "    model          : The trained model to evaluate.\n",
        "    dataset        : The dataset to evaluate (validation or test).\n",
        "    batch_size     : Batch size for DataLoader.\n",
        "    tune_thresholds: Whether to tune thresholds per label (default = False).\n",
        "    average        : The averaging method for metrics ('micro', 'macro', 'weighted', etc.).\n",
        "    phase          : The phase (eval or test) for saving results (default = 'eval').\n",
        "\n",
        "  Returns:\n",
        "    A dictionary with metrics and optionally tuned thresholds.\n",
        "  \"\"\"\n",
        "\n",
        "  # Clear GPU cache\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  # Set the model to eval mode to disable dropout and other training-specific behaviors\n",
        "  model.eval\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "\n",
        "  # Create DataLoader\n",
        "  dataLoader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  all_preds       = []\n",
        "  all_probs       = []\n",
        "  all_true_labels = []\n",
        "\n",
        "  for batch in tqdm(dataLoader):\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "      outputs = model(**batch)\n",
        "    logits = outputs.logits\n",
        "\n",
        "  # Convert logits to probabilities\n",
        "  sigmoid = torch.nn.Sigmoid()\n",
        "  probs   = sigmoid(logits).cpu().numpy()    # Convert to Numpy\n",
        "  all_probs.append(probs)\n",
        "  all_true_labels.append(batch['labels'].cpu().numpy())\n",
        "\n",
        "  # Concatenate results from all batches\n",
        "  all_probs       = np.concatenate(all_probs, axis=0)        # shape: [num_samples, num_labels]\n",
        "  all_true_labels = np.concatenate(all_true_labels, axis=0)  # shape: [num_samples, num_labels]\n",
        "\n",
        "  # Initialize thresholds (default = 0.5)\n",
        "  thresholds = np.full(all_probs.shape[1], 0.5)  # shape: [num_samples]\n",
        "\n",
        "  # Thresholds tuning (if enabled)\n",
        "  if tune_thresholds:\n",
        "    # Tune thresholds for each label\n",
        "    print(f\"Tuning thresholds for {phase} phase\")\n",
        "    for label_idx in range(all_probs.shape[1]):                           # Iterate over labels\n",
        "      precision, recall, thresholds_label = precision_recall_curve(all_true_labels[:, label_idx], all_probs[:, label_idx])\n",
        "      f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)  # Avoid division by zero\n",
        "      best_threshold_idx = np.argmax(f1_scores)\n",
        "      thresholds[label_idx] = thresholds_label[best_threshold_idx]        # Set the best threshold for this label\n",
        "\n",
        "  # Apply thresholds to probabilities to generate predictions\n",
        "  all_preds = (all_probs > thresholds).astype(int)                        # Convert to binary Numpy array; convert the boolean result to int (0 or 1\n",
        "\n",
        "  # Save predictions, probabilities, and true labels to a DataFrame\n",
        "  results_df = pd.DataFrame({\n",
        "      'predictions'  : [list(pred)  for pred  in all_preds],\n",
        "      'probabilities': [list(prob)  for prob  in all_probs],\n",
        "      'true_labels'  : [list(label) for label in all_true_labels]\n",
        "  })\n",
        "  results_file_path = f\"{phase}_results.csv\"\n",
        "  results_df.to_csv(results_file_path, index=False)\n",
        "\n",
        "  # Compute metrics\n",
        "  print(f\"Computing metrics for {phase} phase\")\n",
        "  classification_report_dict = classification_report(\n",
        "      y_true        = all_true_labels,\n",
        "      y_pred        = all_preds,\n",
        "      target_names  = labels,\n",
        "      zero_division = 0,\n",
        "      output_dict   = True\n",
        "  )\n",
        "\n",
        "  # Compute roc_auc\n",
        "  roc_auc = roc_auc_score(\n",
        "      y_true  = all_true_labels,\n",
        "      y_score = all_probs,\n",
        "      average = average\n",
        "  )\n",
        "\n",
        "  # Computer precision_recall_auc\n",
        "  precision_recall_auc = average_precision_score(all_true_labels, all_probs, average=average)\n",
        "\n",
        "  #precision, recall, _ = precision_recall_curve(all_true_labels, all_probs)\n",
        "  #precision_recall_auc = auc(recall, precision)\n",
        "\n",
        "  metrics = {\n",
        "      'classification_report': classification_report_dict,\n",
        "      'roc_auc'              : roc_auc,\n",
        "      'precision_recall_auc' : precision_recall_auc,\n",
        "      'thresholds'           : thresholds.tolist() if tune_thresholds else \"Default (0.5)\",  # Convert numpy array to list\n",
        "  }\n",
        "\n",
        "  metrics_file_path = f\"{phase}_metrics.json\"\n",
        "  with open(metrics_file_path, \"w\") as f:\n",
        "    json.dump(metrics, f, indent=4)\n",
        "\n",
        "  print(f\"{phase} Results Saved to {results_file_path}\")\n",
        "  print(f\"{phase} Metrics Saved to {metrics_file_path}\")\n",
        "\n",
        "  return metrics\n"
      ],
      "metadata": {
        "id": "6qWc2wL8-cQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGKK2rYhwcJ8"
      },
      "outputs": [],
      "source": [
        "\"\"\"## Evaluate\n",
        "\n",
        "After training, we evaluate our model on the validation set.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiloh9eMK91o"
      },
      "source": [
        "First evaluate results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhnKJcVVwcJ8"
      },
      "outputs": [],
      "source": [
        "phase_evaluate = 'evaluate_model_eval'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoJpTASkY8De"
      },
      "outputs": [],
      "source": [
        "get_results(\n",
        "    model      = model,\n",
        "    dataset    = validation_dataset,\n",
        "    batch_size = batch_size,\n",
        "    threshold  = threshold,\n",
        "    phase      = phase_evaluate\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRVTBgrPVJ8L"
      },
      "outputs": [],
      "source": [
        "print(\"First evaluation successfully completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbgMYXu3wcJ8"
      },
      "source": [
        "Second evaluate results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kc70J63wcJ8"
      },
      "outputs": [],
      "source": [
        "trainer_evaluate = trainer.evaluate()\n",
        "print(f\"trainer_evaluate: {type(trainer_evaluate)} len={len(trainer_evaluate)}\\n{trainer_evaluate}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMlebJ83LRYG"
      },
      "outputs": [],
      "source": [
        "file_path = \"trainer_evaluate.json\"\n",
        "with open(file_path, \"w\") as f:\n",
        "  json.dump(trainer_evaluate, f)\n",
        "print(f\"Evaluate output successfully saved to {file_path}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QwshIQGSFxg"
      },
      "outputs": [],
      "source": [
        "print(\"Second evaluation successfully completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQzo6qeGwcJ8"
      },
      "source": [
        "Upload Evaluate Results to HF repo_id_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYmURgAz4Xk-"
      },
      "outputs": [],
      "source": [
        "upload_file(\n",
        "    path_or_fileobj = f\"{phase_evaluate}_results.csv\",\n",
        "    path_in_repo    = f\"{phase_evaluate}_results.csv\",\n",
        "    repo_id         = HF_name,\n",
        "    repo_type       = 'dataset'\n",
        ")\n",
        "upload_file(\n",
        "    path_or_fileobj = f\"{phase_evaluate}_metrics.json\",\n",
        "    path_in_repo    = f\"{phase_evaluate}_metrics.json\",\n",
        "    repo_id         = HF_name,\n",
        "    repo_type       = 'dataset'\n",
        ")\n",
        "upload_file(\n",
        "    path_or_fileobj = 'trainer_evaluate.json',\n",
        "    path_in_repo    = 'trainer_evaluate.json',\n",
        "    repo_id         = HF_name,\n",
        "    repo_type       = 'dataset'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H-pjMiiwcJ9"
      },
      "outputs": [],
      "source": [
        "\"\"\"## Test\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwO6_V_lYcJg"
      },
      "source": [
        "First test results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X0r_TALwcJ9"
      },
      "outputs": [],
      "source": [
        "phase_test = 'test_model_eval'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPHcZrdsEARs"
      },
      "outputs": [],
      "source": [
        "get_results(\n",
        "    model      = model,\n",
        "    dataset    = test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    threshold  = threshold,\n",
        "    phase      = phase_test\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yibAPgTaVljm"
      },
      "outputs": [],
      "source": [
        "print(\"First test successfully completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMtMjR-WwcJ9"
      },
      "source": [
        "Second test results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX1sWMZywcJ9"
      },
      "outputs": [],
      "source": [
        "trainer_predict = trainer.predict(test_dataset)\n",
        "print(f\"trainer_predict: {type(trainer_predict)} len={len(trainer_predict)}\\n{trainer_predict}\")\n",
        "print(f\"trainer_predict.predictions: {type(trainer_predict.predictions)} shape={trainer_predict.predictions.shape}\")  # Model logits\n",
        "print(f\"trainer_predict.label_ids: {type(trainer_predict.label_ids)} shape={trainer_predict.label_ids.shape}\")        # Ground truth labels\n",
        "print(f\"trainer_predict.metrics: {type(trainer_predict.metrics)} len={len(trainer_predict.metrics)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY42YauKwcJ9"
      },
      "outputs": [],
      "source": [
        "trainer_predict_json_serializable = {\n",
        "    'predictions': trainer_predict.predictions.tolist(),  # Convert Numpy array to list\n",
        "    'label_ids'  : trainer_predict.label_ids.tolist(),    # Convert Numpy array to list\n",
        "    'metrics'    : trainer_predict.metrics                # Dictionary is already serializable\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39vYrXt4HSaO"
      },
      "outputs": [],
      "source": [
        "file_path = \"trainer_predict.json\"\n",
        "with open(file_path, \"w\") as f:\n",
        "  json.dump(trainer_predict_json_serializable, f)\n",
        "print(f\"Test output successfully saved to {file_path}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_F1Y8Hve8qsa"
      },
      "outputs": [],
      "source": [
        "print(\"Second test successfully completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4ckvP9rwcJ9"
      },
      "source": [
        "Upload Test Results to HF repo_id_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSy9v4OL7iPa"
      },
      "outputs": [],
      "source": [
        "upload_file(\n",
        "    path_or_fileobj = f\"{phase_test}_results.csv\",\n",
        "    path_in_repo    = f\"{phase_test}_results.csv\",\n",
        "    repo_id         = HF_name,\n",
        "    repo_type       = 'dataset'\n",
        ")\n",
        "upload_file(\n",
        "    path_or_fileobj = f\"{phase_test}_metrics.json\",\n",
        "    path_in_repo    = f\"{phase_test}_metrics.json\",\n",
        "    repo_id         = HF_name,\n",
        "    repo_type       = 'dataset'\n",
        ")\n",
        "upload_file(\n",
        "    path_or_fileobj = 'trainer_predict.json',\n",
        "    path_in_repo    = 'trainer_predict.json',\n",
        "    repo_id         = HF_name,\n",
        "    repo_type       = 'dataset'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvh_jDvdBt26"
      },
      "outputs": [],
      "source": [
        "print(\"It's the end\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "jupytext": {
      "formats": "ipynb,py:nomarker"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}